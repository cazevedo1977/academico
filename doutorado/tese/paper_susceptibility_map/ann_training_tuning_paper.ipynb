{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Training and tuning neural network__\n",
    "- Dataset: Guarujá Municipality - São Paulo - Brazil\n",
    "- Part 2/3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authorship:\n",
    "        Caio Azevedo\n",
    "        São Paulo University, October, 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main goals:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Objetivos__:\n",
    "\n",
    "- Tratamento dos dados para aplicação da rede neural: \n",
    "   - remoção de features irrelevantes;\n",
    "   - normalização do dataset;\n",
    "   - separação do dados de treino e teste da rede neural\n",
    "\n",
    "- Treinamento e avaliação das redes neurais:\n",
    "   - implementação keras\n",
    "   - implementação customizada\n",
    "\n",
    "- Automação do setup dos hiperparâmetros para melhor desempenho (critério inicial acurácia).\n",
    "  - em __15-08-2021__ - Uso Keras Tuning para otimização dos hyperparameters Keras (vide arquivo .ipynb)\n",
    "  - em __29-08-2021__ - Força Bruta para otimização dos hiperparametros na rede customizada\n",
    "\n",
    "- Avaliar performance da rede com e sem 'features irrelevantes'.\n",
    "\n",
    "__Sobre a implementação da Rede Neural Proprietária__:\n",
    "\n",
    "1. ref. https://whimsical.com/artificial-neural-network-4cTMNjQBkkCwJHZhUy7BTV\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparando predição com rede do keras\n",
    "- ref. https://machinelearningmastery.com/tutorial-first-neural-network-python-keras/\n",
    "\n",
    "Tunning neural netowrks\n",
    "- ref. http://karpathy.github.io/2019/04/25/recipe/\n",
    "\n",
    "Sobre as métricas\n",
    "- ref. https://neptune.ai/blog/f1-score-accuracy-roc-auc-pr-auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imported libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "start = datetime.datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Rede_Neural as rna\n",
    "from Rede_Neural import NeuralNetwork\n",
    "from Rede_Neural import Layer\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os, sys\n",
    "import seaborn as sns\n",
    "\n",
    "from matplotlib import pyplot\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from utils import plot\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auxiliary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gráfico que relacionada cada uma das features com o valor predito\n",
    "def plotSwarmChart(dataSet):\n",
    "    dataSet['Constante'] = 0 #feature inútil para referência do gráfico\n",
    "\n",
    "    f, axes = plt.subplots(1, 7, figsize=(35, 17), sharex=False)\n",
    "    f.subplots_adjust(hspace=0.2, wspace=0.7)\n",
    "    \n",
    "    sns.catplot(x=dataSet.columns[6], y='score', kind=\"swarm\", hue='score', data=dataSet,ax=axes[6])\n",
    "    \n",
    "    for i in range(7):\n",
    "        col = dataSet.columns[i] \n",
    "        ax = sns.swarmplot(x=dataSet['Constante'],y=dataSet[col].values,hue=(dataSet['score']>0.5),ax=axes[i])\n",
    "        ax.set_title(col)\n",
    "        \n",
    "    plt.close(2)\n",
    "    plt.close(3)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NetworkPerformance(y_real, y_prob):\n",
    "    y_predict = (y_prob > 0.5)\n",
    "    # accuracy: (tp + tn) / (p + n)\n",
    "    accuracy = accuracy_score(y_real, y_predict)\n",
    "    print('Accuracy: %f' % accuracy)\n",
    "    # precision tp / (tp + fp)\n",
    "    precision = precision_score(y_real, y_predict)\n",
    "    print('Precision: %f' % precision)\n",
    "    # recall: tp / (tp + fn)\n",
    "    recall = recall_score(y_real, y_predict)\n",
    "    print('Recall: %f' % recall)\n",
    "    # f1: 2 tp / (2 tp + fp + fn)\n",
    "    f1 = f1_score(y_real, y_predict)\n",
    "    print('F1 score: %f' % f1)\n",
    "    # kappa\n",
    "    kappa = cohen_kappa_score(y_real, y_predict)\n",
    "    print('Cohens kappa: %f' % kappa)\n",
    "    # ROC AUC\n",
    "    auc = roc_auc_score(y_real, y_prob)\n",
    "    print('ROC AUC: %f' % auc)\n",
    "    # confusion matrix\n",
    "    matrix = confusion_matrix(y_real, y_predict)\n",
    "    print(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printAUROC(y_real,y_predict):\n",
    "    ns_auc = roc_auc_score(y_real, y_predict)\n",
    "    # summarize scores\n",
    "    print('Área curva ROC=%.4f' % (ns_auc))\n",
    "    # calculate roc curves\n",
    "    lr_fpr, lr_tpr, _ = roc_curve(y_real, y_predict)\n",
    "    random_probs = [0 for i in range(len(y_test))]\n",
    "    p_fpr, p_tpr, _ = roc_curve(y_test, random_probs, pos_label=1)\n",
    "    # plot the roc curve for the model\n",
    "    plt.plot(p_fpr, p_tpr, linestyle='--', label='aleatório', color='orange')\n",
    "    pyplot.plot(lr_fpr, lr_tpr, marker='.', label='RNA', color='blue')\n",
    "    \n",
    "    # axis labels\n",
    "    pyplot.xlabel('Taxa de falso positivo')\n",
    "    pyplot.ylabel('Taxa de verdadeiro positivo')\n",
    "    # show the legend\n",
    "    pyplot.legend()\n",
    "    # show the plot\n",
    "    pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#divide o dataset em base de treino e teste\n",
    "def dataPreparation(dataSet):\n",
    "    X = dataSet.drop('class',axis=1)\n",
    "    y= dataSet['class']\n",
    "    \n",
    "    X = X.to_numpy()    #converts dataframe into array to be used at NN\n",
    "    y = y.to_numpy()    #converts dataframe into array to be used at NN\n",
    "    y = y.reshape(-1,1) #reorganiza o array em um array 1 x 1\n",
    "    \n",
    "    # split data into training and testing sets\n",
    "    seed = 7\n",
    "    test_size = 0.30\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=seed)\n",
    "    \n",
    "    # define the scaler\n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1)) \n",
    "    #scaler = MinMaxScaler() \n",
    "    # fit on the training dataset\n",
    "    scaler.fit(X_train)\n",
    "    # scale the training dataset\n",
    "    X_train = scaler.transform(X_train)\n",
    "    \n",
    "    # scale the test dataset\n",
    "    X_test = scaler.transform(X_test)\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#divide o dataset em treino-teste-validação\n",
    "def dataPreparation2(dataSet):\n",
    "    X = dataSet.drop('class',axis=1)\n",
    "    y= dataSet['class']\n",
    "    \n",
    "    X = X.to_numpy()    #converts dataframe into array to be used at NN\n",
    "    y = y.to_numpy()    #converts dataframe into array to be used at NN\n",
    "    y = y.reshape(-1,1) #reorganiza o array em um array 1 x 1\n",
    "    \n",
    "    # split data into training, validation and testing sets\n",
    "    seed = 7\n",
    "    test_size = 0.30\n",
    "    \n",
    "    train_x, test_x, train_y, test_y = train_test_split(X, y, test_size=test_size, shuffle=False)\n",
    "    train_x, val_x, train_y, val_y   = train_test_split(train_x,train_y, test_size=test_size, shuffle=False)\n",
    "\n",
    "    # define the scaler\n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1)) \n",
    "    #scaler = MinMaxScaler() \n",
    "    # fit on the training dataset\n",
    "    scaler.fit(train_x)\n",
    "    # scale the training dataset\n",
    "    train_x = scaler.transform(train_x)\n",
    "    # scale the test dataset\n",
    "    test_x = scaler.transform(test_x)\n",
    "    # scale the test dataset\n",
    "    val_x = scaler.transform(val_x)\n",
    "    return train_x, test_x, val_x, train_y, test_y, val_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtaining and visualize the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FID</th>\n",
       "      <th>X</th>\n",
       "      <th>Y</th>\n",
       "      <th>twi</th>\n",
       "      <th>curvature</th>\n",
       "      <th>slope</th>\n",
       "      <th>elevation</th>\n",
       "      <th>aspect</th>\n",
       "      <th>lithology</th>\n",
       "      <th>land_use</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>98</td>\n",
       "      <td>381913.2155</td>\n",
       "      <td>7358368.235</td>\n",
       "      <td>27.338095</td>\n",
       "      <td>0.002735</td>\n",
       "      <td>27.338095</td>\n",
       "      <td>120.943680</td>\n",
       "      <td>330.186584</td>\n",
       "      <td>65</td>\n",
       "      <td>90</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>152</td>\n",
       "      <td>375711.0000</td>\n",
       "      <td>7348857.000</td>\n",
       "      <td>5.155329</td>\n",
       "      <td>0.006004</td>\n",
       "      <td>24.772690</td>\n",
       "      <td>17.397917</td>\n",
       "      <td>113.472549</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>95</td>\n",
       "      <td>383008.5927</td>\n",
       "      <td>7357558.608</td>\n",
       "      <td>24.757074</td>\n",
       "      <td>0.000368</td>\n",
       "      <td>24.757074</td>\n",
       "      <td>207.428345</td>\n",
       "      <td>121.768433</td>\n",
       "      <td>65</td>\n",
       "      <td>90</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>106</td>\n",
       "      <td>371074.0000</td>\n",
       "      <td>7345111.000</td>\n",
       "      <td>6.688554</td>\n",
       "      <td>-0.010576</td>\n",
       "      <td>29.801399</td>\n",
       "      <td>15.330963</td>\n",
       "      <td>71.881042</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>82</td>\n",
       "      <td>375019.5230</td>\n",
       "      <td>7353140.890</td>\n",
       "      <td>0.221845</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>0.221845</td>\n",
       "      <td>2.162373</td>\n",
       "      <td>185.550385</td>\n",
       "      <td>2</td>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   FID            X            Y        twi  curvature      slope   elevation  \\\n",
       "0   98  381913.2155  7358368.235  27.338095   0.002735  27.338095  120.943680   \n",
       "1  152  375711.0000  7348857.000   5.155329   0.006004  24.772690   17.397917   \n",
       "2   95  383008.5927  7357558.608  24.757074   0.000368  24.757074  207.428345   \n",
       "3  106  371074.0000  7345111.000   6.688554  -0.010576  29.801399   15.330963   \n",
       "4   82  375019.5230  7353140.890   0.221845  -0.000012   0.221845    2.162373   \n",
       "\n",
       "       aspect  lithology  land_use  class  \n",
       "0  330.186584         65        90      0  \n",
       "1  113.472549          2        20      1  \n",
       "2  121.768433         65        90      0  \n",
       "3   71.881042          2        20      1  \n",
       "4  185.550385          2        70      0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "APP_PATH = os.getcwd()\n",
    "\n",
    "#file = \"Input_Guaruja_50m_shuffled.csv\"\n",
    "#file = \"Input_Guaruja_100m_shuffled.csv\"\n",
    "#file = \"Input_Guaruja_150m_shuffled.csv\"\n",
    "#file = \"Input_Guaruja_200m_shuffled.csv\"\n",
    "\n",
    "#file = \"pt_nocorrencias_1km_shuffled.csv\"\n",
    "#file = \"pt_nocorrencias_2km_shuffled.csv\"\n",
    "#file = \"pt_nocorrencias_3km_shuffled.csv\"\n",
    "#file = \"pt_nocorrencias_4km_shuffled.csv\"\n",
    "\n",
    "file = \"guaruja_shuffled.csv\"\n",
    "\n",
    "\n",
    "dataset = pd.read_csv(os.path.join(APP_PATH, os.path.join(\"data\", file))) \n",
    "\n",
    "# load the dataset\n",
    "#dataset = dataset.sample(frac = 1) #embaralha os registros\n",
    "dataset_original = dataset.copy()\n",
    "dataset.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 11)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FID</th>\n",
       "      <th>X</th>\n",
       "      <th>Y</th>\n",
       "      <th>twi</th>\n",
       "      <th>curvature</th>\n",
       "      <th>slope</th>\n",
       "      <th>elevation</th>\n",
       "      <th>aspect</th>\n",
       "      <th>lithology</th>\n",
       "      <th>land_use</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>200.00</td>\n",
       "      <td>200.00</td>\n",
       "      <td>200.00</td>\n",
       "      <td>200.00</td>\n",
       "      <td>200.00</td>\n",
       "      <td>200.00</td>\n",
       "      <td>200.00</td>\n",
       "      <td>200.00</td>\n",
       "      <td>200.00</td>\n",
       "      <td>200.00</td>\n",
       "      <td>200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>100.50</td>\n",
       "      <td>375351.60</td>\n",
       "      <td>7350229.58</td>\n",
       "      <td>14.42</td>\n",
       "      <td>0.03</td>\n",
       "      <td>23.94</td>\n",
       "      <td>64.32</td>\n",
       "      <td>209.79</td>\n",
       "      <td>32.56</td>\n",
       "      <td>75.80</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>57.88</td>\n",
       "      <td>4313.00</td>\n",
       "      <td>4368.70</td>\n",
       "      <td>13.62</td>\n",
       "      <td>0.41</td>\n",
       "      <td>11.79</td>\n",
       "      <td>56.08</td>\n",
       "      <td>103.92</td>\n",
       "      <td>31.56</td>\n",
       "      <td>26.72</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.00</td>\n",
       "      <td>365857.79</td>\n",
       "      <td>7341789.72</td>\n",
       "      <td>0.09</td>\n",
       "      <td>-0.59</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.00</td>\n",
       "      <td>20.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>50.75</td>\n",
       "      <td>372179.26</td>\n",
       "      <td>7346972.48</td>\n",
       "      <td>4.29</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>18.08</td>\n",
       "      <td>14.33</td>\n",
       "      <td>121.04</td>\n",
       "      <td>2.00</td>\n",
       "      <td>90.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>100.50</td>\n",
       "      <td>375219.03</td>\n",
       "      <td>7349677.08</td>\n",
       "      <td>6.13</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>25.33</td>\n",
       "      <td>54.48</td>\n",
       "      <td>202.74</td>\n",
       "      <td>2.00</td>\n",
       "      <td>90.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>150.25</td>\n",
       "      <td>379116.73</td>\n",
       "      <td>7354032.25</td>\n",
       "      <td>26.33</td>\n",
       "      <td>0.00</td>\n",
       "      <td>30.77</td>\n",
       "      <td>94.71</td>\n",
       "      <td>316.40</td>\n",
       "      <td>65.00</td>\n",
       "      <td>90.00</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>200.00</td>\n",
       "      <td>383738.84</td>\n",
       "      <td>7359892.24</td>\n",
       "      <td>69.73</td>\n",
       "      <td>4.30</td>\n",
       "      <td>69.73</td>\n",
       "      <td>226.06</td>\n",
       "      <td>359.51</td>\n",
       "      <td>65.00</td>\n",
       "      <td>90.00</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          FID          X           Y     twi  curvature   slope  elevation  \\\n",
       "count  200.00     200.00      200.00  200.00     200.00  200.00     200.00   \n",
       "mean   100.50  375351.60  7350229.58   14.42       0.03   23.94      64.32   \n",
       "std     57.88    4313.00     4368.70   13.62       0.41   11.79      56.08   \n",
       "min      1.00  365857.79  7341789.72    0.09      -0.59    0.09       0.23   \n",
       "25%     50.75  372179.26  7346972.48    4.29      -0.01   18.08      14.33   \n",
       "50%    100.50  375219.03  7349677.08    6.13      -0.00   25.33      54.48   \n",
       "75%    150.25  379116.73  7354032.25   26.33       0.00   30.77      94.71   \n",
       "max    200.00  383738.84  7359892.24   69.73       4.30   69.73     226.06   \n",
       "\n",
       "       aspect  lithology  land_use  class  \n",
       "count  200.00     200.00    200.00  200.0  \n",
       "mean   209.79      32.56     75.80    0.5  \n",
       "std    103.92      31.56     26.72    0.5  \n",
       "min      0.04       2.00     20.00    0.0  \n",
       "25%    121.04       2.00     90.00    0.0  \n",
       "50%    202.74       2.00     90.00    0.0  \n",
       "75%    316.40      65.00     90.00    1.0  \n",
       "max    359.51      65.00     90.00    1.0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#describing the dataframe to seek for distribution information\n",
    "dataset.describe().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x936 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(15,13))\n",
    "sizes = dataset['elevation']*10\n",
    "scatter = ax.scatter(dataset['slope'], dataset['elevation'], \n",
    "           c=(dataset['class']), cmap='rainbow', alpha=0.5,\n",
    "           s=sizes, edgecolors='none')\n",
    "\n",
    "legend1 = ax.legend(*scatter.legend_elements(),\n",
    "                    loc=\"upper right\", title=\"landslide\")\n",
    "\n",
    "ax.title.set_text(\"{}{}\".format(len(dataset.index), ' landslides samples')) \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataPrep\n",
    " - Based on the results of the feature importance analysis, all features were retained for training the ANN.\n",
    " - We addressed concerns about data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>slope</th>\n",
       "      <th>aspect</th>\n",
       "      <th>elevation</th>\n",
       "      <th>land_use</th>\n",
       "      <th>lithology</th>\n",
       "      <th>twi</th>\n",
       "      <th>curvature</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>27.338095</td>\n",
       "      <td>330.186584</td>\n",
       "      <td>120.943680</td>\n",
       "      <td>90</td>\n",
       "      <td>65</td>\n",
       "      <td>27.338095</td>\n",
       "      <td>0.002735</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>24.772690</td>\n",
       "      <td>113.472549</td>\n",
       "      <td>17.397917</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>5.155329</td>\n",
       "      <td>0.006004</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>24.757074</td>\n",
       "      <td>121.768433</td>\n",
       "      <td>207.428345</td>\n",
       "      <td>90</td>\n",
       "      <td>65</td>\n",
       "      <td>24.757074</td>\n",
       "      <td>0.000368</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>29.801399</td>\n",
       "      <td>71.881042</td>\n",
       "      <td>15.330963</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>6.688554</td>\n",
       "      <td>-0.010576</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.221845</td>\n",
       "      <td>185.550385</td>\n",
       "      <td>2.162373</td>\n",
       "      <td>70</td>\n",
       "      <td>2</td>\n",
       "      <td>0.221845</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       slope      aspect   elevation  land_use  lithology        twi  \\\n",
       "0  27.338095  330.186584  120.943680        90         65  27.338095   \n",
       "1  24.772690  113.472549   17.397917        20          2   5.155329   \n",
       "2  24.757074  121.768433  207.428345        90         65  24.757074   \n",
       "3  29.801399   71.881042   15.330963        20          2   6.688554   \n",
       "4   0.221845  185.550385    2.162373        70          2   0.221845   \n",
       "\n",
       "   curvature  class  \n",
       "0   0.002735      0  \n",
       "1   0.006004      1  \n",
       "2   0.000368      0  \n",
       "3  -0.010576      1  \n",
       "4  -0.000012      0  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#features menos relevantes guaruja\n",
    "#dataset = dataset.drop('elevation',axis=1)\n",
    "#dataset = dataset.drop('curv',axis=1)\n",
    "#dataset = dataset.drop('lito',axis=1)\n",
    "#dataset = dataset.drop('uso_solo',axis=1)\n",
    "# de fato, essas 3 features são irrelevantes pois a acurácia é similar sem elas no dataset\n",
    "dataset = dataset[['slope','aspect','elevation','land_use','lithology','twi','curvature','class']]\n",
    "dataset.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, X_val, y_train, y_test, y_val = dataPreparation2(dataset)\n",
    "#X_train, X_test, y_train, y_test = dataPreparation(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98 train examples\n",
      "42 validation examples\n",
      "60 test examples\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train), 'train examples')\n",
    "print(len(X_val), 'validation examples')\n",
    "print(len(X_test), 'test examples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1. -1. -1. -1. -1. -1. -1.] [ 1.63025317  0.948116    1.03804377  1.          1.          1.35762042\n",
      " -0.98499636]\n"
     ]
    }
   ],
   "source": [
    "print(X_train.min(axis=0), X_test.max(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.concatenate((X_train, X_val))\n",
    "X = np.concatenate((X, X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 7)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.concatenate((y_train, y_val))\n",
    "y = np.concatenate((y, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 1)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.         -1.00147197 -1.00818162 -1.         -1.         -1.\n",
      " -1.26211285] [1.78731424 1.00048694 1.03804377 1.         1.         1.78731424\n",
      " 1.        ]\n"
     ]
    }
   ],
   "source": [
    "print(X.min(axis=0), X.max(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Computa o número de variáveis de entrada (features) e saída (sempre 1)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim, output_dim = X_train.shape[1], y_train.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom neural network training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameters tunning\n",
    "- Brute force method\n",
    "- custom classes and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class hyperparametros: \n",
    "    def __init__(self, acuracia, camadas,neuronios,learning_rate,batch_size,dropout): \n",
    "        self.acuracia = acuracia \n",
    "        self.camadas = camadas\n",
    "        self.neuronios = neuronios \n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.dropout = dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANN_accuracy: \n",
    "    def __init__(self, acuracia, _neuralNetwork): \n",
    "        self.acuracia = acuracia \n",
    "        self.ann = _neuralNetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#patience = early stopping\n",
    "def setBestNeuralNetwork(hidden_layers,neurons, num_learning_rate, droput, input_dim, output_dim):\n",
    "    nn = NeuralNetwork(cost_func = rna.binary_cross_entropy, learning_rate = num_learning_rate, momentum=0.1, patience=50) \n",
    "\n",
    "    #input layer\n",
    "    nn.layers.append(Layer(input_dim=input_dim, output_dim=neurons,activation= rna.relu, weights_initializer=rna.glorot_normal))\n",
    "\n",
    "    for num_hidden_layers in range(1,hidden_layers+1,1):\n",
    "        #nn.layers.append(Layer(input_dim=neurons, output_dim=neurons,activation=rna.relu,reg_func=rna.l2_regularization,reg_strength=1e-2))\n",
    "        nn.layers.append(Layer(input_dim=neurons, output_dim=neurons,activation=rna.relu,dropout_prob = droput,weights_initializer=rna.glorot_normal,biases_initializer=rna.glorot_normal))\n",
    "\n",
    "    #output layer\n",
    "    nn.layers.append(Layer(input_dim=neurons, output_dim=output_dim,activation=rna.sigmoid,weights_initializer=rna.glorot_normal))  \n",
    "\n",
    "    return nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (batch_size) implementa o mini-batch que acelera o tempo de treinamento, requer método de inicialização do mini-batch. \n",
    "#ideal que o batch_size seja multiplo do tamanho do conjunto de dados de treinamento, que aqui é de 128 \n",
    "neurons = [7,8,12] \n",
    "# preciso encontrar uma forma de implementar a alteração do numero de neuronios na camada oculta variando \n",
    "# de n-2 até n+6\n",
    "hidden_layers = [1,2,3]\n",
    "learning_rate = [0.001,0.005,0.01]\n",
    "batch_size = [0,4,8] \n",
    "dropout_rate = [0,0.1]\n",
    "best_of_best_ANN = []\n",
    "best_of_best_hyper = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setTestNeuralNetwork(hidden_layers,neurons, num_learning_rate, droput, input_dim, output_dim):\n",
    "    #nn = NeuralNetwork(cost_func = rna.binary_cross_entropy, learning_rate = num_learning_rate, momentum=0.25, patience=50) \n",
    "\n",
    "    #camada de entrada\n",
    "    print('\\033[1m Input layer: {} inputs neurons  {} outputs neurons {} learning rate \\033[0m'.format(input_dim, neurons,num_learning_rate))\n",
    "    #nn.layers.append(Layer(input_dim=input_dim, output_dim=neurons,activation= rna.relu, weights_initializer=rna.glorot_normal))\n",
    "\n",
    "    for num_hidden_layers in range(1,hidden_layers+1,1):\n",
    "        print(' hidden layer {} neurons {} layers '.format(neurons,hidden_layers))\n",
    "        #nn.layers.append(Layer(input_dim=neurons, output_dim=neurons,activation=rna.relu,reg_func=rna.l2_regularization,reg_strength=1e-2))\n",
    "        #nn.layers.append(Layer(input_dim=neurons, output_dim=neurons,activation=rna.relu,dropout_prob = droput,weights_initializer=rna.glorot_normal))\n",
    "    \n",
    "    #nn.layers.append(Layer(input_dim=neurons, output_dim=output_dim,activation=rna.sigmoid,weights_initializer=rna.glorot_normal))  \n",
    "    print('\\033[1m {} last hidden layer neurons, {} output neurons \\033[0m'.format(neurons,output_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m Input layer: 7 inputs neurons  7 outputs neurons 0.001 learning rate \u001b[0m\n",
      " hidden layer 7 neurons 1 layers \n",
      "\u001b[1m 7 last hidden layer neurons, 1 output neurons \u001b[0m\n",
      "\u001b[1m Input layer: 7 inputs neurons  7 outputs neurons 0.001 learning rate \u001b[0m\n",
      " hidden layer 7 neurons 1 layers \n",
      "\u001b[1m 7 last hidden layer neurons, 1 output neurons \u001b[0m\n",
      "\u001b[1m Input layer: 7 inputs neurons  7 outputs neurons 0.001 learning rate \u001b[0m\n",
      " hidden layer 7 neurons 1 layers \n",
      "\u001b[1m 7 last hidden layer neurons, 1 output neurons \u001b[0m\n",
      "\u001b[1m Input layer: 7 inputs neurons  7 outputs neurons 0.001 learning rate \u001b[0m\n",
      " hidden layer 7 neurons 1 layers \n",
      "\u001b[1m 7 last hidden layer neurons, 1 output neurons \u001b[0m\n",
      "\u001b[1m Input layer: 7 inputs neurons  7 outputs neurons 0.001 learning rate \u001b[0m\n",
      " hidden layer 7 neurons 1 layers \n",
      "\u001b[1m 7 last hidden layer neurons, 1 output neurons \u001b[0m\n",
      "\u001b[1m Input layer: 7 inputs neurons  7 outputs neurons 0.001 learning rate \u001b[0m\n",
      " hidden layer 7 neurons 1 layers \n",
      "\u001b[1m 7 last hidden layer neurons, 1 output neurons \u001b[0m\n",
      "\u001b[1m Input layer: 7 inputs neurons  7 outputs neurons 0.005 learning rate \u001b[0m\n",
      " hidden layer 7 neurons 1 layers \n",
      "\u001b[1m 7 last hidden layer neurons, 1 output neurons \u001b[0m\n",
      "\u001b[1m Input layer: 7 inputs neurons  7 outputs neurons 0.005 learning rate \u001b[0m\n",
      " hidden layer 7 neurons 1 layers \n",
      "\u001b[1m 7 last hidden layer neurons, 1 output neurons \u001b[0m\n",
      "\u001b[1m Input layer: 7 inputs neurons  7 outputs neurons 0.005 learning rate \u001b[0m\n",
      " hidden layer 7 neurons 1 layers \n",
      "\u001b[1m 7 last hidden layer neurons, 1 output neurons \u001b[0m\n",
      "\u001b[1m Input layer: 7 inputs neurons  7 outputs neurons 0.005 learning rate \u001b[0m\n",
      " hidden layer 7 neurons 1 layers \n",
      "\u001b[1m 7 last hidden layer neurons, 1 output neurons \u001b[0m\n",
      "\u001b[1m Input layer: 7 inputs neurons  7 outputs neurons 0.005 learning rate \u001b[0m\n",
      " hidden layer 7 neurons 1 layers \n",
      "\u001b[1m 7 last hidden layer neurons, 1 output neurons \u001b[0m\n",
      "\u001b[1m Input layer: 7 inputs neurons  7 outputs neurons 0.005 learning rate \u001b[0m\n",
      " hidden layer 7 neurons 1 layers \n",
      "\u001b[1m 7 last hidden layer neurons, 1 output neurons \u001b[0m\n",
      "\u001b[1m Input layer: 7 inputs neurons  7 outputs neurons 0.01 learning rate \u001b[0m\n",
      " hidden layer 7 neurons 1 layers \n",
      "\u001b[1m 7 last hidden layer neurons, 1 output neurons \u001b[0m\n",
      "\u001b[1m Input layer: 7 inputs neurons  7 outputs neurons 0.01 learning rate \u001b[0m\n",
      " hidden layer 7 neurons 1 layers \n",
      "\u001b[1m 7 last hidden layer neurons, 1 output neurons \u001b[0m\n",
      "\u001b[1m Input layer: 7 inputs neurons  7 outputs neurons 0.01 learning rate \u001b[0m\n",
      " hidden layer 7 neurons 1 layers \n",
      "\u001b[1m 7 last hidden layer neurons, 1 output neurons \u001b[0m\n",
      "\u001b[1m Input layer: 7 inputs neurons  7 outputs neurons 0.01 learning rate \u001b[0m\n",
      " hidden layer 7 neurons 1 layers \n",
      "\u001b[1m 7 last hidden layer neurons, 1 output neurons \u001b[0m\n",
      "\u001b[1m Input layer: 7 inputs neurons  7 outputs neurons 0.01 learning rate \u001b[0m\n",
      " hidden layer 7 neurons 1 layers \n",
      "\u001b[1m 7 last hidden layer neurons, 1 output neurons \u001b[0m\n",
      "\u001b[1m Input layer: 7 inputs neurons  7 outputs neurons 0.01 learning rate \u001b[0m\n",
      " hidden layer 7 neurons 1 layers \n",
      "\u001b[1m 7 last hidden layer neurons, 1 output neurons \u001b[0m\n",
      "\u001b[1m Input layer: 7 inputs neurons  8 outputs neurons 0.001 learning rate \u001b[0m\n",
      " hidden layer 8 neurons 1 layers \n",
      "\u001b[1m 8 last hidden layer neurons, 1 output neurons \u001b[0m\n",
      "\u001b[1m Input layer: 7 inputs neurons  8 outputs neurons 0.001 learning rate \u001b[0m\n",
      " hidden layer 8 neurons 1 layers \n",
      "\u001b[1m 8 last hidden layer neurons, 1 output neurons \u001b[0m\n",
      "\u001b[1m Input layer: 7 inputs neurons  8 outputs neurons 0.001 learning rate \u001b[0m\n",
      " hidden layer 8 neurons 1 layers \n",
      "\u001b[1m 8 last hidden layer neurons, 1 output neurons \u001b[0m\n",
      "\u001b[1m Input layer: 7 inputs neurons  8 outputs neurons 0.001 learning rate \u001b[0m\n",
      " hidden layer 8 neurons 1 layers \n",
      "\u001b[1m 8 last hidden layer neurons, 1 output neurons \u001b[0m\n",
      "\u001b[1m Input layer: 7 inputs neurons  8 outputs neurons 0.001 learning rate \u001b[0m\n",
      " hidden layer 8 neurons 1 layers \n",
      "\u001b[1m 8 last hidden layer neurons, 1 output neurons \u001b[0m\n",
      "\u001b[1m Input layer: 7 inputs neurons  8 outputs neurons 0.001 learning rate \u001b[0m\n",
      " hidden layer 8 neurons 1 layers \n",
      "\u001b[1m 8 last hidden layer neurons, 1 output neurons \u001b[0m\n",
      "\u001b[1m Input layer: 7 inputs neurons  8 outputs neurons 0.005 learning rate \u001b[0m\n",
      " hidden layer 8 neurons 1 layers \n",
      "\u001b[1m 8 last hidden layer neurons, 1 output neurons \u001b[0m\n",
      "\u001b[1m Input layer: 7 inputs neurons  8 outputs neurons 0.005 learning rate \u001b[0m\n",
      " hidden layer 8 neurons 1 layers \n",
      "\u001b[1m 8 last hidden layer neurons, 1 output neurons \u001b[0m\n",
      "\u001b[1m Input layer: 7 inputs neurons  8 outputs neurons 0.005 learning rate \u001b[0m\n",
      " hidden layer 8 neurons 1 layers \n",
      "\u001b[1m 8 last hidden layer neurons, 1 output neurons \u001b[0m\n",
      "\u001b[1m Input layer: 7 inputs neurons  8 outputs neurons 0.005 learning rate \u001b[0m\n",
      " hidden layer 8 neurons 1 layers \n",
      "\u001b[1m 8 last hidden layer neurons, 1 output neurons \u001b[0m\n",
      "\u001b[1m Input layer: 7 inputs neurons  8 outputs neurons 0.005 learning rate \u001b[0m\n",
      " hidden layer 8 neurons 1 layers \n",
      "\u001b[1m 8 last hidden layer neurons, 1 output neurons \u001b[0m\n",
      "\u001b[1m Input layer: 7 inputs neurons  8 outputs neurons 0.005 learning rate \u001b[0m\n",
      " hidden layer 8 neurons 1 layers \n",
      "\u001b[1m 8 last hidden layer neurons, 1 output neurons \u001b[0m\n",
      "\u001b[1m Input layer: 7 inputs neurons  8 outputs neurons 0.01 learning rate \u001b[0m\n",
      " hidden layer 8 neurons 1 layers \n",
      "\u001b[1m 8 last hidden layer neurons, 1 output neurons \u001b[0m\n",
      "\u001b[1m Input layer: 7 inputs neurons  8 outputs neurons 0.01 learning rate \u001b[0m\n",
      " hidden layer 8 neurons 1 layers \n",
      "\u001b[1m 8 last hidden layer neurons, 1 output neurons \u001b[0m\n",
      "\u001b[1m Input layer: 7 inputs neurons  8 outputs neurons 0.01 learning rate \u001b[0m\n",
      " hidden layer 8 neurons 1 layers \n",
      "\u001b[1m 8 last hidden layer neurons, 1 output neurons \u001b[0m\n",
      "\u001b[1m Input layer: 7 inputs neurons  8 outputs neurons 0.01 learning rate \u001b[0m\n",
      " hidden layer 8 neurons 1 layers \n",
      "\u001b[1m 8 last hidden layer neurons, 1 output neurons \u001b[0m\n",
      "\u001b[1m Input layer: 7 inputs neurons  8 outputs neurons 0.01 learning rate \u001b[0m\n",
      " hidden layer 8 neurons 1 layers \n",
      "\u001b[1m 8 last hidden layer neurons, 1 output neurons \u001b[0m\n",
      "\u001b[1m Input layer: 7 inputs neurons  8 outputs neurons 0.01 learning rate \u001b[0m\n",
      " hidden layer 8 neurons 1 layers \n",
      "\u001b[1m 8 last hidden layer neurons, 1 output neurons \u001b[0m\n",
      "\u001b[1m Input layer: 7 inputs neurons  12 outputs neurons 0.001 learning rate \u001b[0m\n",
      " hidden layer 12 neurons 1 layers \n",
      "\u001b[1m 12 last hidden layer neurons, 1 output neurons \u001b[0m\n",
      "\u001b[1m Input layer: 7 inputs neurons  12 outputs neurons 0.001 learning rate \u001b[0m\n",
      " hidden layer 12 neurons 1 layers \n",
      "\u001b[1m 12 last hidden layer neurons, 1 output neurons \u001b[0m\n",
      "\u001b[1m Input layer: 7 inputs neurons  12 outputs neurons 0.001 learning rate \u001b[0m\n",
      " hidden layer 12 neurons 1 layers \n",
      "\u001b[1m 12 last hidden layer neurons, 1 output neurons \u001b[0m\n",
      "\u001b[1m Input layer: 7 inputs neurons  12 outputs neurons 0.001 learning rate \u001b[0m\n",
      " hidden layer 12 neurons 1 layers \n",
      "\u001b[1m 12 last hidden layer neurons, 1 output neurons \u001b[0m\n",
      "\u001b[1m Input layer: 7 inputs neurons  12 outputs neurons 0.001 learning rate \u001b[0m\n",
      " hidden layer 12 neurons 1 layers \n",
      "\u001b[1m 12 last hidden layer neurons, 1 output neurons \u001b[0m\n",
      "\u001b[1m Input layer: 7 inputs neurons  12 outputs neurons 0.001 learning rate \u001b[0m\n",
      " hidden layer 12 neurons 1 layers \n",
      "\u001b[1m 12 last hidden layer neurons, 1 output neurons \u001b[0m\n",
      "\u001b[1m Input layer: 7 inputs neurons  12 outputs neurons 0.005 learning rate \u001b[0m\n",
      " hidden layer 12 neurons 1 layers \n",
      "\u001b[1m 12 last hidden layer neurons, 1 output neurons \u001b[0m\n",
      "\u001b[1m Input layer: 7 inputs neurons  12 outputs neurons 0.005 learning rate \u001b[0m\n",
      " hidden layer 12 neurons 1 layers \n",
      "\u001b[1m 12 last hidden layer neurons, 1 output neurons \u001b[0m\n",
      "\u001b[1m Input layer: 7 inputs neurons  12 outputs neurons 0.005 learning rate \u001b[0m\n",
      " hidden layer 12 neurons 1 layers \n",
      "\u001b[1m 12 last hidden layer neurons, 1 output neurons \u001b[0m\n",
      "\u001b[1m Input layer: 7 inputs neurons  12 outputs neurons 0.005 learning rate \u001b[0m\n",
      " hidden layer 12 neurons 1 layers \n",
      "\u001b[1m 12 last hidden layer neurons, 1 output neurons \u001b[0m\n",
      "\u001b[1m Input layer: 7 inputs neurons  12 outputs neurons 0.005 learning rate \u001b[0m\n",
      " hidden layer 12 neurons 1 layers \n",
      "\u001b[1m 12 last hidden layer neurons, 1 output neurons \u001b[0m\n",
      "\u001b[1m Input layer: 7 inputs neurons  12 outputs neurons 0.005 learning rate \u001b[0m\n",
      " hidden layer 12 neurons 1 layers \n",
      "\u001b[1m 12 last hidden layer neurons, 1 output neurons \u001b[0m\n",
      "\u001b[1m Input layer: 7 inputs neurons  12 outputs neurons 0.01 learning rate \u001b[0m\n",
      " hidden layer 12 neurons 1 layers \n",
      "\u001b[1m 12 last hidden layer neurons, 1 output neurons \u001b[0m\n",
      "\u001b[1m Input layer: 7 inputs neurons  12 outputs neurons 0.01 learning rate \u001b[0m\n",
      " hidden layer 12 neurons 1 layers \n",
      "\u001b[1m 12 last hidden layer neurons, 1 output neurons \u001b[0m\n",
      "\u001b[1m Input layer: 7 inputs neurons  12 outputs neurons 0.01 learning rate \u001b[0m\n",
      " hidden layer 12 neurons 1 layers \n",
      "\u001b[1m 12 last hidden layer neurons, 1 output neurons \u001b[0m\n",
      "\u001b[1m Input layer: 7 inputs neurons  12 outputs neurons 0.01 learning rate \u001b[0m\n",
      " hidden layer 12 neurons 1 layers \n",
      "\u001b[1m 12 last hidden layer neurons, 1 output neurons \u001b[0m\n",
      "\u001b[1m Input layer: 7 inputs neurons  12 outputs neurons 0.01 learning rate \u001b[0m\n",
      " hidden layer 12 neurons 1 layers \n",
      "\u001b[1m 12 last hidden layer neurons, 1 output neurons \u001b[0m\n",
      "\u001b[1m Input layer: 7 inputs neurons  12 outputs neurons 0.01 learning rate \u001b[0m\n",
      " hidden layer 12 neurons 1 layers \n",
      "\u001b[1m 12 last hidden layer neurons, 1 output neurons \u001b[0m\n",
      "\u001b[1m Input layer: 7 inputs neurons  7 outputs neurons 0.001 learning rate \u001b[0m\n",
      " hidden layer 7 neurons 2 layers \n",
      " hidden layer 7 neurons 2 layers \n",
      "\u001b[1m 7 last hidden layer neurons, 1 output neurons \u001b[0m\n",
      "\u001b[1m Input layer: 7 inputs neurons  7 outputs neurons 0.001 learning rate \u001b[0m\n",
      " hidden layer 7 neurons 2 layers \n",
      " hidden layer 7 neurons 2 layers \n",
      "\u001b[1m 7 last hidden layer neurons, 1 output neurons \u001b[0m\n",
      "\u001b[1m Input layer: 7 inputs neurons  7 outputs neurons 0.001 learning rate \u001b[0m\n",
      " hidden layer 7 neurons 2 layers \n",
      " hidden layer 7 neurons 2 layers \n",
      "\u001b[1m 7 last hidden layer neurons, 1 output neurons \u001b[0m\n",
      "\u001b[1m Input layer: 7 inputs neurons  7 outputs neurons 0.001 learning rate \u001b[0m\n",
      " hidden layer 7 neurons 2 layers \n",
      " hidden layer 7 neurons 2 layers \n",
      "\u001b[1m 7 last hidden layer neurons, 1 output neurons \u001b[0m\n",
      "\u001b[1m Input layer: 7 inputs neurons  7 outputs neurons 0.001 learning rate \u001b[0m\n",
      " hidden layer 7 neurons 2 layers \n",
      " hidden layer 7 neurons 2 layers \n",
      "\u001b[1m 7 last hidden layer neurons, 1 output neurons \u001b[0m\n",
      "\u001b[1m Input layer: 7 inputs neurons  7 outputs neurons 0.001 learning rate \u001b[0m\n",
      " hidden layer 7 neurons 2 layers \n",
      " hidden layer 7 neurons 2 layers \n",
      "\u001b[1m 7 last hidden layer neurons, 1 output neurons \u001b[0m\n",
      "\u001b[1m Input layer: 7 inputs neurons  7 outputs neurons 0.005 learning rate \u001b[0m\n",
      " hidden layer 7 neurons 2 layers \n",
      " hidden layer 7 neurons 2 layers \n",
      "\u001b[1m 7 last hidden layer neurons, 1 output neurons \u001b[0m\n",
      "\u001b[1m Input layer: 7 inputs neurons  7 outputs neurons 0.005 learning rate \u001b[0m\n",
      " hidden layer 7 neurons 2 layers \n",
      " hidden layer 7 neurons 2 layers \n",
      "\u001b[1m 7 last hidden layer neurons, 1 output neurons \u001b[0m\n",
      "\u001b[1m Input layer: 7 inputs neurons  7 outputs neurons 0.005 learning rate \u001b[0m\n",
      " hidden layer 7 neurons 2 layers \n",
      " hidden layer 7 neurons 2 layers \n",
      "\u001b[1m 7 last hidden layer neurons, 1 output neurons \u001b[0m\n",
      "\u001b[1m Input layer: 7 inputs neurons  7 outputs neurons 0.005 learning rate \u001b[0m\n",
      " hidden layer 7 neurons 2 layers \n",
      " hidden layer 7 neurons 2 layers \n",
      "\u001b[1m 7 last hidden layer neurons, 1 output neurons \u001b[0m\n",
      "\u001b[1m Input layer: 7 inputs neurons  7 outputs neurons 0.005 learning rate \u001b[0m\n",
      " hidden layer 7 neurons 2 layers \n",
      " hidden layer 7 neurons 2 layers \n",
      "\u001b[1m 7 last hidden layer neurons, 1 output neurons \u001b[0m\n",
      "\u001b[1m Input layer: 7 inputs neurons  7 outputs neurons 0.005 learning rate \u001b[0m\n",
      " hidden layer 7 neurons 2 layers \n",
      " hidden layer 7 neurons 2 layers \n",
      "\u001b[1m 7 last hidden layer neurons, 1 output neurons \u001b[0m\n",
      "\u001b[1m Input layer: 7 inputs neurons  7 outputs neurons 0.01 learning rate \u001b[0m\n",
      " hidden layer 7 neurons 2 layers \n",
      " hidden layer 7 neurons 2 layers \n",
      "\u001b[1m 7 last hidden layer neurons, 1 output neurons \u001b[0m\n",
      "\u001b[1m Input layer: 7 inputs neurons  7 outputs neurons 0.01 learning rate \u001b[0m\n",
      " hidden layer 7 neurons 2 layers \n",
      " hidden layer 7 neurons 2 layers \n",
      "\u001b[1m 7 last hidden layer neurons, 1 output neurons \u001b[0m\n",
      "\u001b[1m Input layer: 7 inputs neurons  7 outputs neurons 0.01 learning rate \u001b[0m\n",
      " hidden layer 7 neurons 2 layers \n",
      " hidden layer 7 neurons 2 layers \n",
      "\u001b[1m 7 last hidden layer neurons, 1 output neurons \u001b[0m\n",
      "\u001b[1m Input layer: 7 inputs neurons  7 outputs neurons 0.01 learning rate \u001b[0m\n",
      " hidden layer 7 neurons 2 layers \n",
      " hidden layer 7 neurons 2 layers \n",
      "\u001b[1m 7 last hidden layer neurons, 1 output neurons \u001b[0m\n",
      "\u001b[1m Input layer: 7 inputs neurons  7 outputs neurons 0.01 learning rate \u001b[0m\n",
      " hidden layer 7 neurons 2 layers \n",
      " hidden layer 7 neurons 2 layers \n",
      "\u001b[1m 7 last hidden layer neurons, 1 output neurons \u001b[0m\n",
      "\u001b[1m Input layer: 7 inputs neurons  7 outputs neurons 0.01 learning rate \u001b[0m\n",
      " hidden layer 7 neurons 2 layers \n",
      " hidden layer 7 neurons 2 layers \n",
      "\u001b[1m 7 last hidden layer neurons, 1 output neurons \u001b[0m\n",
      "\u001b[1m Input layer: 7 inputs neurons  8 outputs neurons 0.001 learning rate \u001b[0m\n",
      " hidden layer 8 neurons 2 layers \n",
      " hidden layer 8 neurons 2 layers \n",
      "\u001b[1m 8 last hidden layer neurons, 1 output neurons \u001b[0m\n",
      "\u001b[1m Input layer: 7 inputs neurons  8 outputs neurons 0.001 learning rate \u001b[0m\n",
      " hidden layer 8 neurons 2 layers \n",
      " hidden layer 8 neurons 2 layers \n",
      "\u001b[1m 8 last hidden layer neurons, 1 output neurons \u001b[0m\n",
      "\u001b[1m Input layer: 7 inputs neurons  8 outputs neurons 0.001 learning rate \u001b[0m\n",
      " hidden layer 8 neurons 2 layers \n",
      " hidden layer 8 neurons 2 layers \n",
      "\u001b[1m 8 last hidden layer neurons, 1 output neurons \u001b[0m\n",
      "\u001b[1m Input layer: 7 inputs neurons  8 outputs neurons 0.001 learning rate \u001b[0m\n",
      " hidden layer 8 neurons 2 layers \n",
      " hidden layer 8 neurons 2 layers \n",
      "\u001b[1m 8 last hidden layer neurons, 1 output neurons \u001b[0m\n",
      "\u001b[1m Input layer: 7 inputs neurons  8 outputs neurons 0.001 learning rate \u001b[0m\n",
      " hidden layer 8 neurons 2 layers \n",
      " hidden layer 8 neurons 2 layers \n",
      "\u001b[1m 8 last hidden layer neurons, 1 output neurons \u001b[0m\n",
      "\u001b[1m Input layer: 7 inputs neurons  8 outputs neurons 0.001 learning rate \u001b[0m\n",
      " hidden layer 8 neurons 2 layers \n",
      " hidden layer 8 neurons 2 layers \n",
      "\u001b[1m 8 last hidden layer neurons, 1 output neurons \u001b[0m\n",
      "\u001b[1m Input layer: 7 inputs neurons  8 outputs neurons 0.005 learning rate \u001b[0m\n",
      " hidden layer 8 neurons 2 layers \n",
      " hidden layer 8 neurons 2 layers \n",
      "\u001b[1m 8 last hidden layer neurons, 1 output neurons \u001b[0m\n",
      "\u001b[1m Input layer: 7 inputs neurons  8 outputs neurons 0.005 learning rate \u001b[0m\n",
      " hidden layer 8 neurons 2 layers \n",
      " hidden layer 8 neurons 2 layers \n",
      "\u001b[1m 8 last hidden layer neurons, 1 output neurons \u001b[0m\n",
      "\u001b[1m Input layer: 7 inputs neurons  8 outputs neurons 0.005 learning rate \u001b[0m\n",
      " hidden layer 8 neurons 2 layers \n",
      " hidden layer 8 neurons 2 layers \n",
      "\u001b[1m 8 last hidden layer neurons, 1 output neurons \u001b[0m\n",
      "\u001b[1m Input layer: 7 inputs neurons  8 outputs neurons 0.005 learning rate \u001b[0m\n",
      " hidden layer 8 neurons 2 layers \n",
      " hidden layer 8 neurons 2 layers \n",
      "\u001b[1m 8 last hidden layer neurons, 1 output neurons \u001b[0m\n",
      "\u001b[1m Input layer: 7 inputs neurons  8 outputs neurons 0.005 learning rate \u001b[0m\n",
      " hidden layer 8 neurons 2 layers \n",
      " hidden layer 8 neurons 2 layers \n",
      "\u001b[1m 8 last hidden layer neurons, 1 output neurons \u001b[0m\n",
      "\u001b[1m Input layer: 7 inputs neurons  8 outputs neurons 0.005 learning rate \u001b[0m\n",
      " hidden layer 8 neurons 2 layers \n",
      " hidden layer 8 neurons 2 layers \n",
      "\u001b[1m 8 last hidden layer neurons, 1 output neurons \u001b[0m\n",
      "\u001b[1m Input layer: 7 inputs neurons  8 outputs neurons 0.01 learning rate \u001b[0m\n",
      " hidden layer 8 neurons 2 layers \n",
      " hidden layer 8 neurons 2 layers \n",
      "\u001b[1m 8 last hidden layer neurons, 1 output neurons \u001b[0m\n",
      "\u001b[1m Input layer: 7 inputs neurons  8 outputs neurons 0.01 learning rate \u001b[0m\n",
      " hidden layer 8 neurons 2 layers \n",
      " hidden layer 8 neurons 2 layers \n",
      "\u001b[1m 8 last hidden layer neurons, 1 output neurons \u001b[0m\n",
      "\u001b[1m Input layer: 7 inputs neurons  8 outputs neurons 0.01 learning rate \u001b[0m\n",
      " hidden layer 8 neurons 2 layers \n",
      " hidden layer 8 neurons 2 layers \n",
      "\u001b[1m 8 last hidden layer neurons, 1 output neurons \u001b[0m\n",
      "\u001b[1m Input layer: 7 inputs neurons  8 outputs neurons 0.01 learning rate \u001b[0m\n",
      " hidden layer 8 neurons 2 layers \n",
      " hidden layer 8 neurons 2 layers \n",
      "\u001b[1m 8 last hidden layer neurons, 1 output neurons \u001b[0m\n",
      "\u001b[1m Input layer: 7 inputs neurons  8 outputs neurons 0.01 learning rate \u001b[0m\n",
      " hidden layer 8 neurons 2 layers \n",
      " hidden layer 8 neurons 2 layers \n",
      "\u001b[1m 8 last hidden layer neurons, 1 output neurons \u001b[0m\n",
      "\u001b[1m Input layer: 7 inputs neurons  8 outputs neurons 0.01 learning rate \u001b[0m\n",
      " hidden layer 8 neurons 2 layers \n",
      " hidden layer 8 neurons 2 layers \n",
      "\u001b[1m 8 last hidden layer neurons, 1 output neurons \u001b[0m\n",
      "\u001b[1m Input layer: 7 inputs neurons  12 outputs neurons 0.001 learning rate \u001b[0m\n",
      " hidden layer 12 neurons 2 layers \n",
      " hidden layer 12 neurons 2 layers \n",
      "\u001b[1m 12 last hidden layer neurons, 1 output neurons \u001b[0m\n",
      "\u001b[1m Input layer: 7 inputs neurons  12 outputs neurons 0.001 learning rate \u001b[0m\n",
      " hidden layer 12 neurons 2 layers \n",
      " hidden layer 12 neurons 2 layers \n",
      "\u001b[1m 12 last hidden layer neurons, 1 output neurons \u001b[0m\n",
      "\u001b[1m Input layer: 7 inputs neurons  12 outputs neurons 0.001 learning rate \u001b[0m\n",
      " hidden layer 12 neurons 2 layers \n",
      " hidden layer 12 neurons 2 layers \n",
      "\u001b[1m 12 last hidden layer neurons, 1 output neurons \u001b[0m\n",
      "\u001b[1m Input layer: 7 inputs neurons  12 outputs neurons 0.001 learning rate \u001b[0m\n",
      " hidden layer 12 neurons 2 layers \n",
      " hidden layer 12 neurons 2 layers \n",
      "\u001b[1m 12 last hidden layer neurons, 1 output neurons \u001b[0m\n",
      "\u001b[1m Input layer: 7 inputs neurons  12 outputs neurons 0.001 learning rate \u001b[0m\n",
      " hidden layer 12 neurons 2 layers \n",
      " hidden layer 12 neurons 2 layers \n",
      "\u001b[1m 12 last hidden layer neurons, 1 output neurons \u001b[0m\n",
      "\u001b[1m Input layer: 7 inputs neurons  12 outputs neurons 0.001 learning rate \u001b[0m\n",
      " hidden layer 12 neurons 2 layers \n",
      " hidden layer 12 neurons 2 layers \n",
      "\u001b[1m 12 last hidden layer neurons, 1 output neurons \u001b[0m\n",
      "\u001b[1m Input layer: 7 inputs neurons  12 outputs neurons 0.005 learning rate \u001b[0m\n",
      " hidden layer 12 neurons 2 layers \n",
      " hidden layer 12 neurons 2 layers \n",
      "\u001b[1m 12 last hidden layer neurons, 1 output neurons \u001b[0m\n",
      "\u001b[1m Input layer: 7 inputs neurons  12 outputs neurons 0.005 learning rate \u001b[0m\n",
      " hidden layer 12 neurons 2 layers \n",
      " hidden layer 12 neurons 2 layers \n",
      "\u001b[1m 12 last hidden layer neurons, 1 output neurons \u001b[0m\n",
      "\u001b[1m Input layer: 7 inputs neurons  12 outputs neurons 0.005 learning rate \u001b[0m\n",
      " hidden layer 12 neurons 2 layers \n",
      " hidden layer 12 neurons 2 layers \n",
      "\u001b[1m 12 last hidden layer neurons, 1 output neurons \u001b[0m\n",
      "\u001b[1m Input layer: 7 inputs neurons  12 outputs neurons 0.005 learning rate \u001b[0m\n",
      " hidden layer 12 neurons 2 layers \n",
      " hidden layer 12 neurons 2 layers \n",
      "\u001b[1m 12 last hidden layer neurons, 1 output neurons \u001b[0m\n",
      "\u001b[1m Input layer: 7 inputs neurons  12 outputs neurons 0.005 learning rate \u001b[0m\n",
      " hidden layer 12 neurons 2 layers \n",
      " hidden layer 12 neurons 2 layers \n",
      "\u001b[1m 12 last hidden layer neurons, 1 output neurons \u001b[0m\n",
      "\u001b[1m Input layer: 7 inputs neurons  12 outputs neurons 0.005 learning rate \u001b[0m\n",
      " hidden layer 12 neurons 2 layers \n",
      " hidden layer 12 neurons 2 layers \n",
      "\u001b[1m 12 last hidden layer neurons, 1 output neurons \u001b[0m\n",
      "\u001b[1m Input layer: 7 inputs neurons  12 outputs neurons 0.01 learning rate \u001b[0m\n",
      " hidden layer 12 neurons 2 layers \n",
      " hidden layer 12 neurons 2 layers \n",
      "\u001b[1m 12 last hidden layer neurons, 1 output neurons \u001b[0m\n",
      "\u001b[1m Input layer: 7 inputs neurons  12 outputs neurons 0.01 learning rate \u001b[0m\n",
      " hidden layer 12 neurons 2 layers \n",
      " hidden layer 12 neurons 2 layers \n",
      "\u001b[1m 12 last hidden layer neurons, 1 output neurons \u001b[0m\n",
      "\u001b[1m Input layer: 7 inputs neurons  12 outputs neurons 0.01 learning rate \u001b[0m\n",
      " hidden layer 12 neurons 2 layers \n",
      " hidden layer 12 neurons 2 layers \n",
      "\u001b[1m 12 last hidden layer neurons, 1 output neurons \u001b[0m\n",
      "\u001b[1m Input layer: 7 inputs neurons  12 outputs neurons 0.01 learning rate \u001b[0m\n",
      " hidden layer 12 neurons 2 layers \n",
      " hidden layer 12 neurons 2 layers \n",
      "\u001b[1m 12 last hidden layer neurons, 1 output neurons \u001b[0m\n",
      "\u001b[1m Input layer: 7 inputs neurons  12 outputs neurons 0.01 learning rate \u001b[0m\n",
      " hidden layer 12 neurons 2 layers \n",
      " hidden layer 12 neurons 2 layers \n",
      "\u001b[1m 12 last hidden layer neurons, 1 output neurons \u001b[0m\n",
      "\u001b[1m Input layer: 7 inputs neurons  12 outputs neurons 0.01 learning rate \u001b[0m\n",
      " hidden layer 12 neurons 2 layers \n",
      " hidden layer 12 neurons 2 layers \n",
      "\u001b[1m 12 last hidden layer neurons, 1 output neurons \u001b[0m\n",
      "\u001b[1m Input layer: 7 inputs neurons  7 outputs neurons 0.001 learning rate \u001b[0m\n",
      " hidden layer 7 neurons 3 layers \n",
      " hidden layer 7 neurons 3 layers \n",
      " hidden layer 7 neurons 3 layers \n",
      "\u001b[1m 7 last hidden layer neurons, 1 output neurons \u001b[0m\n",
      "\u001b[1m Input layer: 7 inputs neurons  7 outputs neurons 0.001 learning rate \u001b[0m\n",
      " hidden layer 7 neurons 3 layers \n",
      " hidden layer 7 neurons 3 layers \n",
      " hidden layer 7 neurons 3 layers \n",
      "\u001b[1m 7 last hidden layer neurons, 1 output neurons \u001b[0m\n",
      "\u001b[1m Input layer: 7 inputs neurons  7 outputs neurons 0.001 learning rate \u001b[0m\n",
      " hidden layer 7 neurons 3 layers \n",
      " hidden layer 7 neurons 3 layers \n",
      " hidden layer 7 neurons 3 layers \n",
      "\u001b[1m 7 last hidden layer neurons, 1 output neurons \u001b[0m\n",
      "\u001b[1m Input layer: 7 inputs neurons  7 outputs neurons 0.001 learning rate \u001b[0m\n",
      " hidden layer 7 neurons 3 layers \n",
      " hidden layer 7 neurons 3 layers \n",
      " hidden layer 7 neurons 3 layers \n",
      "\u001b[1m 7 last hidden layer neurons, 1 output neurons \u001b[0m\n",
      "\u001b[1m Input layer: 7 inputs neurons  7 outputs neurons 0.001 learning rate \u001b[0m\n",
      " hidden layer 7 neurons 3 layers \n",
      " hidden layer 7 neurons 3 layers \n",
      " hidden layer 7 neurons 3 layers \n",
      "\u001b[1m 7 last hidden layer neurons, 1 output neurons \u001b[0m\n",
      "\u001b[1m Input layer: 7 inputs neurons  7 outputs neurons 0.001 learning rate \u001b[0m\n",
      " hidden layer 7 neurons 3 layers \n",
      " hidden layer 7 neurons 3 layers \n",
      " hidden layer 7 neurons 3 layers \n",
      "\u001b[1m 7 last hidden layer neurons, 1 output neurons \u001b[0m\n",
      "\u001b[1m Input layer: 7 inputs neurons  7 outputs neurons 0.005 learning rate \u001b[0m\n",
      " hidden layer 7 neurons 3 layers \n",
      " hidden layer 7 neurons 3 layers \n",
      " hidden layer 7 neurons 3 layers \n",
      "\u001b[1m 7 last hidden layer neurons, 1 output neurons \u001b[0m\n",
      "\u001b[1m Input layer: 7 inputs neurons  7 outputs neurons 0.005 learning rate \u001b[0m\n",
      " hidden layer 7 neurons 3 layers \n",
      " hidden layer 7 neurons 3 layers \n",
      " hidden layer 7 neurons 3 layers \n",
      "\u001b[1m 7 last hidden layer neurons, 1 output neurons \u001b[0m\n",
      "\u001b[1m Input layer: 7 inputs neurons  7 outputs neurons 0.005 learning rate \u001b[0m\n",
      " hidden layer 7 neurons 3 layers \n",
      " hidden layer 7 neurons 3 layers \n",
      " hidden layer 7 neurons 3 layers \n",
      "\u001b[1m 7 last hidden layer neurons, 1 output neurons \u001b[0m\n",
      "\u001b[1m Input layer: 7 inputs neurons  7 outputs neurons 0.005 learning rate \u001b[0m\n",
      " hidden layer 7 neurons 3 layers \n",
      " hidden layer 7 neurons 3 layers \n",
      " hidden layer 7 neurons 3 layers \n",
      "\u001b[1m 7 last hidden layer neurons, 1 output neurons \u001b[0m\n",
      "\u001b[1m Input layer: 7 inputs neurons  7 outputs neurons 0.005 learning rate \u001b[0m\n",
      " hidden layer 7 neurons 3 layers \n",
      " hidden layer 7 neurons 3 layers \n",
      " hidden layer 7 neurons 3 layers \n",
      "\u001b[1m 7 last hidden layer neurons, 1 output neurons \u001b[0m\n",
      "\u001b[1m Input layer: 7 inputs neurons  7 outputs neurons 0.005 learning rate \u001b[0m\n",
      " hidden layer 7 neurons 3 layers \n",
      " hidden layer 7 neurons 3 layers \n",
      " hidden layer 7 neurons 3 layers \n",
      "\u001b[1m 7 last hidden layer neurons, 1 output neurons \u001b[0m\n",
      "\u001b[1m Input layer: 7 inputs neurons  7 outputs neurons 0.01 learning rate \u001b[0m\n",
      " hidden layer 7 neurons 3 layers \n",
      " hidden layer 7 neurons 3 layers \n",
      " hidden layer 7 neurons 3 layers \n",
      "\u001b[1m 7 last hidden layer neurons, 1 output neurons \u001b[0m\n",
      "\u001b[1m Input layer: 7 inputs neurons  7 outputs neurons 0.01 learning rate \u001b[0m\n",
      " hidden layer 7 neurons 3 layers \n",
      " hidden layer 7 neurons 3 layers \n",
      " hidden layer 7 neurons 3 layers \n",
      "\u001b[1m 7 last hidden layer neurons, 1 output neurons \u001b[0m\n",
      "\u001b[1m Input layer: 7 inputs neurons  7 outputs neurons 0.01 learning rate \u001b[0m\n",
      " hidden layer 7 neurons 3 layers \n",
      " hidden layer 7 neurons 3 layers \n",
      " hidden layer 7 neurons 3 layers \n",
      "\u001b[1m 7 last hidden layer neurons, 1 output neurons \u001b[0m\n",
      "\u001b[1m Input layer: 7 inputs neurons  7 outputs neurons 0.01 learning rate \u001b[0m\n",
      " hidden layer 7 neurons 3 layers \n",
      " hidden layer 7 neurons 3 layers \n",
      " hidden layer 7 neurons 3 layers \n",
      "\u001b[1m 7 last hidden layer neurons, 1 output neurons \u001b[0m\n",
      "\u001b[1m Input layer: 7 inputs neurons  7 outputs neurons 0.01 learning rate \u001b[0m\n",
      " hidden layer 7 neurons 3 layers \n",
      " hidden layer 7 neurons 3 layers \n",
      " hidden layer 7 neurons 3 layers \n",
      "\u001b[1m 7 last hidden layer neurons, 1 output neurons \u001b[0m\n",
      "\u001b[1m Input layer: 7 inputs neurons  7 outputs neurons 0.01 learning rate \u001b[0m\n",
      " hidden layer 7 neurons 3 layers \n",
      " hidden layer 7 neurons 3 layers \n",
      " hidden layer 7 neurons 3 layers \n",
      "\u001b[1m 7 last hidden layer neurons, 1 output neurons \u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m Input layer: 7 inputs neurons  8 outputs neurons 0.001 learning rate \u001b[0m\n",
      " hidden layer 8 neurons 3 layers \n",
      " hidden layer 8 neurons 3 layers \n",
      " hidden layer 8 neurons 3 layers \n",
      "\u001b[1m 8 last hidden layer neurons, 1 output neurons \u001b[0m\n",
      "\u001b[1m Input layer: 7 inputs neurons  8 outputs neurons 0.001 learning rate \u001b[0m\n",
      " hidden layer 8 neurons 3 layers \n",
      " hidden layer 8 neurons 3 layers \n",
      " hidden layer 8 neurons 3 layers \n",
      "\u001b[1m 8 last hidden layer neurons, 1 output neurons \u001b[0m\n",
      "\u001b[1m Input layer: 7 inputs neurons  8 outputs neurons 0.001 learning rate \u001b[0m\n",
      " hidden layer 8 neurons 3 layers \n",
      " hidden layer 8 neurons 3 layers \n",
      " hidden layer 8 neurons 3 layers \n",
      "\u001b[1m 8 last hidden layer neurons, 1 output neurons \u001b[0m\n",
      "\u001b[1m Input layer: 7 inputs neurons  8 outputs neurons 0.001 learning rate \u001b[0m\n",
      " hidden layer 8 neurons 3 layers \n",
      " hidden layer 8 neurons 3 layers \n",
      " hidden layer 8 neurons 3 layers \n",
      "\u001b[1m 8 last hidden layer neurons, 1 output neurons \u001b[0m\n",
      "\u001b[1m Input layer: 7 inputs neurons  8 outputs neurons 0.001 learning rate \u001b[0m\n",
      " hidden layer 8 neurons 3 layers \n",
      " hidden layer 8 neurons 3 layers \n",
      " hidden layer 8 neurons 3 layers \n",
      "\u001b[1m 8 last hidden layer neurons, 1 output neurons \u001b[0m\n",
      "\u001b[1m Input layer: 7 inputs neurons  8 outputs neurons 0.001 learning rate \u001b[0m\n",
      " hidden layer 8 neurons 3 layers \n",
      " hidden layer 8 neurons 3 layers \n",
      " hidden layer 8 neurons 3 layers \n",
      "\u001b[1m 8 last hidden layer neurons, 1 output neurons \u001b[0m\n",
      "\u001b[1m Input layer: 7 inputs neurons  8 outputs neurons 0.005 learning rate \u001b[0m\n",
      " hidden layer 8 neurons 3 layers \n",
      " hidden layer 8 neurons 3 layers \n",
      " hidden layer 8 neurons 3 layers \n",
      "\u001b[1m 8 last hidden layer neurons, 1 output neurons \u001b[0m\n",
      "\u001b[1m Input layer: 7 inputs neurons  8 outputs neurons 0.005 learning rate \u001b[0m\n",
      " hidden layer 8 neurons 3 layers \n",
      " hidden layer 8 neurons 3 layers \n",
      " hidden layer 8 neurons 3 layers \n",
      "\u001b[1m 8 last hidden layer neurons, 1 output neurons \u001b[0m\n",
      "\u001b[1m Input layer: 7 inputs neurons  8 outputs neurons 0.005 learning rate \u001b[0m\n",
      " hidden layer 8 neurons 3 layers \n",
      " hidden layer 8 neurons 3 layers \n",
      " hidden layer 8 neurons 3 layers \n",
      "\u001b[1m 8 last hidden layer neurons, 1 output neurons \u001b[0m\n",
      "\u001b[1m Input layer: 7 inputs neurons  8 outputs neurons 0.005 learning rate \u001b[0m\n",
      " hidden layer 8 neurons 3 layers \n",
      " hidden layer 8 neurons 3 layers \n",
      " hidden layer 8 neurons 3 layers \n",
      "\u001b[1m 8 last hidden layer neurons, 1 output neurons \u001b[0m\n",
      "\u001b[1m Input layer: 7 inputs neurons  8 outputs neurons 0.005 learning rate \u001b[0m\n",
      " hidden layer 8 neurons 3 layers \n",
      " hidden layer 8 neurons 3 layers \n",
      " hidden layer 8 neurons 3 layers \n",
      "\u001b[1m 8 last hidden layer neurons, 1 output neurons \u001b[0m\n",
      "\u001b[1m Input layer: 7 inputs neurons  8 outputs neurons 0.005 learning rate \u001b[0m\n",
      " hidden layer 8 neurons 3 layers \n",
      " hidden layer 8 neurons 3 layers \n",
      " hidden layer 8 neurons 3 layers \n",
      "\u001b[1m 8 last hidden layer neurons, 1 output neurons \u001b[0m\n",
      "\u001b[1m Input layer: 7 inputs neurons  8 outputs neurons 0.01 learning rate \u001b[0m\n",
      " hidden layer 8 neurons 3 layers \n",
      " hidden layer 8 neurons 3 layers \n",
      " hidden layer 8 neurons 3 layers \n",
      "\u001b[1m 8 last hidden layer neurons, 1 output neurons \u001b[0m\n",
      "\u001b[1m Input layer: 7 inputs neurons  8 outputs neurons 0.01 learning rate \u001b[0m\n",
      " hidden layer 8 neurons 3 layers \n",
      " hidden layer 8 neurons 3 layers \n",
      " hidden layer 8 neurons 3 layers \n",
      "\u001b[1m 8 last hidden layer neurons, 1 output neurons \u001b[0m\n",
      "\u001b[1m Input layer: 7 inputs neurons  8 outputs neurons 0.01 learning rate \u001b[0m\n",
      " hidden layer 8 neurons 3 layers \n",
      " hidden layer 8 neurons 3 layers \n",
      " hidden layer 8 neurons 3 layers \n",
      "\u001b[1m 8 last hidden layer neurons, 1 output neurons \u001b[0m\n",
      "\u001b[1m Input layer: 7 inputs neurons  8 outputs neurons 0.01 learning rate \u001b[0m\n",
      " hidden layer 8 neurons 3 layers \n",
      " hidden layer 8 neurons 3 layers \n",
      " hidden layer 8 neurons 3 layers \n",
      "\u001b[1m 8 last hidden layer neurons, 1 output neurons \u001b[0m\n",
      "\u001b[1m Input layer: 7 inputs neurons  8 outputs neurons 0.01 learning rate \u001b[0m\n",
      " hidden layer 8 neurons 3 layers \n",
      " hidden layer 8 neurons 3 layers \n",
      " hidden layer 8 neurons 3 layers \n",
      "\u001b[1m 8 last hidden layer neurons, 1 output neurons \u001b[0m\n",
      "\u001b[1m Input layer: 7 inputs neurons  8 outputs neurons 0.01 learning rate \u001b[0m\n",
      " hidden layer 8 neurons 3 layers \n",
      " hidden layer 8 neurons 3 layers \n",
      " hidden layer 8 neurons 3 layers \n",
      "\u001b[1m 8 last hidden layer neurons, 1 output neurons \u001b[0m\n",
      "\u001b[1m Input layer: 7 inputs neurons  12 outputs neurons 0.001 learning rate \u001b[0m\n",
      " hidden layer 12 neurons 3 layers \n",
      " hidden layer 12 neurons 3 layers \n",
      " hidden layer 12 neurons 3 layers \n",
      "\u001b[1m 12 last hidden layer neurons, 1 output neurons \u001b[0m\n",
      "\u001b[1m Input layer: 7 inputs neurons  12 outputs neurons 0.001 learning rate \u001b[0m\n",
      " hidden layer 12 neurons 3 layers \n",
      " hidden layer 12 neurons 3 layers \n",
      " hidden layer 12 neurons 3 layers \n",
      "\u001b[1m 12 last hidden layer neurons, 1 output neurons \u001b[0m\n",
      "\u001b[1m Input layer: 7 inputs neurons  12 outputs neurons 0.001 learning rate \u001b[0m\n",
      " hidden layer 12 neurons 3 layers \n",
      " hidden layer 12 neurons 3 layers \n",
      " hidden layer 12 neurons 3 layers \n",
      "\u001b[1m 12 last hidden layer neurons, 1 output neurons \u001b[0m\n",
      "\u001b[1m Input layer: 7 inputs neurons  12 outputs neurons 0.001 learning rate \u001b[0m\n",
      " hidden layer 12 neurons 3 layers \n",
      " hidden layer 12 neurons 3 layers \n",
      " hidden layer 12 neurons 3 layers \n",
      "\u001b[1m 12 last hidden layer neurons, 1 output neurons \u001b[0m\n",
      "\u001b[1m Input layer: 7 inputs neurons  12 outputs neurons 0.001 learning rate \u001b[0m\n",
      " hidden layer 12 neurons 3 layers \n",
      " hidden layer 12 neurons 3 layers \n",
      " hidden layer 12 neurons 3 layers \n",
      "\u001b[1m 12 last hidden layer neurons, 1 output neurons \u001b[0m\n",
      "\u001b[1m Input layer: 7 inputs neurons  12 outputs neurons 0.001 learning rate \u001b[0m\n",
      " hidden layer 12 neurons 3 layers \n",
      " hidden layer 12 neurons 3 layers \n",
      " hidden layer 12 neurons 3 layers \n",
      "\u001b[1m 12 last hidden layer neurons, 1 output neurons \u001b[0m\n",
      "\u001b[1m Input layer: 7 inputs neurons  12 outputs neurons 0.005 learning rate \u001b[0m\n",
      " hidden layer 12 neurons 3 layers \n",
      " hidden layer 12 neurons 3 layers \n",
      " hidden layer 12 neurons 3 layers \n",
      "\u001b[1m 12 last hidden layer neurons, 1 output neurons \u001b[0m\n",
      "\u001b[1m Input layer: 7 inputs neurons  12 outputs neurons 0.005 learning rate \u001b[0m\n",
      " hidden layer 12 neurons 3 layers \n",
      " hidden layer 12 neurons 3 layers \n",
      " hidden layer 12 neurons 3 layers \n",
      "\u001b[1m 12 last hidden layer neurons, 1 output neurons \u001b[0m\n",
      "\u001b[1m Input layer: 7 inputs neurons  12 outputs neurons 0.005 learning rate \u001b[0m\n",
      " hidden layer 12 neurons 3 layers \n",
      " hidden layer 12 neurons 3 layers \n",
      " hidden layer 12 neurons 3 layers \n",
      "\u001b[1m 12 last hidden layer neurons, 1 output neurons \u001b[0m\n",
      "\u001b[1m Input layer: 7 inputs neurons  12 outputs neurons 0.005 learning rate \u001b[0m\n",
      " hidden layer 12 neurons 3 layers \n",
      " hidden layer 12 neurons 3 layers \n",
      " hidden layer 12 neurons 3 layers \n",
      "\u001b[1m 12 last hidden layer neurons, 1 output neurons \u001b[0m\n",
      "\u001b[1m Input layer: 7 inputs neurons  12 outputs neurons 0.005 learning rate \u001b[0m\n",
      " hidden layer 12 neurons 3 layers \n",
      " hidden layer 12 neurons 3 layers \n",
      " hidden layer 12 neurons 3 layers \n",
      "\u001b[1m 12 last hidden layer neurons, 1 output neurons \u001b[0m\n",
      "\u001b[1m Input layer: 7 inputs neurons  12 outputs neurons 0.005 learning rate \u001b[0m\n",
      " hidden layer 12 neurons 3 layers \n",
      " hidden layer 12 neurons 3 layers \n",
      " hidden layer 12 neurons 3 layers \n",
      "\u001b[1m 12 last hidden layer neurons, 1 output neurons \u001b[0m\n",
      "\u001b[1m Input layer: 7 inputs neurons  12 outputs neurons 0.01 learning rate \u001b[0m\n",
      " hidden layer 12 neurons 3 layers \n",
      " hidden layer 12 neurons 3 layers \n",
      " hidden layer 12 neurons 3 layers \n",
      "\u001b[1m 12 last hidden layer neurons, 1 output neurons \u001b[0m\n",
      "\u001b[1m Input layer: 7 inputs neurons  12 outputs neurons 0.01 learning rate \u001b[0m\n",
      " hidden layer 12 neurons 3 layers \n",
      " hidden layer 12 neurons 3 layers \n",
      " hidden layer 12 neurons 3 layers \n",
      "\u001b[1m 12 last hidden layer neurons, 1 output neurons \u001b[0m\n",
      "\u001b[1m Input layer: 7 inputs neurons  12 outputs neurons 0.01 learning rate \u001b[0m\n",
      " hidden layer 12 neurons 3 layers \n",
      " hidden layer 12 neurons 3 layers \n",
      " hidden layer 12 neurons 3 layers \n",
      "\u001b[1m 12 last hidden layer neurons, 1 output neurons \u001b[0m\n",
      "\u001b[1m Input layer: 7 inputs neurons  12 outputs neurons 0.01 learning rate \u001b[0m\n",
      " hidden layer 12 neurons 3 layers \n",
      " hidden layer 12 neurons 3 layers \n",
      " hidden layer 12 neurons 3 layers \n",
      "\u001b[1m 12 last hidden layer neurons, 1 output neurons \u001b[0m\n",
      "\u001b[1m Input layer: 7 inputs neurons  12 outputs neurons 0.01 learning rate \u001b[0m\n",
      " hidden layer 12 neurons 3 layers \n",
      " hidden layer 12 neurons 3 layers \n",
      " hidden layer 12 neurons 3 layers \n",
      "\u001b[1m 12 last hidden layer neurons, 1 output neurons \u001b[0m\n",
      "\u001b[1m Input layer: 7 inputs neurons  12 outputs neurons 0.01 learning rate \u001b[0m\n",
      " hidden layer 12 neurons 3 layers \n",
      " hidden layer 12 neurons 3 layers \n",
      " hidden layer 12 neurons 3 layers \n",
      "\u001b[1m 12 last hidden layer neurons, 1 output neurons \u001b[0m\n"
     ]
    }
   ],
   "source": [
    "for num_hidden_layers in hidden_layers:\n",
    "    for num_neurons_layers in neurons:\n",
    "        for num_learning_rate in learning_rate:\n",
    "            for num_batch_size in batch_size:\n",
    "                for prob_dropout in dropout_rate:\n",
    "                    setTestNeuralNetwork(num_hidden_layers,num_neurons_layers, num_learning_rate, prob_dropout, input_dim, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:    0/40000 loss_train: 1.56831541 + 0.00000000 = 1.56831541 loss_val = 1.56831541\n",
      "epoch: 3000/40000 loss_train: 0.41321902 + 0.00000000 = 0.41321902 loss_val = 0.41321902\n",
      "epoch: 6000/40000 loss_train: 0.28665351 + 0.00000000 = 0.28665351 loss_val = 0.28665351\n",
      "epoch: 9000/40000 loss_train: 0.19038210 + 0.00000000 = 0.19038210 loss_val = 0.19038210\n",
      "epoch: 12000/40000 loss_train: 0.11219534 + 0.00000000 = 0.11219534 loss_val = 0.11219534\n",
      "epoch: 15000/40000 loss_train: 0.08372737 + 0.00000000 = 0.08372737 loss_val = 0.08372737\n",
      "epoch: 18000/40000 loss_train: 0.06918236 + 0.00000000 = 0.06918236 loss_val = 0.06918236\n",
      "epoch: 21000/40000 loss_train: 0.06031171 + 0.00000000 = 0.06031171 loss_val = 0.06031171\n",
      "epoch: 24000/40000 loss_train: 0.05414765 + 0.00000000 = 0.05414765 loss_val = 0.05414765\n",
      "epoch: 27000/40000 loss_train: 0.04950660 + 0.00000000 = 0.04950660 loss_val = 0.04950660\n",
      "epoch: 30000/40000 loss_train: 0.04581219 + 0.00000000 = 0.04581219 loss_val = 0.04581219\n",
      "epoch: 33000/40000 loss_train: 0.04274917 + 0.00000000 = 0.04274917 loss_val = 0.04274917\n",
      "epoch: 36000/40000 loss_train: 0.04013955 + 0.00000000 = 0.04013955 loss_val = 0.04013955\n",
      "epoch: 39000/40000 loss_train: 0.03785248 + 0.00000000 = 0.03785248 loss_val = 0.03785248\n",
      "To 7 neurons, 1 layer(s),  0.001 learning rate, 0 batch size, 0 dropout rate, we have: \u001b[1m Accuracy: 100.00% \u001b[0m\n",
      "epoch:    0/40000 loss_train: 0.91012226 + 0.00000000 = 0.91012226 loss_val = 0.91012226\n",
      "epoch: 3000/40000 loss_train: 0.46791986 + 0.00000000 = 0.46791986 loss_val = 0.46791986\n",
      "epoch: 6000/40000 loss_train: 0.28329791 + 0.00000000 = 0.28329791 loss_val = 0.28329791\n",
      "epoch: 9000/40000 loss_train: 0.18969636 + 0.00000000 = 0.18969636 loss_val = 0.18969636\n",
      "epoch: 12000/40000 loss_train: 0.14605817 + 0.00000000 = 0.14605817 loss_val = 0.14605817\n",
      "epoch: 15000/40000 loss_train: 0.12236618 + 0.00000000 = 0.12236618 loss_val = 0.12236618\n",
      "epoch: 18000/40000 loss_train: 0.10765342 + 0.00000000 = 0.10765342 loss_val = 0.10765342\n",
      "epoch: 21000/40000 loss_train: 0.09728876 + 0.00000000 = 0.09728876 loss_val = 0.09728876\n",
      "epoch: 24000/40000 loss_train: 0.08911652 + 0.00000000 = 0.08911652 loss_val = 0.08911652\n",
      "epoch: 27000/40000 loss_train: 0.08253245 + 0.00000000 = 0.08253245 loss_val = 0.08253245\n",
      "epoch: 30000/40000 loss_train: 0.07732384 + 0.00000000 = 0.07732384 loss_val = 0.07732384\n",
      "epoch: 33000/40000 loss_train: 0.07287307 + 0.00000000 = 0.07287307 loss_val = 0.07287307\n",
      "epoch: 36000/40000 loss_train: 0.06912891 + 0.00000000 = 0.06912891 loss_val = 0.06912891\n",
      "epoch: 39000/40000 loss_train: 0.06596268 + 0.00000000 = 0.06596268 loss_val = 0.06596268\n",
      "To 7 neurons, 1 layer(s),  0.001 learning rate, 0 batch size, 0.1 dropout rate, we have: \u001b[1m Accuracy: 97.62% \u001b[0m\n",
      "epoch:    0/40000 loss_train: 0.99804804 + 0.00000000 = 0.99804804 loss_val = 0.99804804\n",
      "epoch: 3000/40000 loss_train: 0.61850257 + 0.00000000 = 0.61850257 loss_val = 0.61850257\n",
      "epoch: 6000/40000 loss_train: 0.48678596 + 0.00000000 = 0.48678596 loss_val = 0.48678596\n",
      "epoch: 9000/40000 loss_train: 0.30185569 + 0.00000000 = 0.30185569 loss_val = 0.30185569\n",
      "epoch: 12000/40000 loss_train: 0.19015214 + 0.00000000 = 0.19015214 loss_val = 0.19015214\n",
      "epoch: 15000/40000 loss_train: 0.13881688 + 0.00000000 = 0.13881688 loss_val = 0.13881688\n",
      "epoch: 18000/40000 loss_train: 0.11431970 + 0.00000000 = 0.11431970 loss_val = 0.11431970\n",
      "epoch: 21000/40000 loss_train: 0.10079992 + 0.00000000 = 0.10079992 loss_val = 0.10079992\n",
      "epoch: 24000/40000 loss_train: 0.09228596 + 0.00000000 = 0.09228596 loss_val = 0.09228596\n",
      "epoch: 27000/40000 loss_train: 0.08616099 + 0.00000000 = 0.08616099 loss_val = 0.08616099\n",
      "epoch: 30000/40000 loss_train: 0.08138613 + 0.00000000 = 0.08138613 loss_val = 0.08138613\n",
      "epoch: 33000/40000 loss_train: 0.07746977 + 0.00000000 = 0.07746977 loss_val = 0.07746977\n",
      "epoch: 36000/40000 loss_train: 0.07414171 + 0.00000000 = 0.07414171 loss_val = 0.07414171\n",
      "epoch: 39000/40000 loss_train: 0.07121779 + 0.00000000 = 0.07121779 loss_val = 0.07121779\n",
      "To 7 neurons, 1 layer(s),  0.001 learning rate, 4 batch size, 0 dropout rate, we have: \u001b[1m Accuracy: 100.00% \u001b[0m\n",
      "epoch:    0/40000 loss_train: 0.81111180 + 0.00000000 = 0.81111180 loss_val = 0.81111180\n",
      "epoch: 3000/40000 loss_train: 0.38147303 + 0.00000000 = 0.38147303 loss_val = 0.38147303\n",
      "epoch: 6000/40000 loss_train: 0.23476256 + 0.00000000 = 0.23476256 loss_val = 0.23476256\n",
      "epoch: 9000/40000 loss_train: 0.16761921 + 0.00000000 = 0.16761921 loss_val = 0.16761921\n",
      "epoch: 12000/40000 loss_train: 0.12534964 + 0.00000000 = 0.12534964 loss_val = 0.12534964\n",
      "epoch: 15000/40000 loss_train: 0.10357961 + 0.00000000 = 0.10357961 loss_val = 0.10357961\n",
      "epoch: 18000/40000 loss_train: 0.09009638 + 0.00000000 = 0.09009638 loss_val = 0.09009638\n",
      "epoch: 21000/40000 loss_train: 0.08023715 + 0.00000000 = 0.08023715 loss_val = 0.08023715\n",
      "epoch: 24000/40000 loss_train: 0.07329542 + 0.00000000 = 0.07329542 loss_val = 0.07329542\n",
      "epoch: 27000/40000 loss_train: 0.06794837 + 0.00000000 = 0.06794837 loss_val = 0.06794837\n",
      "epoch: 30000/40000 loss_train: 0.06351625 + 0.00000000 = 0.06351625 loss_val = 0.06351625\n",
      "epoch: 33000/40000 loss_train: 0.05979182 + 0.00000000 = 0.05979182 loss_val = 0.05979182\n",
      "epoch: 36000/40000 loss_train: 0.05660431 + 0.00000000 = 0.05660431 loss_val = 0.05660431\n",
      "epoch: 39000/40000 loss_train: 0.05381416 + 0.00000000 = 0.05381416 loss_val = 0.05381416\n",
      "To 7 neurons, 1 layer(s),  0.001 learning rate, 4 batch size, 0.1 dropout rate, we have: \u001b[1m Accuracy: 97.62% \u001b[0m\n",
      "epoch:    0/40000 loss_train: 1.04178597 + 0.00000000 = 1.04178597 loss_val = 1.04178597\n",
      "epoch: 3000/40000 loss_train: 0.67476009 + 0.00000000 = 0.67476009 loss_val = 0.67476009\n",
      "epoch: 6000/40000 loss_train: 0.65590987 + 0.00000000 = 0.65590987 loss_val = 0.65590987\n",
      "epoch: 9000/40000 loss_train: 0.63845682 + 0.00000000 = 0.63845682 loss_val = 0.63845682\n",
      "epoch: 12000/40000 loss_train: 0.61581374 + 0.00000000 = 0.61581374 loss_val = 0.61581374\n",
      "epoch: 15000/40000 loss_train: 0.58231636 + 0.00000000 = 0.58231636 loss_val = 0.58231636\n",
      "epoch: 18000/40000 loss_train: 0.51668799 + 0.00000000 = 0.51668799 loss_val = 0.51668799\n",
      "epoch: 21000/40000 loss_train: 0.40980472 + 0.00000000 = 0.40980472 loss_val = 0.40980472\n",
      "epoch: 24000/40000 loss_train: 0.30146121 + 0.00000000 = 0.30146121 loss_val = 0.30146121\n",
      "epoch: 27000/40000 loss_train: 0.22859122 + 0.00000000 = 0.22859122 loss_val = 0.22859122\n",
      "epoch: 30000/40000 loss_train: 0.18444949 + 0.00000000 = 0.18444949 loss_val = 0.18444949\n",
      "epoch: 33000/40000 loss_train: 0.15745350 + 0.00000000 = 0.15745350 loss_val = 0.15745350\n",
      "epoch: 36000/40000 loss_train: 0.11808994 + 0.00000000 = 0.11808994 loss_val = 0.11808994\n",
      "epoch: 39000/40000 loss_train: 0.10142280 + 0.00000000 = 0.10142280 loss_val = 0.10142280\n",
      "To 7 neurons, 1 layer(s),  0.001 learning rate, 8 batch size, 0 dropout rate, we have: \u001b[1m Accuracy: 97.62% \u001b[0m\n",
      "epoch:    0/40000 loss_train: 0.75866877 + 0.00000000 = 0.75866877 loss_val = 0.75866877\n",
      "epoch: 3000/40000 loss_train: 0.52183442 + 0.00000000 = 0.52183442 loss_val = 0.52183442\n",
      "epoch: 6000/40000 loss_train: 0.38680775 + 0.00000000 = 0.38680775 loss_val = 0.38680775\n",
      "epoch: 9000/40000 loss_train: 0.30406150 + 0.00000000 = 0.30406150 loss_val = 0.30406150\n",
      "epoch: 12000/40000 loss_train: 0.25360646 + 0.00000000 = 0.25360646 loss_val = 0.25360646\n",
      "epoch: 15000/40000 loss_train: 0.22038834 + 0.00000000 = 0.22038834 loss_val = 0.22038834\n",
      "epoch: 18000/40000 loss_train: 0.18938288 + 0.00000000 = 0.18938288 loss_val = 0.18938288\n",
      "epoch: 21000/40000 loss_train: 0.15553194 + 0.00000000 = 0.15553194 loss_val = 0.15553194\n",
      "epoch: 24000/40000 loss_train: 0.12843617 + 0.00000000 = 0.12843617 loss_val = 0.12843617\n",
      "epoch: 27000/40000 loss_train: 0.10799442 + 0.00000000 = 0.10799442 loss_val = 0.10799442\n",
      "epoch: 30000/40000 loss_train: 0.09363605 + 0.00000000 = 0.09363605 loss_val = 0.09363605\n",
      "epoch: 33000/40000 loss_train: 0.08352326 + 0.00000000 = 0.08352326 loss_val = 0.08352326\n",
      "epoch: 36000/40000 loss_train: 0.07596516 + 0.00000000 = 0.07596516 loss_val = 0.07596516\n",
      "epoch: 39000/40000 loss_train: 0.07008409 + 0.00000000 = 0.07008409 loss_val = 0.07008409\n",
      "To 7 neurons, 1 layer(s),  0.001 learning rate, 8 batch size, 0.1 dropout rate, we have: \u001b[1m Accuracy: 97.62% \u001b[0m\n",
      "epoch:    0/40000 loss_train: 0.85714191 + 0.00000000 = 0.85714191 loss_val = 0.85714191\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3000/40000 loss_train: 0.11608807 + 0.00000000 = 0.11608807 loss_val = 0.11608807\n",
      "epoch: 6000/40000 loss_train: 0.08061863 + 0.00000000 = 0.08061863 loss_val = 0.08061863\n",
      "epoch: 9000/40000 loss_train: 0.07022027 + 0.00000000 = 0.07022027 loss_val = 0.07022027\n",
      "epoch: 12000/40000 loss_train: 0.06075012 + 0.00000000 = 0.06075012 loss_val = 0.06075012\n",
      "epoch: 15000/40000 loss_train: 0.05538212 + 0.00000000 = 0.05538212 loss_val = 0.05538212\n",
      "epoch: 18000/40000 loss_train: 0.05118133 + 0.00000000 = 0.05118133 loss_val = 0.05118133\n",
      "epoch: 21000/40000 loss_train: 0.04749536 + 0.00000000 = 0.04749536 loss_val = 0.04749536\n",
      "epoch: 24000/40000 loss_train: 0.04397478 + 0.00000000 = 0.04397478 loss_val = 0.04397478\n",
      "epoch: 27000/40000 loss_train: 0.04039085 + 0.00000000 = 0.04039085 loss_val = 0.04039085\n",
      "epoch: 30000/40000 loss_train: 0.03700926 + 0.00000000 = 0.03700926 loss_val = 0.03700926\n",
      "epoch: 33000/40000 loss_train: 0.03390392 + 0.00000000 = 0.03390392 loss_val = 0.03390392\n",
      "epoch: 36000/40000 loss_train: 0.03102644 + 0.00000000 = 0.03102644 loss_val = 0.03102644\n",
      "epoch: 39000/40000 loss_train: 0.02837036 + 0.00000000 = 0.02837036 loss_val = 0.02837036\n",
      "To 7 neurons, 1 layer(s),  0.005 learning rate, 0 batch size, 0 dropout rate, we have: \u001b[1m Accuracy: 97.62% \u001b[0m\n",
      "epoch:    0/40000 loss_train: 0.91668934 + 0.00000000 = 0.91668934 loss_val = 0.91668934\n",
      "epoch: 3000/40000 loss_train: 0.13162517 + 0.00000000 = 0.13162517 loss_val = 0.13162517\n",
      "epoch: 6000/40000 loss_train: 0.08609011 + 0.00000000 = 0.08609011 loss_val = 0.08609011\n",
      "epoch: 9000/40000 loss_train: 0.07185950 + 0.00000000 = 0.07185950 loss_val = 0.07185950\n",
      "To 7 neurons, 1 layer(s),  0.005 learning rate, 0 batch size, 0.1 dropout rate, we have: \u001b[1m Accuracy: 95.24% \u001b[0m\n",
      "epoch:    0/40000 loss_train: 1.21246189 + 0.00000000 = 1.21246189 loss_val = 1.21246189\n",
      "epoch: 3000/40000 loss_train: 0.53031975 + 0.00000000 = 0.53031975 loss_val = 0.53031975\n",
      "epoch: 6000/40000 loss_train: 0.15574604 + 0.00000000 = 0.15574604 loss_val = 0.15574604\n",
      "epoch: 9000/40000 loss_train: 0.07666025 + 0.00000000 = 0.07666025 loss_val = 0.07666025\n",
      "epoch: 12000/40000 loss_train: 0.05825583 + 0.00000000 = 0.05825583 loss_val = 0.05825583\n",
      "epoch: 15000/40000 loss_train: 0.04910236 + 0.00000000 = 0.04910236 loss_val = 0.04910236\n",
      "epoch: 18000/40000 loss_train: 0.04258794 + 0.00000000 = 0.04258794 loss_val = 0.04258794\n",
      "epoch: 21000/40000 loss_train: 0.03712915 + 0.00000000 = 0.03712915 loss_val = 0.03712915\n",
      "epoch: 24000/40000 loss_train: 0.03233899 + 0.00000000 = 0.03233899 loss_val = 0.03233899\n",
      "epoch: 27000/40000 loss_train: 0.02798222 + 0.00000000 = 0.02798222 loss_val = 0.02798222\n",
      "epoch: 30000/40000 loss_train: 0.02408600 + 0.00000000 = 0.02408600 loss_val = 0.02408600\n",
      "epoch: 33000/40000 loss_train: 0.02070316 + 0.00000000 = 0.02070316 loss_val = 0.02070316\n",
      "epoch: 36000/40000 loss_train: 0.01786918 + 0.00000000 = 0.01786918 loss_val = 0.01786918\n",
      "epoch: 39000/40000 loss_train: 0.01552482 + 0.00000000 = 0.01552482 loss_val = 0.01552482\n",
      "To 7 neurons, 1 layer(s),  0.005 learning rate, 4 batch size, 0 dropout rate, we have: \u001b[1m Accuracy: 97.62% \u001b[0m\n",
      "epoch:    0/40000 loss_train: 1.08596077 + 0.00000000 = 1.08596077 loss_val = 1.08596077\n",
      "epoch: 3000/40000 loss_train: 0.19526265 + 0.00000000 = 0.19526265 loss_val = 0.19526265\n",
      "epoch: 6000/40000 loss_train: 0.08906442 + 0.00000000 = 0.08906442 loss_val = 0.08906442\n",
      "epoch: 9000/40000 loss_train: 0.06563618 + 0.00000000 = 0.06563618 loss_val = 0.06563618\n",
      "epoch: 12000/40000 loss_train: 0.05539246 + 0.00000000 = 0.05539246 loss_val = 0.05539246\n",
      "epoch: 15000/40000 loss_train: 0.04952202 + 0.00000000 = 0.04952202 loss_val = 0.04952202\n",
      "epoch: 18000/40000 loss_train: 0.04556475 + 0.00000000 = 0.04556475 loss_val = 0.04556475\n",
      "To 7 neurons, 1 layer(s),  0.005 learning rate, 4 batch size, 0.1 dropout rate, we have: \u001b[1m Accuracy: 97.62% \u001b[0m\n",
      "epoch:    0/40000 loss_train: 1.01649130 + 0.00000000 = 1.01649130 loss_val = 1.01649130\n",
      "epoch: 3000/40000 loss_train: 0.11333880 + 0.00000000 = 0.11333880 loss_val = 0.11333880\n",
      "epoch: 6000/40000 loss_train: 0.07917650 + 0.00000000 = 0.07917650 loss_val = 0.07917650\n",
      "epoch: 9000/40000 loss_train: 0.06356276 + 0.00000000 = 0.06356276 loss_val = 0.06356276\n",
      "epoch: 12000/40000 loss_train: 0.05391598 + 0.00000000 = 0.05391598 loss_val = 0.05391598\n",
      "epoch: 15000/40000 loss_train: 0.04643791 + 0.00000000 = 0.04643791 loss_val = 0.04643791\n",
      "epoch: 18000/40000 loss_train: 0.04025667 + 0.00000000 = 0.04025667 loss_val = 0.04025667\n",
      "epoch: 21000/40000 loss_train: 0.03380898 + 0.00000000 = 0.03380898 loss_val = 0.03380898\n",
      "epoch: 24000/40000 loss_train: 0.02857231 + 0.00000000 = 0.02857231 loss_val = 0.02857231\n",
      "epoch: 27000/40000 loss_train: 0.02415247 + 0.00000000 = 0.02415247 loss_val = 0.02415247\n",
      "epoch: 30000/40000 loss_train: 0.02034150 + 0.00000000 = 0.02034150 loss_val = 0.02034150\n",
      "epoch: 33000/40000 loss_train: 0.01713142 + 0.00000000 = 0.01713142 loss_val = 0.01713142\n",
      "epoch: 36000/40000 loss_train: 0.01447467 + 0.00000000 = 0.01447467 loss_val = 0.01447467\n",
      "epoch: 39000/40000 loss_train: 0.01229166 + 0.00000000 = 0.01229166 loss_val = 0.01229166\n",
      "To 7 neurons, 1 layer(s),  0.005 learning rate, 8 batch size, 0 dropout rate, we have: \u001b[1m Accuracy: 100.00% \u001b[0m\n",
      "epoch:    0/40000 loss_train: 0.90056825 + 0.00000000 = 0.90056825 loss_val = 0.90056825\n",
      "epoch: 3000/40000 loss_train: 0.14045875 + 0.00000000 = 0.14045875 loss_val = 0.14045875\n",
      "epoch: 6000/40000 loss_train: 0.06995852 + 0.00000000 = 0.06995852 loss_val = 0.06995852\n",
      "epoch: 9000/40000 loss_train: 0.04993270 + 0.00000000 = 0.04993270 loss_val = 0.04993270\n",
      "epoch: 12000/40000 loss_train: 0.03844314 + 0.00000000 = 0.03844314 loss_val = 0.03844314\n",
      "epoch: 15000/40000 loss_train: 0.03089544 + 0.00000000 = 0.03089544 loss_val = 0.03089544\n",
      "epoch: 18000/40000 loss_train: 0.02539235 + 0.00000000 = 0.02539235 loss_val = 0.02539235\n",
      "epoch: 21000/40000 loss_train: 0.02089466 + 0.00000000 = 0.02089466 loss_val = 0.02089466\n",
      "epoch: 24000/40000 loss_train: 0.01721056 + 0.00000000 = 0.01721056 loss_val = 0.01721056\n",
      "epoch: 27000/40000 loss_train: 0.01415045 + 0.00000000 = 0.01415045 loss_val = 0.01415045\n",
      "epoch: 30000/40000 loss_train: 0.01168090 + 0.00000000 = 0.01168090 loss_val = 0.01168090\n",
      "epoch: 33000/40000 loss_train: 0.00978398 + 0.00000000 = 0.00978398 loss_val = 0.00978398\n",
      "epoch: 36000/40000 loss_train: 0.00823609 + 0.00000000 = 0.00823609 loss_val = 0.00823609\n",
      "epoch: 39000/40000 loss_train: 0.00702560 + 0.00000000 = 0.00702560 loss_val = 0.00702560\n",
      "To 7 neurons, 1 layer(s),  0.005 learning rate, 8 batch size, 0.1 dropout rate, we have: \u001b[1m Accuracy: 97.62% \u001b[0m\n",
      "epoch:    0/40000 loss_train: 0.83852008 + 0.00000000 = 0.83852008 loss_val = 0.83852008\n",
      "epoch: 3000/40000 loss_train: 0.08644405 + 0.00000000 = 0.08644405 loss_val = 0.08644405\n",
      "epoch: 6000/40000 loss_train: 0.03304028 + 0.00000000 = 0.03304028 loss_val = 0.03304028\n",
      "epoch: 9000/40000 loss_train: 0.02091852 + 0.00000000 = 0.02091852 loss_val = 0.02091852\n",
      "epoch: 12000/40000 loss_train: 0.01492793 + 0.00000000 = 0.01492793 loss_val = 0.01492793\n",
      "epoch: 15000/40000 loss_train: 0.01120707 + 0.00000000 = 0.01120707 loss_val = 0.01120707\n",
      "epoch: 18000/40000 loss_train: 0.00870096 + 0.00000000 = 0.00870096 loss_val = 0.00870096\n",
      "epoch: 21000/40000 loss_train: 0.00693827 + 0.00000000 = 0.00693827 loss_val = 0.00693827\n",
      "epoch: 24000/40000 loss_train: 0.00566004 + 0.00000000 = 0.00566004 loss_val = 0.00566004\n",
      "epoch: 27000/40000 loss_train: 0.00470766 + 0.00000000 = 0.00470766 loss_val = 0.00470766\n",
      "epoch: 30000/40000 loss_train: 0.00398087 + 0.00000000 = 0.00398087 loss_val = 0.00398087\n",
      "epoch: 33000/40000 loss_train: 0.00341470 + 0.00000000 = 0.00341470 loss_val = 0.00341470\n",
      "epoch: 36000/40000 loss_train: 0.00297311 + 0.00000000 = 0.00297311 loss_val = 0.00297311\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Caio\\DevProjects\\Python\\academico\\doutorado\\tese\\paper_susceptibility_map\\Rede_Neural.py:137: RuntimeWarning: divide by zero encountered in log\n",
      "  return -np.mean(y*np.log(y_pred) +  (1 - y)*np.log(1 - y_pred))\n",
      "C:\\Caio\\DevProjects\\Python\\academico\\doutorado\\tese\\paper_susceptibility_map\\Rede_Neural.py:137: RuntimeWarning: invalid value encountered in multiply\n",
      "  return -np.mean(y*np.log(y_pred) +  (1 - y)*np.log(1 - y_pred))\n",
      "C:\\Caio\\DevProjects\\Python\\academico\\doutorado\\tese\\paper_susceptibility_map\\Rede_Neural.py:136: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return -(y - y_pred) / (y_pred * (1 - y_pred) * y.shape[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To 7 neurons, 1 layer(s),  0.01 learning rate, 0 batch size, 0 dropout rate, we have: \u001b[1m Accuracy: 57.14% \u001b[0m\n",
      "epoch:    0/40000 loss_train: 1.50823713 + 0.00000000 = 1.50823713 loss_val = 1.50823713\n",
      "epoch: 3000/40000 loss_train: 0.09216418 + 0.00000000 = 0.09216418 loss_val = 0.09216418\n",
      "epoch: 6000/40000 loss_train: 0.06617560 + 0.00000000 = 0.06617560 loss_val = 0.06617560\n",
      "To 7 neurons, 1 layer(s),  0.01 learning rate, 0 batch size, 0.1 dropout rate, we have: \u001b[1m Accuracy: 97.62% \u001b[0m\n",
      "epoch:    0/40000 loss_train: 0.86637316 + 0.00000000 = 0.86637316 loss_val = 0.86637316\n",
      "epoch: 3000/40000 loss_train: 0.05944029 + 0.00000000 = 0.05944029 loss_val = 0.05944029\n",
      "epoch: 6000/40000 loss_train: 0.03587299 + 0.00000000 = 0.03587299 loss_val = 0.03587299\n",
      "epoch: 9000/40000 loss_train: 0.02477531 + 0.00000000 = 0.02477531 loss_val = 0.02477531\n",
      "epoch: 12000/40000 loss_train: 0.01757104 + 0.00000000 = 0.01757104 loss_val = 0.01757104\n",
      "epoch: 15000/40000 loss_train: 0.01269389 + 0.00000000 = 0.01269389 loss_val = 0.01269389\n",
      "epoch: 18000/40000 loss_train: 0.00939227 + 0.00000000 = 0.00939227 loss_val = 0.00939227\n",
      "epoch: 21000/40000 loss_train: 0.00714466 + 0.00000000 = 0.00714466 loss_val = 0.00714466\n",
      "epoch: 24000/40000 loss_train: 0.00558849 + 0.00000000 = 0.00558849 loss_val = 0.00558849\n",
      "epoch: 27000/40000 loss_train: 0.00448672 + 0.00000000 = 0.00448672 loss_val = 0.00448672\n",
      "epoch: 30000/40000 loss_train: 0.00368659 + 0.00000000 = 0.00368659 loss_val = 0.00368659\n",
      "epoch: 33000/40000 loss_train: 0.00308871 + 0.00000000 = 0.00308871 loss_val = 0.00308871\n",
      "epoch: 36000/40000 loss_train: 0.00263323 + 0.00000000 = 0.00263323 loss_val = 0.00263323\n",
      "epoch: 39000/40000 loss_train: 0.00227780 + 0.00000000 = 0.00227780 loss_val = 0.00227780\n",
      "To 7 neurons, 1 layer(s),  0.01 learning rate, 4 batch size, 0 dropout rate, we have: \u001b[1m Accuracy: 100.00% \u001b[0m\n",
      "epoch:    0/40000 loss_train: 0.76149112 + 0.00000000 = 0.76149112 loss_val = 0.76149112\n",
      "epoch: 3000/40000 loss_train: 0.08490491 + 0.00000000 = 0.08490491 loss_val = 0.08490491\n",
      "epoch: 6000/40000 loss_train: 0.04949645 + 0.00000000 = 0.04949645 loss_val = 0.04949645\n",
      "epoch: 9000/40000 loss_train: 0.03537628 + 0.00000000 = 0.03537628 loss_val = 0.03537628\n",
      "epoch: 12000/40000 loss_train: 0.02579062 + 0.00000000 = 0.02579062 loss_val = 0.02579062\n",
      "To 7 neurons, 1 layer(s),  0.01 learning rate, 4 batch size, 0.1 dropout rate, we have: \u001b[1m Accuracy: 97.62% \u001b[0m\n",
      "epoch:    0/40000 loss_train: 0.70256780 + 0.00000000 = 0.70256780 loss_val = 0.70256780\n",
      "epoch: 3000/40000 loss_train: 0.04861040 + 0.00000000 = 0.04861040 loss_val = 0.04861040\n",
      "epoch: 6000/40000 loss_train: 0.02698198 + 0.00000000 = 0.02698198 loss_val = 0.02698198\n",
      "epoch: 9000/40000 loss_train: 0.01879510 + 0.00000000 = 0.01879510 loss_val = 0.01879510\n",
      "epoch: 12000/40000 loss_train: 0.01379736 + 0.00000000 = 0.01379736 loss_val = 0.01379736\n",
      "epoch: 15000/40000 loss_train: 0.01043009 + 0.00000000 = 0.01043009 loss_val = 0.01043009\n",
      "epoch: 18000/40000 loss_train: 0.00805480 + 0.00000000 = 0.00805480 loss_val = 0.00805480\n",
      "epoch: 21000/40000 loss_train: 0.00635242 + 0.00000000 = 0.00635242 loss_val = 0.00635242\n",
      "epoch: 24000/40000 loss_train: 0.00511650 + 0.00000000 = 0.00511650 loss_val = 0.00511650\n",
      "epoch: 27000/40000 loss_train: 0.00419913 + 0.00000000 = 0.00419913 loss_val = 0.00419913\n",
      "epoch: 30000/40000 loss_train: 0.00350931 + 0.00000000 = 0.00350931 loss_val = 0.00350931\n",
      "epoch: 33000/40000 loss_train: 0.00297986 + 0.00000000 = 0.00297986 loss_val = 0.00297986\n",
      "epoch: 36000/40000 loss_train: 0.00256576 + 0.00000000 = 0.00256576 loss_val = 0.00256576\n",
      "epoch: 39000/40000 loss_train: 0.00223529 + 0.00000000 = 0.00223529 loss_val = 0.00223529\n",
      "To 7 neurons, 1 layer(s),  0.01 learning rate, 8 batch size, 0 dropout rate, we have: \u001b[1m Accuracy: 97.62% \u001b[0m\n",
      "epoch:    0/40000 loss_train: 1.15000896 + 0.00000000 = 1.15000896 loss_val = 1.15000896\n",
      "epoch: 3000/40000 loss_train: 0.09573422 + 0.00000000 = 0.09573422 loss_val = 0.09573422\n",
      "epoch: 6000/40000 loss_train: 0.04486899 + 0.00000000 = 0.04486899 loss_val = 0.04486899\n",
      "epoch: 9000/40000 loss_train: 0.02785395 + 0.00000000 = 0.02785395 loss_val = 0.02785395\n",
      "epoch: 12000/40000 loss_train: 0.01929252 + 0.00000000 = 0.01929252 loss_val = 0.01929252\n",
      "epoch: 15000/40000 loss_train: 0.01328905 + 0.00000000 = 0.01328905 loss_val = 0.01328905\n",
      "epoch: 18000/40000 loss_train: 0.00940507 + 0.00000000 = 0.00940507 loss_val = 0.00940507\n",
      "To 7 neurons, 1 layer(s),  0.01 learning rate, 8 batch size, 0.1 dropout rate, we have: \u001b[1m Accuracy: 100.00% \u001b[0m\n",
      "epoch:    0/40000 loss_train: 0.80335290 + 0.00000000 = 0.80335290 loss_val = 0.80335290\n",
      "epoch: 3000/40000 loss_train: 0.54674877 + 0.00000000 = 0.54674877 loss_val = 0.54674877\n",
      "epoch: 6000/40000 loss_train: 0.38009417 + 0.00000000 = 0.38009417 loss_val = 0.38009417\n",
      "epoch: 9000/40000 loss_train: 0.24504229 + 0.00000000 = 0.24504229 loss_val = 0.24504229\n",
      "epoch: 12000/40000 loss_train: 0.17462751 + 0.00000000 = 0.17462751 loss_val = 0.17462751\n",
      "epoch: 15000/40000 loss_train: 0.12962969 + 0.00000000 = 0.12962969 loss_val = 0.12962969\n",
      "epoch: 18000/40000 loss_train: 0.10353410 + 0.00000000 = 0.10353410 loss_val = 0.10353410\n",
      "epoch: 21000/40000 loss_train: 0.08681895 + 0.00000000 = 0.08681895 loss_val = 0.08681895\n",
      "epoch: 24000/40000 loss_train: 0.07588085 + 0.00000000 = 0.07588085 loss_val = 0.07588085\n",
      "epoch: 27000/40000 loss_train: 0.06807812 + 0.00000000 = 0.06807812 loss_val = 0.06807812\n",
      "epoch: 30000/40000 loss_train: 0.06210591 + 0.00000000 = 0.06210591 loss_val = 0.06210591\n",
      "epoch: 33000/40000 loss_train: 0.05718311 + 0.00000000 = 0.05718311 loss_val = 0.05718311\n",
      "epoch: 36000/40000 loss_train: 0.05307167 + 0.00000000 = 0.05307167 loss_val = 0.05307167\n",
      "epoch: 39000/40000 loss_train: 0.04962678 + 0.00000000 = 0.04962678 loss_val = 0.04962678\n",
      "To 8 neurons, 1 layer(s),  0.001 learning rate, 0 batch size, 0 dropout rate, we have: \u001b[1m Accuracy: 97.62% \u001b[0m\n",
      "epoch:    0/40000 loss_train: 0.71834671 + 0.00000000 = 0.71834671 loss_val = 0.71834671\n",
      "epoch: 3000/40000 loss_train: 0.54950391 + 0.00000000 = 0.54950391 loss_val = 0.54950391\n",
      "epoch: 6000/40000 loss_train: 0.40185497 + 0.00000000 = 0.40185497 loss_val = 0.40185497\n",
      "epoch: 9000/40000 loss_train: 0.27634564 + 0.00000000 = 0.27634564 loss_val = 0.27634564\n",
      "epoch: 12000/40000 loss_train: 0.19306497 + 0.00000000 = 0.19306497 loss_val = 0.19306497\n",
      "epoch: 15000/40000 loss_train: 0.15054380 + 0.00000000 = 0.15054380 loss_val = 0.15054380\n",
      "epoch: 18000/40000 loss_train: 0.12791199 + 0.00000000 = 0.12791199 loss_val = 0.12791199\n",
      "epoch: 21000/40000 loss_train: 0.11449221 + 0.00000000 = 0.11449221 loss_val = 0.11449221\n",
      "epoch: 24000/40000 loss_train: 0.10599693 + 0.00000000 = 0.10599693 loss_val = 0.10599693\n",
      "epoch: 27000/40000 loss_train: 0.09936982 + 0.00000000 = 0.09936982 loss_val = 0.09936982\n",
      "epoch: 30000/40000 loss_train: 0.09255769 + 0.00000000 = 0.09255769 loss_val = 0.09255769\n",
      "epoch: 33000/40000 loss_train: 0.08613362 + 0.00000000 = 0.08613362 loss_val = 0.08613362\n",
      "epoch: 36000/40000 loss_train: 0.08071294 + 0.00000000 = 0.08071294 loss_val = 0.08071294\n",
      "epoch: 39000/40000 loss_train: 0.07672161 + 0.00000000 = 0.07672161 loss_val = 0.07672161\n",
      "To 8 neurons, 1 layer(s),  0.001 learning rate, 0 batch size, 0.1 dropout rate, we have: \u001b[1m Accuracy: 97.62% \u001b[0m\n",
      "epoch:    0/40000 loss_train: 0.78114202 + 0.00000000 = 0.78114202 loss_val = 0.78114202\n",
      "epoch: 3000/40000 loss_train: 0.55037231 + 0.00000000 = 0.55037231 loss_val = 0.55037231\n",
      "epoch: 6000/40000 loss_train: 0.35243661 + 0.00000000 = 0.35243661 loss_val = 0.35243661\n",
      "epoch: 9000/40000 loss_train: 0.21069789 + 0.00000000 = 0.21069789 loss_val = 0.21069789\n",
      "epoch: 12000/40000 loss_train: 0.14121703 + 0.00000000 = 0.14121703 loss_val = 0.14121703\n",
      "epoch: 15000/40000 loss_train: 0.10842150 + 0.00000000 = 0.10842150 loss_val = 0.10842150\n",
      "epoch: 18000/40000 loss_train: 0.09107449 + 0.00000000 = 0.09107449 loss_val = 0.09107449\n",
      "epoch: 21000/40000 loss_train: 0.08034068 + 0.00000000 = 0.08034068 loss_val = 0.08034068\n",
      "epoch: 24000/40000 loss_train: 0.07280237 + 0.00000000 = 0.07280237 loss_val = 0.07280237\n",
      "epoch: 27000/40000 loss_train: 0.06700576 + 0.00000000 = 0.06700576 loss_val = 0.06700576\n",
      "epoch: 30000/40000 loss_train: 0.06232149 + 0.00000000 = 0.06232149 loss_val = 0.06232149\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 33000/40000 loss_train: 0.05838129 + 0.00000000 = 0.05838129 loss_val = 0.05838129\n",
      "epoch: 36000/40000 loss_train: 0.05499764 + 0.00000000 = 0.05499764 loss_val = 0.05499764\n",
      "epoch: 39000/40000 loss_train: 0.05204909 + 0.00000000 = 0.05204909 loss_val = 0.05204909\n",
      "To 8 neurons, 1 layer(s),  0.001 learning rate, 4 batch size, 0 dropout rate, we have: \u001b[1m Accuracy: 100.00% \u001b[0m\n",
      "epoch:    0/40000 loss_train: 0.82273666 + 0.00000000 = 0.82273666 loss_val = 0.82273666\n",
      "epoch: 3000/40000 loss_train: 0.62785892 + 0.00000000 = 0.62785892 loss_val = 0.62785892\n",
      "epoch: 6000/40000 loss_train: 0.49804911 + 0.00000000 = 0.49804911 loss_val = 0.49804911\n",
      "epoch: 9000/40000 loss_train: 0.32393655 + 0.00000000 = 0.32393655 loss_val = 0.32393655\n",
      "epoch: 12000/40000 loss_train: 0.21977925 + 0.00000000 = 0.21977925 loss_val = 0.21977925\n",
      "epoch: 15000/40000 loss_train: 0.16803981 + 0.00000000 = 0.16803981 loss_val = 0.16803981\n",
      "epoch: 18000/40000 loss_train: 0.13971626 + 0.00000000 = 0.13971626 loss_val = 0.13971626\n",
      "epoch: 21000/40000 loss_train: 0.12392953 + 0.00000000 = 0.12392953 loss_val = 0.12392953\n",
      "epoch: 24000/40000 loss_train: 0.11409842 + 0.00000000 = 0.11409842 loss_val = 0.11409842\n",
      "epoch: 27000/40000 loss_train: 0.10724047 + 0.00000000 = 0.10724047 loss_val = 0.10724047\n",
      "epoch: 30000/40000 loss_train: 0.10186825 + 0.00000000 = 0.10186825 loss_val = 0.10186825\n",
      "epoch: 33000/40000 loss_train: 0.09750777 + 0.00000000 = 0.09750777 loss_val = 0.09750777\n",
      "epoch: 36000/40000 loss_train: 0.09385745 + 0.00000000 = 0.09385745 loss_val = 0.09385745\n",
      "epoch: 39000/40000 loss_train: 0.09089912 + 0.00000000 = 0.09089912 loss_val = 0.09089912\n",
      "To 8 neurons, 1 layer(s),  0.001 learning rate, 4 batch size, 0.1 dropout rate, we have: \u001b[1m Accuracy: 95.24% \u001b[0m\n",
      "epoch:    0/40000 loss_train: 1.09438077 + 0.00000000 = 1.09438077 loss_val = 1.09438077\n",
      "epoch: 3000/40000 loss_train: 0.47186096 + 0.00000000 = 0.47186096 loss_val = 0.47186096\n",
      "epoch: 6000/40000 loss_train: 0.33362480 + 0.00000000 = 0.33362480 loss_val = 0.33362480\n",
      "epoch: 9000/40000 loss_train: 0.24793088 + 0.00000000 = 0.24793088 loss_val = 0.24793088\n",
      "epoch: 12000/40000 loss_train: 0.18569027 + 0.00000000 = 0.18569027 loss_val = 0.18569027\n",
      "epoch: 15000/40000 loss_train: 0.13838025 + 0.00000000 = 0.13838025 loss_val = 0.13838025\n",
      "epoch: 18000/40000 loss_train: 0.10445178 + 0.00000000 = 0.10445178 loss_val = 0.10445178\n",
      "epoch: 21000/40000 loss_train: 0.08375514 + 0.00000000 = 0.08375514 loss_val = 0.08375514\n",
      "epoch: 24000/40000 loss_train: 0.07030075 + 0.00000000 = 0.07030075 loss_val = 0.07030075\n",
      "epoch: 27000/40000 loss_train: 0.06073658 + 0.00000000 = 0.06073658 loss_val = 0.06073658\n",
      "epoch: 30000/40000 loss_train: 0.05353544 + 0.00000000 = 0.05353544 loss_val = 0.05353544\n",
      "epoch: 33000/40000 loss_train: 0.04788105 + 0.00000000 = 0.04788105 loss_val = 0.04788105\n",
      "epoch: 36000/40000 loss_train: 0.04333758 + 0.00000000 = 0.04333758 loss_val = 0.04333758\n",
      "epoch: 39000/40000 loss_train: 0.03959556 + 0.00000000 = 0.03959556 loss_val = 0.03959556\n",
      "To 8 neurons, 1 layer(s),  0.001 learning rate, 8 batch size, 0 dropout rate, we have: \u001b[1m Accuracy: 100.00% \u001b[0m\n",
      "epoch:    0/40000 loss_train: 0.74857676 + 0.00000000 = 0.74857676 loss_val = 0.74857676\n",
      "epoch: 3000/40000 loss_train: 0.50527349 + 0.00000000 = 0.50527349 loss_val = 0.50527349\n",
      "epoch: 6000/40000 loss_train: 0.38054811 + 0.00000000 = 0.38054811 loss_val = 0.38054811\n",
      "epoch: 9000/40000 loss_train: 0.27497189 + 0.00000000 = 0.27497189 loss_val = 0.27497189\n",
      "epoch: 12000/40000 loss_train: 0.19764306 + 0.00000000 = 0.19764306 loss_val = 0.19764306\n",
      "epoch: 15000/40000 loss_train: 0.15600162 + 0.00000000 = 0.15600162 loss_val = 0.15600162\n",
      "epoch: 18000/40000 loss_train: 0.13382798 + 0.00000000 = 0.13382798 loss_val = 0.13382798\n",
      "epoch: 21000/40000 loss_train: 0.11996119 + 0.00000000 = 0.11996119 loss_val = 0.11996119\n",
      "epoch: 24000/40000 loss_train: 0.11034476 + 0.00000000 = 0.11034476 loss_val = 0.11034476\n",
      "epoch: 27000/40000 loss_train: 0.10337102 + 0.00000000 = 0.10337102 loss_val = 0.10337102\n",
      "epoch: 30000/40000 loss_train: 0.09782283 + 0.00000000 = 0.09782283 loss_val = 0.09782283\n",
      "epoch: 33000/40000 loss_train: 0.09331729 + 0.00000000 = 0.09331729 loss_val = 0.09331729\n",
      "epoch: 36000/40000 loss_train: 0.08955235 + 0.00000000 = 0.08955235 loss_val = 0.08955235\n",
      "epoch: 39000/40000 loss_train: 0.08605916 + 0.00000000 = 0.08605916 loss_val = 0.08605916\n",
      "To 8 neurons, 1 layer(s),  0.001 learning rate, 8 batch size, 0.1 dropout rate, we have: \u001b[1m Accuracy: 97.62% \u001b[0m\n",
      "epoch:    0/40000 loss_train: 0.93065033 + 0.00000000 = 0.93065033 loss_val = 0.93065033\n",
      "epoch: 3000/40000 loss_train: 0.22166914 + 0.00000000 = 0.22166914 loss_val = 0.22166914\n",
      "epoch: 6000/40000 loss_train: 0.07345146 + 0.00000000 = 0.07345146 loss_val = 0.07345146\n",
      "epoch: 9000/40000 loss_train: 0.04827176 + 0.00000000 = 0.04827176 loss_val = 0.04827176\n",
      "epoch: 12000/40000 loss_train: 0.03771613 + 0.00000000 = 0.03771613 loss_val = 0.03771613\n",
      "epoch: 15000/40000 loss_train: 0.03141237 + 0.00000000 = 0.03141237 loss_val = 0.03141237\n",
      "epoch: 18000/40000 loss_train: 0.02696237 + 0.00000000 = 0.02696237 loss_val = 0.02696237\n",
      "epoch: 21000/40000 loss_train: 0.02365413 + 0.00000000 = 0.02365413 loss_val = 0.02365413\n",
      "epoch: 24000/40000 loss_train: 0.02098943 + 0.00000000 = 0.02098943 loss_val = 0.02098943\n",
      "epoch: 27000/40000 loss_train: 0.01877070 + 0.00000000 = 0.01877070 loss_val = 0.01877070\n",
      "epoch: 30000/40000 loss_train: 0.01689558 + 0.00000000 = 0.01689558 loss_val = 0.01689558\n",
      "epoch: 33000/40000 loss_train: 0.01525986 + 0.00000000 = 0.01525986 loss_val = 0.01525986\n",
      "epoch: 36000/40000 loss_train: 0.01382379 + 0.00000000 = 0.01382379 loss_val = 0.01382379\n",
      "epoch: 39000/40000 loss_train: 0.01255623 + 0.00000000 = 0.01255623 loss_val = 0.01255623\n",
      "To 8 neurons, 1 layer(s),  0.005 learning rate, 0 batch size, 0 dropout rate, we have: \u001b[1m Accuracy: 97.62% \u001b[0m\n",
      "epoch:    0/40000 loss_train: 0.67298540 + 0.00000000 = 0.67298540 loss_val = 0.67298540\n",
      "epoch: 3000/40000 loss_train: 0.13888343 + 0.00000000 = 0.13888343 loss_val = 0.13888343\n",
      "epoch: 6000/40000 loss_train: 0.08225776 + 0.00000000 = 0.08225776 loss_val = 0.08225776\n",
      "epoch: 9000/40000 loss_train: 0.06350398 + 0.00000000 = 0.06350398 loss_val = 0.06350398\n",
      "To 8 neurons, 1 layer(s),  0.005 learning rate, 0 batch size, 0.1 dropout rate, we have: \u001b[1m Accuracy: 97.62% \u001b[0m\n",
      "epoch:    0/40000 loss_train: 1.18247336 + 0.00000000 = 1.18247336 loss_val = 1.18247336\n",
      "epoch: 3000/40000 loss_train: 0.11206660 + 0.00000000 = 0.11206660 loss_val = 0.11206660\n",
      "epoch: 6000/40000 loss_train: 0.08022509 + 0.00000000 = 0.08022509 loss_val = 0.08022509\n",
      "epoch: 9000/40000 loss_train: 0.06792266 + 0.00000000 = 0.06792266 loss_val = 0.06792266\n",
      "epoch: 12000/40000 loss_train: 0.05525898 + 0.00000000 = 0.05525898 loss_val = 0.05525898\n",
      "epoch: 15000/40000 loss_train: 0.04433073 + 0.00000000 = 0.04433073 loss_val = 0.04433073\n",
      "epoch: 18000/40000 loss_train: 0.03626482 + 0.00000000 = 0.03626482 loss_val = 0.03626482\n",
      "epoch: 21000/40000 loss_train: 0.02938825 + 0.00000000 = 0.02938825 loss_val = 0.02938825\n",
      "epoch: 24000/40000 loss_train: 0.02361477 + 0.00000000 = 0.02361477 loss_val = 0.02361477\n",
      "epoch: 27000/40000 loss_train: 0.01881099 + 0.00000000 = 0.01881099 loss_val = 0.01881099\n",
      "epoch: 30000/40000 loss_train: 0.01492244 + 0.00000000 = 0.01492244 loss_val = 0.01492244\n",
      "epoch: 33000/40000 loss_train: 0.01210840 + 0.00000000 = 0.01210840 loss_val = 0.01210840\n",
      "epoch: 36000/40000 loss_train: 0.00999640 + 0.00000000 = 0.00999640 loss_val = 0.00999640\n",
      "epoch: 39000/40000 loss_train: 0.00837934 + 0.00000000 = 0.00837934 loss_val = 0.00837934\n",
      "To 8 neurons, 1 layer(s),  0.005 learning rate, 4 batch size, 0 dropout rate, we have: \u001b[1m Accuracy: 97.62% \u001b[0m\n",
      "epoch:    0/40000 loss_train: 1.71336423 + 0.00000000 = 1.71336423 loss_val = 1.71336423\n",
      "epoch: 3000/40000 loss_train: 0.20029890 + 0.00000000 = 0.20029890 loss_val = 0.20029890\n",
      "epoch: 6000/40000 loss_train: 0.12456854 + 0.00000000 = 0.12456854 loss_val = 0.12456854\n",
      "epoch: 9000/40000 loss_train: 0.09324477 + 0.00000000 = 0.09324477 loss_val = 0.09324477\n",
      "epoch: 12000/40000 loss_train: 0.07550768 + 0.00000000 = 0.07550768 loss_val = 0.07550768\n",
      "epoch: 15000/40000 loss_train: 0.06306779 + 0.00000000 = 0.06306779 loss_val = 0.06306779\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 18000/40000 loss_train: 0.05171151 + 0.00000000 = 0.05171151 loss_val = 0.05171151\n",
      "epoch: 21000/40000 loss_train: 0.04204380 + 0.00000000 = 0.04204380 loss_val = 0.04204380\n",
      "To 8 neurons, 1 layer(s),  0.005 learning rate, 4 batch size, 0.1 dropout rate, we have: \u001b[1m Accuracy: 97.62% \u001b[0m\n",
      "epoch:    0/40000 loss_train: 0.78327757 + 0.00000000 = 0.78327757 loss_val = 0.78327757\n",
      "epoch: 3000/40000 loss_train: 0.11762831 + 0.00000000 = 0.11762831 loss_val = 0.11762831\n",
      "epoch: 6000/40000 loss_train: 0.08507498 + 0.00000000 = 0.08507498 loss_val = 0.08507498\n",
      "epoch: 9000/40000 loss_train: 0.07158795 + 0.00000000 = 0.07158795 loss_val = 0.07158795\n",
      "epoch: 12000/40000 loss_train: 0.05988722 + 0.00000000 = 0.05988722 loss_val = 0.05988722\n",
      "epoch: 15000/40000 loss_train: 0.05468446 + 0.00000000 = 0.05468446 loss_val = 0.05468446\n",
      "epoch: 18000/40000 loss_train: 0.05050242 + 0.00000000 = 0.05050242 loss_val = 0.05050242\n",
      "epoch: 21000/40000 loss_train: 0.04685570 + 0.00000000 = 0.04685570 loss_val = 0.04685570\n",
      "epoch: 24000/40000 loss_train: 0.04250923 + 0.00000000 = 0.04250923 loss_val = 0.04250923\n",
      "epoch: 27000/40000 loss_train: 0.03859493 + 0.00000000 = 0.03859493 loss_val = 0.03859493\n",
      "epoch: 30000/40000 loss_train: 0.03495328 + 0.00000000 = 0.03495328 loss_val = 0.03495328\n",
      "epoch: 33000/40000 loss_train: 0.02968185 + 0.00000000 = 0.02968185 loss_val = 0.02968185\n",
      "epoch: 36000/40000 loss_train: 0.02450644 + 0.00000000 = 0.02450644 loss_val = 0.02450644\n",
      "epoch: 39000/40000 loss_train: 0.01909648 + 0.00000000 = 0.01909648 loss_val = 0.01909648\n",
      "To 8 neurons, 1 layer(s),  0.005 learning rate, 8 batch size, 0 dropout rate, we have: \u001b[1m Accuracy: 97.62% \u001b[0m\n",
      "epoch:    0/40000 loss_train: 0.98729673 + 0.00000000 = 0.98729673 loss_val = 0.98729673\n",
      "epoch: 3000/40000 loss_train: 0.14257977 + 0.00000000 = 0.14257977 loss_val = 0.14257977\n",
      "epoch: 6000/40000 loss_train: 0.05857574 + 0.00000000 = 0.05857574 loss_val = 0.05857574\n",
      "epoch: 9000/40000 loss_train: 0.04226584 + 0.00000000 = 0.04226584 loss_val = 0.04226584\n",
      "epoch: 12000/40000 loss_train: 0.03440390 + 0.00000000 = 0.03440390 loss_val = 0.03440390\n",
      "epoch: 15000/40000 loss_train: 0.02944837 + 0.00000000 = 0.02944837 loss_val = 0.02944837\n",
      "epoch: 18000/40000 loss_train: 0.02561953 + 0.00000000 = 0.02561953 loss_val = 0.02561953\n",
      "epoch: 21000/40000 loss_train: 0.02245778 + 0.00000000 = 0.02245778 loss_val = 0.02245778\n",
      "epoch: 24000/40000 loss_train: 0.01973907 + 0.00000000 = 0.01973907 loss_val = 0.01973907\n",
      "epoch: 27000/40000 loss_train: 0.01732830 + 0.00000000 = 0.01732830 loss_val = 0.01732830\n",
      "epoch: 30000/40000 loss_train: 0.01522486 + 0.00000000 = 0.01522486 loss_val = 0.01522486\n",
      "epoch: 33000/40000 loss_train: 0.01339193 + 0.00000000 = 0.01339193 loss_val = 0.01339193\n",
      "epoch: 36000/40000 loss_train: 0.01181397 + 0.00000000 = 0.01181397 loss_val = 0.01181397\n",
      "epoch: 39000/40000 loss_train: 0.01036315 + 0.00000000 = 0.01036315 loss_val = 0.01036315\n",
      "To 8 neurons, 1 layer(s),  0.005 learning rate, 8 batch size, 0.1 dropout rate, we have: \u001b[1m Accuracy: 97.62% \u001b[0m\n",
      "epoch:    0/40000 loss_train: 1.03827578 + 0.00000000 = 1.03827578 loss_val = 1.03827578\n",
      "epoch: 3000/40000 loss_train: 0.08901167 + 0.00000000 = 0.08901167 loss_val = 0.08901167\n",
      "epoch: 6000/40000 loss_train: 0.06475663 + 0.00000000 = 0.06475663 loss_val = 0.06475663\n",
      "epoch: 9000/40000 loss_train: 0.05272336 + 0.00000000 = 0.05272336 loss_val = 0.05272336\n",
      "epoch: 12000/40000 loss_train: 0.04732386 + 0.00000000 = 0.04732386 loss_val = 0.04732386\n",
      "epoch: 15000/40000 loss_train: 0.04293280 + 0.00000000 = 0.04293280 loss_val = 0.04293280\n",
      "epoch: 18000/40000 loss_train: 0.03806411 + 0.00000000 = 0.03806411 loss_val = 0.03806411\n",
      "epoch: 21000/40000 loss_train: 0.03336858 + 0.00000000 = 0.03336858 loss_val = 0.03336858\n",
      "epoch: 24000/40000 loss_train: 0.02937392 + 0.00000000 = 0.02937392 loss_val = 0.02937392\n",
      "epoch: 27000/40000 loss_train: 0.02615044 + 0.00000000 = 0.02615044 loss_val = 0.02615044\n",
      "epoch: 30000/40000 loss_train: 0.02320725 + 0.00000000 = 0.02320725 loss_val = 0.02320725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Caio\\DevProjects\\Python\\academico\\doutorado\\tese\\paper_susceptibility_map\\Rede_Neural.py:137: RuntimeWarning: divide by zero encountered in log\n",
      "  return -np.mean(y*np.log(y_pred) +  (1 - y)*np.log(1 - y_pred))\n",
      "C:\\Caio\\DevProjects\\Python\\academico\\doutorado\\tese\\paper_susceptibility_map\\Rede_Neural.py:137: RuntimeWarning: invalid value encountered in multiply\n",
      "  return -np.mean(y*np.log(y_pred) +  (1 - y)*np.log(1 - y_pred))\n",
      "C:\\Caio\\DevProjects\\Python\\academico\\doutorado\\tese\\paper_susceptibility_map\\Rede_Neural.py:136: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return -(y - y_pred) / (y_pred * (1 - y_pred) * y.shape[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To 8 neurons, 1 layer(s),  0.01 learning rate, 0 batch size, 0 dropout rate, we have: \u001b[1m Accuracy: 57.14% \u001b[0m\n",
      "epoch:    0/40000 loss_train: 0.78804363 + 0.00000000 = 0.78804363 loss_val = 0.78804363\n",
      "epoch: 3000/40000 loss_train: 0.10031221 + 0.00000000 = 0.10031221 loss_val = 0.10031221\n",
      "epoch: 6000/40000 loss_train: 0.05969013 + 0.00000000 = 0.05969013 loss_val = 0.05969013\n",
      "epoch: 9000/40000 loss_train: 0.04599157 + 0.00000000 = 0.04599157 loss_val = 0.04599157\n",
      "To 8 neurons, 1 layer(s),  0.01 learning rate, 0 batch size, 0.1 dropout rate, we have: \u001b[1m Accuracy: 95.24% \u001b[0m\n",
      "epoch:    0/40000 loss_train: 1.19451333 + 0.00000000 = 1.19451333 loss_val = 1.19451333\n",
      "epoch: 3000/40000 loss_train: 0.06965550 + 0.00000000 = 0.06965550 loss_val = 0.06965550\n",
      "epoch: 6000/40000 loss_train: 0.04247008 + 0.00000000 = 0.04247008 loss_val = 0.04247008\n",
      "epoch: 9000/40000 loss_train: 0.03350689 + 0.00000000 = 0.03350689 loss_val = 0.03350689\n",
      "epoch: 12000/40000 loss_train: 0.02822817 + 0.00000000 = 0.02822817 loss_val = 0.02822817\n",
      "epoch: 15000/40000 loss_train: 0.02424457 + 0.00000000 = 0.02424457 loss_val = 0.02424457\n",
      "epoch: 18000/40000 loss_train: 0.02106328 + 0.00000000 = 0.02106328 loss_val = 0.02106328\n",
      "epoch: 21000/40000 loss_train: 0.01843305 + 0.00000000 = 0.01843305 loss_val = 0.01843305\n",
      "epoch: 24000/40000 loss_train: 0.01617712 + 0.00000000 = 0.01617712 loss_val = 0.01617712\n",
      "epoch: 27000/40000 loss_train: 0.01419979 + 0.00000000 = 0.01419979 loss_val = 0.01419979\n",
      "epoch: 30000/40000 loss_train: 0.01246042 + 0.00000000 = 0.01246042 loss_val = 0.01246042\n",
      "epoch: 33000/40000 loss_train: 0.01093598 + 0.00000000 = 0.01093598 loss_val = 0.01093598\n",
      "epoch: 36000/40000 loss_train: 0.00960116 + 0.00000000 = 0.00960116 loss_val = 0.00960116\n",
      "epoch: 39000/40000 loss_train: 0.00844489 + 0.00000000 = 0.00844489 loss_val = 0.00844489\n",
      "To 8 neurons, 1 layer(s),  0.01 learning rate, 4 batch size, 0 dropout rate, we have: \u001b[1m Accuracy: 97.62% \u001b[0m\n",
      "epoch:    0/40000 loss_train: 1.54739275 + 0.00000000 = 1.54739275 loss_val = 1.54739275\n",
      "epoch: 3000/40000 loss_train: 0.06997594 + 0.00000000 = 0.06997594 loss_val = 0.06997594\n",
      "epoch: 6000/40000 loss_train: 0.04190483 + 0.00000000 = 0.04190483 loss_val = 0.04190483\n",
      "epoch: 9000/40000 loss_train: 0.02883955 + 0.00000000 = 0.02883955 loss_val = 0.02883955\n",
      "epoch: 12000/40000 loss_train: 0.02058757 + 0.00000000 = 0.02058757 loss_val = 0.02058757\n",
      "epoch: 15000/40000 loss_train: 0.01457535 + 0.00000000 = 0.01457535 loss_val = 0.01457535\n",
      "epoch: 18000/40000 loss_train: 0.01043366 + 0.00000000 = 0.01043366 loss_val = 0.01043366\n",
      "epoch: 21000/40000 loss_train: 0.00765596 + 0.00000000 = 0.00765596 loss_val = 0.00765596\n",
      "epoch: 24000/40000 loss_train: 0.00584285 + 0.00000000 = 0.00584285 loss_val = 0.00584285\n",
      "To 8 neurons, 1 layer(s),  0.01 learning rate, 4 batch size, 0.1 dropout rate, we have: \u001b[1m Accuracy: 97.62% \u001b[0m\n",
      "epoch:    0/40000 loss_train: 0.74536808 + 0.00000000 = 0.74536808 loss_val = 0.74536808\n",
      "epoch: 3000/40000 loss_train: 0.07301998 + 0.00000000 = 0.07301998 loss_val = 0.07301998\n",
      "epoch: 6000/40000 loss_train: 0.04738365 + 0.00000000 = 0.04738365 loss_val = 0.04738365\n",
      "epoch: 9000/40000 loss_train: 0.03400316 + 0.00000000 = 0.03400316 loss_val = 0.03400316\n",
      "epoch: 12000/40000 loss_train: 0.02244431 + 0.00000000 = 0.02244431 loss_val = 0.02244431\n",
      "epoch: 15000/40000 loss_train: 0.01348762 + 0.00000000 = 0.01348762 loss_val = 0.01348762\n",
      "epoch: 18000/40000 loss_train: 0.00840463 + 0.00000000 = 0.00840463 loss_val = 0.00840463\n",
      "epoch: 21000/40000 loss_train: 0.00575258 + 0.00000000 = 0.00575258 loss_val = 0.00575258\n",
      "epoch: 24000/40000 loss_train: 0.00424115 + 0.00000000 = 0.00424115 loss_val = 0.00424115\n",
      "epoch: 27000/40000 loss_train: 0.00329271 + 0.00000000 = 0.00329271 loss_val = 0.00329271\n",
      "epoch: 30000/40000 loss_train: 0.00265122 + 0.00000000 = 0.00265122 loss_val = 0.00265122\n",
      "epoch: 33000/40000 loss_train: 0.00219646 + 0.00000000 = 0.00219646 loss_val = 0.00219646\n",
      "epoch: 36000/40000 loss_train: 0.00186037 + 0.00000000 = 0.00186037 loss_val = 0.00186037\n",
      "epoch: 39000/40000 loss_train: 0.00160348 + 0.00000000 = 0.00160348 loss_val = 0.00160348\n",
      "To 8 neurons, 1 layer(s),  0.01 learning rate, 8 batch size, 0 dropout rate, we have: \u001b[1m Accuracy: 100.00% \u001b[0m\n",
      "epoch:    0/40000 loss_train: 0.89106092 + 0.00000000 = 0.89106092 loss_val = 0.89106092\n",
      "epoch: 3000/40000 loss_train: 0.06746989 + 0.00000000 = 0.06746989 loss_val = 0.06746989\n",
      "epoch: 6000/40000 loss_train: 0.04132861 + 0.00000000 = 0.04132861 loss_val = 0.04132861\n",
      "epoch: 9000/40000 loss_train: 0.02882103 + 0.00000000 = 0.02882103 loss_val = 0.02882103\n",
      "epoch: 12000/40000 loss_train: 0.01875565 + 0.00000000 = 0.01875565 loss_val = 0.01875565\n",
      "epoch: 15000/40000 loss_train: 0.01225826 + 0.00000000 = 0.01225826 loss_val = 0.01225826\n",
      "epoch: 18000/40000 loss_train: 0.00839692 + 0.00000000 = 0.00839692 loss_val = 0.00839692\n",
      "epoch: 21000/40000 loss_train: 0.00599925 + 0.00000000 = 0.00599925 loss_val = 0.00599925\n",
      "To 8 neurons, 1 layer(s),  0.01 learning rate, 8 batch size, 0.1 dropout rate, we have: \u001b[1m Accuracy: 100.00% \u001b[0m\n",
      "epoch:    0/40000 loss_train: 0.79983060 + 0.00000000 = 0.79983060 loss_val = 0.79983060\n",
      "epoch: 3000/40000 loss_train: 0.40100199 + 0.00000000 = 0.40100199 loss_val = 0.40100199\n",
      "epoch: 6000/40000 loss_train: 0.25445178 + 0.00000000 = 0.25445178 loss_val = 0.25445178\n",
      "epoch: 9000/40000 loss_train: 0.17428307 + 0.00000000 = 0.17428307 loss_val = 0.17428307\n",
      "epoch: 12000/40000 loss_train: 0.13566533 + 0.00000000 = 0.13566533 loss_val = 0.13566533\n",
      "epoch: 15000/40000 loss_train: 0.11671544 + 0.00000000 = 0.11671544 loss_val = 0.11671544\n",
      "epoch: 18000/40000 loss_train: 0.10615526 + 0.00000000 = 0.10615526 loss_val = 0.10615526\n",
      "epoch: 21000/40000 loss_train: 0.09954527 + 0.00000000 = 0.09954527 loss_val = 0.09954527\n",
      "epoch: 24000/40000 loss_train: 0.09499610 + 0.00000000 = 0.09499610 loss_val = 0.09499610\n",
      "epoch: 27000/40000 loss_train: 0.09162343 + 0.00000000 = 0.09162343 loss_val = 0.09162343\n",
      "epoch: 30000/40000 loss_train: 0.08896041 + 0.00000000 = 0.08896041 loss_val = 0.08896041\n",
      "epoch: 33000/40000 loss_train: 0.08678703 + 0.00000000 = 0.08678703 loss_val = 0.08678703\n",
      "epoch: 36000/40000 loss_train: 0.08497462 + 0.00000000 = 0.08497462 loss_val = 0.08497462\n",
      "epoch: 39000/40000 loss_train: 0.08343241 + 0.00000000 = 0.08343241 loss_val = 0.08343241\n",
      "To 12 neurons, 1 layer(s),  0.001 learning rate, 0 batch size, 0 dropout rate, we have: \u001b[1m Accuracy: 97.62% \u001b[0m\n",
      "epoch:    0/40000 loss_train: 0.97727114 + 0.00000000 = 0.97727114 loss_val = 0.97727114\n",
      "epoch: 3000/40000 loss_train: 0.28899839 + 0.00000000 = 0.28899839 loss_val = 0.28899839\n",
      "epoch: 6000/40000 loss_train: 0.18261306 + 0.00000000 = 0.18261306 loss_val = 0.18261306\n",
      "epoch: 9000/40000 loss_train: 0.13253077 + 0.00000000 = 0.13253077 loss_val = 0.13253077\n",
      "epoch: 12000/40000 loss_train: 0.10735007 + 0.00000000 = 0.10735007 loss_val = 0.10735007\n",
      "epoch: 15000/40000 loss_train: 0.09322287 + 0.00000000 = 0.09322287 loss_val = 0.09322287\n",
      "epoch: 18000/40000 loss_train: 0.08413542 + 0.00000000 = 0.08413542 loss_val = 0.08413542\n",
      "epoch: 21000/40000 loss_train: 0.07757855 + 0.00000000 = 0.07757855 loss_val = 0.07757855\n",
      "epoch: 24000/40000 loss_train: 0.07240991 + 0.00000000 = 0.07240991 loss_val = 0.07240991\n",
      "epoch: 27000/40000 loss_train: 0.06821649 + 0.00000000 = 0.06821649 loss_val = 0.06821649\n",
      "epoch: 30000/40000 loss_train: 0.06450266 + 0.00000000 = 0.06450266 loss_val = 0.06450266\n",
      "epoch: 33000/40000 loss_train: 0.06149795 + 0.00000000 = 0.06149795 loss_val = 0.06149795\n",
      "epoch: 36000/40000 loss_train: 0.05885480 + 0.00000000 = 0.05885480 loss_val = 0.05885480\n",
      "epoch: 39000/40000 loss_train: 0.05647400 + 0.00000000 = 0.05647400 loss_val = 0.05647400\n",
      "To 12 neurons, 1 layer(s),  0.001 learning rate, 0 batch size, 0.1 dropout rate, we have: \u001b[1m Accuracy: 97.62% \u001b[0m\n",
      "epoch:    0/40000 loss_train: 0.76171945 + 0.00000000 = 0.76171945 loss_val = 0.76171945\n",
      "epoch: 3000/40000 loss_train: 0.52918447 + 0.00000000 = 0.52918447 loss_val = 0.52918447\n",
      "epoch: 6000/40000 loss_train: 0.33576470 + 0.00000000 = 0.33576470 loss_val = 0.33576470\n",
      "epoch: 9000/40000 loss_train: 0.20396802 + 0.00000000 = 0.20396802 loss_val = 0.20396802\n",
      "epoch: 12000/40000 loss_train: 0.13589402 + 0.00000000 = 0.13589402 loss_val = 0.13589402\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 15000/40000 loss_train: 0.10369470 + 0.00000000 = 0.10369470 loss_val = 0.10369470\n",
      "epoch: 18000/40000 loss_train: 0.08629283 + 0.00000000 = 0.08629283 loss_val = 0.08629283\n",
      "epoch: 21000/40000 loss_train: 0.07534573 + 0.00000000 = 0.07534573 loss_val = 0.07534573\n",
      "epoch: 24000/40000 loss_train: 0.06780596 + 0.00000000 = 0.06780596 loss_val = 0.06780596\n",
      "epoch: 27000/40000 loss_train: 0.06211584 + 0.00000000 = 0.06211584 loss_val = 0.06211584\n",
      "epoch: 30000/40000 loss_train: 0.05762764 + 0.00000000 = 0.05762764 loss_val = 0.05762764\n",
      "epoch: 33000/40000 loss_train: 0.05387638 + 0.00000000 = 0.05387638 loss_val = 0.05387638\n",
      "epoch: 36000/40000 loss_train: 0.05067039 + 0.00000000 = 0.05067039 loss_val = 0.05067039\n",
      "epoch: 39000/40000 loss_train: 0.04788078 + 0.00000000 = 0.04788078 loss_val = 0.04788078\n",
      "To 12 neurons, 1 layer(s),  0.001 learning rate, 4 batch size, 0 dropout rate, we have: \u001b[1m Accuracy: 97.62% \u001b[0m\n",
      "epoch:    0/40000 loss_train: 1.24704772 + 0.00000000 = 1.24704772 loss_val = 1.24704772\n",
      "epoch: 3000/40000 loss_train: 0.46528850 + 0.00000000 = 0.46528850 loss_val = 0.46528850\n",
      "epoch: 6000/40000 loss_train: 0.29326307 + 0.00000000 = 0.29326307 loss_val = 0.29326307\n",
      "epoch: 9000/40000 loss_train: 0.18407525 + 0.00000000 = 0.18407525 loss_val = 0.18407525\n",
      "epoch: 12000/40000 loss_train: 0.13030987 + 0.00000000 = 0.13030987 loss_val = 0.13030987\n",
      "epoch: 15000/40000 loss_train: 0.10120541 + 0.00000000 = 0.10120541 loss_val = 0.10120541\n",
      "epoch: 18000/40000 loss_train: 0.08390191 + 0.00000000 = 0.08390191 loss_val = 0.08390191\n",
      "epoch: 21000/40000 loss_train: 0.07260142 + 0.00000000 = 0.07260142 loss_val = 0.07260142\n",
      "epoch: 24000/40000 loss_train: 0.06460111 + 0.00000000 = 0.06460111 loss_val = 0.06460111\n",
      "epoch: 27000/40000 loss_train: 0.05876671 + 0.00000000 = 0.05876671 loss_val = 0.05876671\n",
      "epoch: 30000/40000 loss_train: 0.05421107 + 0.00000000 = 0.05421107 loss_val = 0.05421107\n",
      "epoch: 33000/40000 loss_train: 0.05057941 + 0.00000000 = 0.05057941 loss_val = 0.05057941\n",
      "epoch: 36000/40000 loss_train: 0.04751349 + 0.00000000 = 0.04751349 loss_val = 0.04751349\n",
      "epoch: 39000/40000 loss_train: 0.04492794 + 0.00000000 = 0.04492794 loss_val = 0.04492794\n",
      "To 12 neurons, 1 layer(s),  0.001 learning rate, 4 batch size, 0.1 dropout rate, we have: \u001b[1m Accuracy: 100.00% \u001b[0m\n",
      "epoch:    0/40000 loss_train: 0.73988935 + 0.00000000 = 0.73988935 loss_val = 0.73988935\n",
      "epoch: 3000/40000 loss_train: 0.52556587 + 0.00000000 = 0.52556587 loss_val = 0.52556587\n",
      "epoch: 6000/40000 loss_train: 0.38282810 + 0.00000000 = 0.38282810 loss_val = 0.38282810\n",
      "epoch: 9000/40000 loss_train: 0.25164086 + 0.00000000 = 0.25164086 loss_val = 0.25164086\n",
      "epoch: 12000/40000 loss_train: 0.17662085 + 0.00000000 = 0.17662085 loss_val = 0.17662085\n",
      "epoch: 15000/40000 loss_train: 0.13297744 + 0.00000000 = 0.13297744 loss_val = 0.13297744\n",
      "epoch: 18000/40000 loss_train: 0.10687175 + 0.00000000 = 0.10687175 loss_val = 0.10687175\n",
      "epoch: 21000/40000 loss_train: 0.08992760 + 0.00000000 = 0.08992760 loss_val = 0.08992760\n",
      "epoch: 24000/40000 loss_train: 0.07861366 + 0.00000000 = 0.07861366 loss_val = 0.07861366\n",
      "epoch: 27000/40000 loss_train: 0.07047062 + 0.00000000 = 0.07047062 loss_val = 0.07047062\n",
      "epoch: 30000/40000 loss_train: 0.06404782 + 0.00000000 = 0.06404782 loss_val = 0.06404782\n",
      "epoch: 33000/40000 loss_train: 0.05871257 + 0.00000000 = 0.05871257 loss_val = 0.05871257\n",
      "epoch: 36000/40000 loss_train: 0.05438853 + 0.00000000 = 0.05438853 loss_val = 0.05438853\n",
      "epoch: 39000/40000 loss_train: 0.05071664 + 0.00000000 = 0.05071664 loss_val = 0.05071664\n",
      "To 12 neurons, 1 layer(s),  0.001 learning rate, 8 batch size, 0 dropout rate, we have: \u001b[1m Accuracy: 100.00% \u001b[0m\n",
      "epoch:    0/40000 loss_train: 0.73530110 + 0.00000000 = 0.73530110 loss_val = 0.73530110\n",
      "epoch: 3000/40000 loss_train: 0.55560924 + 0.00000000 = 0.55560924 loss_val = 0.55560924\n",
      "epoch: 6000/40000 loss_train: 0.38287547 + 0.00000000 = 0.38287547 loss_val = 0.38287547\n",
      "epoch: 9000/40000 loss_train: 0.23734915 + 0.00000000 = 0.23734915 loss_val = 0.23734915\n",
      "epoch: 12000/40000 loss_train: 0.15531031 + 0.00000000 = 0.15531031 loss_val = 0.15531031\n",
      "epoch: 15000/40000 loss_train: 0.11703909 + 0.00000000 = 0.11703909 loss_val = 0.11703909\n",
      "epoch: 18000/40000 loss_train: 0.09657421 + 0.00000000 = 0.09657421 loss_val = 0.09657421\n",
      "epoch: 21000/40000 loss_train: 0.08457762 + 0.00000000 = 0.08457762 loss_val = 0.08457762\n",
      "epoch: 24000/40000 loss_train: 0.07619177 + 0.00000000 = 0.07619177 loss_val = 0.07619177\n",
      "epoch: 27000/40000 loss_train: 0.06966232 + 0.00000000 = 0.06966232 loss_val = 0.06966232\n",
      "epoch: 30000/40000 loss_train: 0.06451422 + 0.00000000 = 0.06451422 loss_val = 0.06451422\n",
      "epoch: 33000/40000 loss_train: 0.06026930 + 0.00000000 = 0.06026930 loss_val = 0.06026930\n",
      "epoch: 36000/40000 loss_train: 0.05676323 + 0.00000000 = 0.05676323 loss_val = 0.05676323\n",
      "epoch: 39000/40000 loss_train: 0.05377773 + 0.00000000 = 0.05377773 loss_val = 0.05377773\n",
      "To 12 neurons, 1 layer(s),  0.001 learning rate, 8 batch size, 0.1 dropout rate, we have: \u001b[1m Accuracy: 100.00% \u001b[0m\n",
      "epoch:    0/40000 loss_train: 0.71651679 + 0.00000000 = 0.71651679 loss_val = 0.71651679\n",
      "epoch: 3000/40000 loss_train: 0.13161606 + 0.00000000 = 0.13161606 loss_val = 0.13161606\n",
      "epoch: 6000/40000 loss_train: 0.08244981 + 0.00000000 = 0.08244981 loss_val = 0.08244981\n",
      "epoch: 9000/40000 loss_train: 0.06754770 + 0.00000000 = 0.06754770 loss_val = 0.06754770\n",
      "epoch: 12000/40000 loss_train: 0.05928421 + 0.00000000 = 0.05928421 loss_val = 0.05928421\n",
      "epoch: 15000/40000 loss_train: 0.05389705 + 0.00000000 = 0.05389705 loss_val = 0.05389705\n",
      "epoch: 18000/40000 loss_train: 0.04944405 + 0.00000000 = 0.04944405 loss_val = 0.04944405\n",
      "epoch: 21000/40000 loss_train: 0.04533686 + 0.00000000 = 0.04533686 loss_val = 0.04533686\n",
      "epoch: 24000/40000 loss_train: 0.04199431 + 0.00000000 = 0.04199431 loss_val = 0.04199431\n",
      "epoch: 27000/40000 loss_train: 0.03868331 + 0.00000000 = 0.03868331 loss_val = 0.03868331\n",
      "epoch: 30000/40000 loss_train: 0.03462038 + 0.00000000 = 0.03462038 loss_val = 0.03462038\n",
      "epoch: 33000/40000 loss_train: 0.03006032 + 0.00000000 = 0.03006032 loss_val = 0.03006032\n",
      "epoch: 36000/40000 loss_train: 0.02571176 + 0.00000000 = 0.02571176 loss_val = 0.02571176\n",
      "epoch: 39000/40000 loss_train: 0.02180585 + 0.00000000 = 0.02180585 loss_val = 0.02180585\n",
      "To 12 neurons, 1 layer(s),  0.005 learning rate, 0 batch size, 0 dropout rate, we have: \u001b[1m Accuracy: 97.62% \u001b[0m\n",
      "epoch:    0/40000 loss_train: 1.49301782 + 0.00000000 = 1.49301782 loss_val = 1.49301782\n",
      "epoch: 3000/40000 loss_train: 0.11942583 + 0.00000000 = 0.11942583 loss_val = 0.11942583\n",
      "epoch: 6000/40000 loss_train: 0.06944083 + 0.00000000 = 0.06944083 loss_val = 0.06944083\n",
      "epoch: 9000/40000 loss_train: 0.05438656 + 0.00000000 = 0.05438656 loss_val = 0.05438656\n",
      "epoch: 12000/40000 loss_train: 0.04654580 + 0.00000000 = 0.04654580 loss_val = 0.04654580\n",
      "epoch: 15000/40000 loss_train: 0.04160005 + 0.00000000 = 0.04160005 loss_val = 0.04160005\n",
      "epoch: 18000/40000 loss_train: 0.03769202 + 0.00000000 = 0.03769202 loss_val = 0.03769202\n",
      "epoch: 21000/40000 loss_train: 0.03416096 + 0.00000000 = 0.03416096 loss_val = 0.03416096\n",
      "To 12 neurons, 1 layer(s),  0.005 learning rate, 0 batch size, 0.1 dropout rate, we have: \u001b[1m Accuracy: 97.62% \u001b[0m\n",
      "epoch:    0/40000 loss_train: 0.88756926 + 0.00000000 = 0.88756926 loss_val = 0.88756926\n",
      "epoch: 3000/40000 loss_train: 0.11949936 + 0.00000000 = 0.11949936 loss_val = 0.11949936\n",
      "epoch: 6000/40000 loss_train: 0.08647573 + 0.00000000 = 0.08647573 loss_val = 0.08647573\n",
      "epoch: 9000/40000 loss_train: 0.07473186 + 0.00000000 = 0.07473186 loss_val = 0.07473186\n",
      "epoch: 12000/40000 loss_train: 0.06737894 + 0.00000000 = 0.06737894 loss_val = 0.06737894\n",
      "epoch: 15000/40000 loss_train: 0.06116966 + 0.00000000 = 0.06116966 loss_val = 0.06116966\n",
      "epoch: 18000/40000 loss_train: 0.05578319 + 0.00000000 = 0.05578319 loss_val = 0.05578319\n",
      "epoch: 21000/40000 loss_train: 0.05109394 + 0.00000000 = 0.05109394 loss_val = 0.05109394\n",
      "epoch: 24000/40000 loss_train: 0.04655077 + 0.00000000 = 0.04655077 loss_val = 0.04655077\n",
      "epoch: 27000/40000 loss_train: 0.04199187 + 0.00000000 = 0.04199187 loss_val = 0.04199187\n",
      "epoch: 30000/40000 loss_train: 0.03732114 + 0.00000000 = 0.03732114 loss_val = 0.03732114\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 33000/40000 loss_train: 0.03245138 + 0.00000000 = 0.03245138 loss_val = 0.03245138\n",
      "epoch: 36000/40000 loss_train: 0.02764372 + 0.00000000 = 0.02764372 loss_val = 0.02764372\n",
      "epoch: 39000/40000 loss_train: 0.02304193 + 0.00000000 = 0.02304193 loss_val = 0.02304193\n",
      "To 12 neurons, 1 layer(s),  0.005 learning rate, 4 batch size, 0 dropout rate, we have: \u001b[1m Accuracy: 100.00% \u001b[0m\n",
      "epoch:    0/40000 loss_train: 0.85770124 + 0.00000000 = 0.85770124 loss_val = 0.85770124\n",
      "epoch: 3000/40000 loss_train: 0.11992142 + 0.00000000 = 0.11992142 loss_val = 0.11992142\n",
      "epoch: 6000/40000 loss_train: 0.08603146 + 0.00000000 = 0.08603146 loss_val = 0.08603146\n",
      "epoch: 9000/40000 loss_train: 0.07379684 + 0.00000000 = 0.07379684 loss_val = 0.07379684\n",
      "epoch: 12000/40000 loss_train: 0.06418463 + 0.00000000 = 0.06418463 loss_val = 0.06418463\n",
      "epoch: 15000/40000 loss_train: 0.05644756 + 0.00000000 = 0.05644756 loss_val = 0.05644756\n",
      "epoch: 18000/40000 loss_train: 0.05028505 + 0.00000000 = 0.05028505 loss_val = 0.05028505\n",
      "epoch: 21000/40000 loss_train: 0.04543479 + 0.00000000 = 0.04543479 loss_val = 0.04543479\n",
      "To 12 neurons, 1 layer(s),  0.005 learning rate, 4 batch size, 0.1 dropout rate, we have: \u001b[1m Accuracy: 97.62% \u001b[0m\n",
      "epoch:    0/40000 loss_train: 0.80220540 + 0.00000000 = 0.80220540 loss_val = 0.80220540\n",
      "epoch: 3000/40000 loss_train: 0.15775748 + 0.00000000 = 0.15775748 loss_val = 0.15775748\n",
      "epoch: 6000/40000 loss_train: 0.07008747 + 0.00000000 = 0.07008747 loss_val = 0.07008747\n",
      "epoch: 9000/40000 loss_train: 0.04985771 + 0.00000000 = 0.04985771 loss_val = 0.04985771\n",
      "epoch: 12000/40000 loss_train: 0.03996213 + 0.00000000 = 0.03996213 loss_val = 0.03996213\n",
      "epoch: 15000/40000 loss_train: 0.03331888 + 0.00000000 = 0.03331888 loss_val = 0.03331888\n",
      "epoch: 18000/40000 loss_train: 0.02827285 + 0.00000000 = 0.02827285 loss_val = 0.02827285\n",
      "epoch: 21000/40000 loss_train: 0.02419025 + 0.00000000 = 0.02419025 loss_val = 0.02419025\n",
      "epoch: 24000/40000 loss_train: 0.02080755 + 0.00000000 = 0.02080755 loss_val = 0.02080755\n",
      "epoch: 27000/40000 loss_train: 0.01797305 + 0.00000000 = 0.01797305 loss_val = 0.01797305\n",
      "epoch: 30000/40000 loss_train: 0.01556889 + 0.00000000 = 0.01556889 loss_val = 0.01556889\n",
      "epoch: 33000/40000 loss_train: 0.01352900 + 0.00000000 = 0.01352900 loss_val = 0.01352900\n",
      "epoch: 36000/40000 loss_train: 0.01179588 + 0.00000000 = 0.01179588 loss_val = 0.01179588\n",
      "epoch: 39000/40000 loss_train: 0.01031933 + 0.00000000 = 0.01031933 loss_val = 0.01031933\n",
      "To 12 neurons, 1 layer(s),  0.005 learning rate, 8 batch size, 0 dropout rate, we have: \u001b[1m Accuracy: 97.62% \u001b[0m\n",
      "epoch:    0/40000 loss_train: 1.32226096 + 0.00000000 = 1.32226096 loss_val = 1.32226096\n",
      "epoch: 3000/40000 loss_train: 0.13524981 + 0.00000000 = 0.13524981 loss_val = 0.13524981\n",
      "epoch: 6000/40000 loss_train: 0.09189387 + 0.00000000 = 0.09189387 loss_val = 0.09189387\n",
      "To 12 neurons, 1 layer(s),  0.005 learning rate, 8 batch size, 0.1 dropout rate, we have: \u001b[1m Accuracy: 95.24% \u001b[0m\n",
      "epoch:    0/40000 loss_train: 0.78716552 + 0.00000000 = 0.78716552 loss_val = 0.78716552\n",
      "epoch: 3000/40000 loss_train: 0.08480531 + 0.00000000 = 0.08480531 loss_val = 0.08480531\n",
      "epoch: 6000/40000 loss_train: 0.03331269 + 0.00000000 = 0.03331269 loss_val = 0.03331269\n",
      "epoch: 9000/40000 loss_train: 0.02150786 + 0.00000000 = 0.02150786 loss_val = 0.02150786\n",
      "epoch: 12000/40000 loss_train: 0.01547494 + 0.00000000 = 0.01547494 loss_val = 0.01547494\n",
      "epoch: 15000/40000 loss_train: 0.01160142 + 0.00000000 = 0.01160142 loss_val = 0.01160142\n",
      "epoch: 18000/40000 loss_train: 0.00892487 + 0.00000000 = 0.00892487 loss_val = 0.00892487\n",
      "epoch: 21000/40000 loss_train: 0.00704893 + 0.00000000 = 0.00704893 loss_val = 0.00704893\n",
      "epoch: 24000/40000 loss_train: 0.00569531 + 0.00000000 = 0.00569531 loss_val = 0.00569531\n",
      "epoch: 27000/40000 loss_train: 0.00469647 + 0.00000000 = 0.00469647 loss_val = 0.00469647\n",
      "epoch: 30000/40000 loss_train: 0.00389601 + 0.00000000 = 0.00389601 loss_val = 0.00389601\n",
      "epoch: 33000/40000 loss_train: 0.00327866 + 0.00000000 = 0.00327866 loss_val = 0.00327866\n",
      "epoch: 36000/40000 loss_train: 0.00280923 + 0.00000000 = 0.00280923 loss_val = 0.00280923\n",
      "epoch: 39000/40000 loss_train: 0.00244007 + 0.00000000 = 0.00244007 loss_val = 0.00244007\n",
      "To 12 neurons, 1 layer(s),  0.01 learning rate, 0 batch size, 0 dropout rate, we have: \u001b[1m Accuracy: 97.62% \u001b[0m\n",
      "epoch:    0/40000 loss_train: 0.90484125 + 0.00000000 = 0.90484125 loss_val = 0.90484125\n",
      "epoch: 3000/40000 loss_train: 0.08582273 + 0.00000000 = 0.08582273 loss_val = 0.08582273\n",
      "epoch: 6000/40000 loss_train: 0.05146328 + 0.00000000 = 0.05146328 loss_val = 0.05146328\n",
      "epoch: 9000/40000 loss_train: 0.03714941 + 0.00000000 = 0.03714941 loss_val = 0.03714941\n",
      "epoch: 12000/40000 loss_train: 0.02731389 + 0.00000000 = 0.02731389 loss_val = 0.02731389\n",
      "epoch: 15000/40000 loss_train: 0.01975790 + 0.00000000 = 0.01975790 loss_val = 0.01975790\n",
      "epoch: 18000/40000 loss_train: 0.01375431 + 0.00000000 = 0.01375431 loss_val = 0.01375431\n",
      "To 12 neurons, 1 layer(s),  0.01 learning rate, 0 batch size, 0.1 dropout rate, we have: \u001b[1m Accuracy: 97.62% \u001b[0m\n",
      "epoch:    0/40000 loss_train: 0.72512059 + 0.00000000 = 0.72512059 loss_val = 0.72512059\n",
      "epoch: 3000/40000 loss_train: 0.06296321 + 0.00000000 = 0.06296321 loss_val = 0.06296321\n",
      "epoch: 6000/40000 loss_train: 0.04071137 + 0.00000000 = 0.04071137 loss_val = 0.04071137\n",
      "epoch: 9000/40000 loss_train: 0.02947753 + 0.00000000 = 0.02947753 loss_val = 0.02947753\n",
      "epoch: 12000/40000 loss_train: 0.02186785 + 0.00000000 = 0.02186785 loss_val = 0.02186785\n",
      "epoch: 15000/40000 loss_train: 0.01634259 + 0.00000000 = 0.01634259 loss_val = 0.01634259\n",
      "epoch: 18000/40000 loss_train: 0.01213911 + 0.00000000 = 0.01213911 loss_val = 0.01213911\n",
      "epoch: 21000/40000 loss_train: 0.00908439 + 0.00000000 = 0.00908439 loss_val = 0.00908439\n",
      "epoch: 24000/40000 loss_train: 0.00696087 + 0.00000000 = 0.00696087 loss_val = 0.00696087\n",
      "epoch: 27000/40000 loss_train: 0.00546456 + 0.00000000 = 0.00546456 loss_val = 0.00546456\n",
      "epoch: 30000/40000 loss_train: 0.00439244 + 0.00000000 = 0.00439244 loss_val = 0.00439244\n",
      "epoch: 33000/40000 loss_train: 0.00360933 + 0.00000000 = 0.00360933 loss_val = 0.00360933\n",
      "epoch: 36000/40000 loss_train: 0.00302298 + 0.00000000 = 0.00302298 loss_val = 0.00302298\n",
      "epoch: 39000/40000 loss_train: 0.00257382 + 0.00000000 = 0.00257382 loss_val = 0.00257382\n",
      "To 12 neurons, 1 layer(s),  0.01 learning rate, 4 batch size, 0 dropout rate, we have: \u001b[1m Accuracy: 100.00% \u001b[0m\n",
      "epoch:    0/40000 loss_train: 0.82467255 + 0.00000000 = 0.82467255 loss_val = 0.82467255\n",
      "epoch: 3000/40000 loss_train: 0.08452107 + 0.00000000 = 0.08452107 loss_val = 0.08452107\n",
      "epoch: 6000/40000 loss_train: 0.05962804 + 0.00000000 = 0.05962804 loss_val = 0.05962804\n",
      "epoch: 9000/40000 loss_train: 0.04506280 + 0.00000000 = 0.04506280 loss_val = 0.04506280\n",
      "epoch: 12000/40000 loss_train: 0.03664290 + 0.00000000 = 0.03664290 loss_val = 0.03664290\n",
      "To 12 neurons, 1 layer(s),  0.01 learning rate, 4 batch size, 0.1 dropout rate, we have: \u001b[1m Accuracy: 97.62% \u001b[0m\n",
      "epoch:    0/40000 loss_train: 0.73288726 + 0.00000000 = 0.73288726 loss_val = 0.73288726\n",
      "epoch: 3000/40000 loss_train: 0.07211480 + 0.00000000 = 0.07211480 loss_val = 0.07211480\n",
      "epoch: 6000/40000 loss_train: 0.04032825 + 0.00000000 = 0.04032825 loss_val = 0.04032825\n",
      "epoch: 9000/40000 loss_train: 0.02703471 + 0.00000000 = 0.02703471 loss_val = 0.02703471\n",
      "epoch: 12000/40000 loss_train: 0.01975348 + 0.00000000 = 0.01975348 loss_val = 0.01975348\n",
      "epoch: 15000/40000 loss_train: 0.01467914 + 0.00000000 = 0.01467914 loss_val = 0.01467914\n",
      "epoch: 18000/40000 loss_train: 0.01111400 + 0.00000000 = 0.01111400 loss_val = 0.01111400\n",
      "epoch: 21000/40000 loss_train: 0.00857179 + 0.00000000 = 0.00857179 loss_val = 0.00857179\n",
      "epoch: 24000/40000 loss_train: 0.00676247 + 0.00000000 = 0.00676247 loss_val = 0.00676247\n",
      "epoch: 27000/40000 loss_train: 0.00545205 + 0.00000000 = 0.00545205 loss_val = 0.00545205\n",
      "epoch: 30000/40000 loss_train: 0.00448690 + 0.00000000 = 0.00448690 loss_val = 0.00448690\n",
      "epoch: 33000/40000 loss_train: 0.00376052 + 0.00000000 = 0.00376052 loss_val = 0.00376052\n",
      "epoch: 36000/40000 loss_train: 0.00318276 + 0.00000000 = 0.00318276 loss_val = 0.00318276\n",
      "epoch: 39000/40000 loss_train: 0.00272542 + 0.00000000 = 0.00272542 loss_val = 0.00272542\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To 12 neurons, 1 layer(s),  0.01 learning rate, 8 batch size, 0 dropout rate, we have: \u001b[1m Accuracy: 100.00% \u001b[0m\n",
      "epoch:    0/40000 loss_train: 1.30952680 + 0.00000000 = 1.30952680 loss_val = 1.30952680\n",
      "epoch: 3000/40000 loss_train: 0.06365851 + 0.00000000 = 0.06365851 loss_val = 0.06365851\n",
      "epoch: 6000/40000 loss_train: 0.03632744 + 0.00000000 = 0.03632744 loss_val = 0.03632744\n",
      "epoch: 9000/40000 loss_train: 0.02362842 + 0.00000000 = 0.02362842 loss_val = 0.02362842\n",
      "epoch: 12000/40000 loss_train: 0.01566564 + 0.00000000 = 0.01566564 loss_val = 0.01566564\n",
      "epoch: 15000/40000 loss_train: 0.01084411 + 0.00000000 = 0.01084411 loss_val = 0.01084411\n",
      "epoch: 18000/40000 loss_train: 0.00777305 + 0.00000000 = 0.00777305 loss_val = 0.00777305\n",
      "epoch: 21000/40000 loss_train: 0.00575764 + 0.00000000 = 0.00575764 loss_val = 0.00575764\n",
      "To 12 neurons, 1 layer(s),  0.01 learning rate, 8 batch size, 0.1 dropout rate, we have: \u001b[1m Accuracy: 97.62% \u001b[0m\n",
      "epoch:    0/40000 loss_train: 0.88786954 + 0.00000000 = 0.88786954 loss_val = 0.88786954\n",
      "epoch: 3000/40000 loss_train: 0.62711965 + 0.00000000 = 0.62711965 loss_val = 0.62711965\n",
      "epoch: 6000/40000 loss_train: 0.37722622 + 0.00000000 = 0.37722622 loss_val = 0.37722622\n",
      "epoch: 9000/40000 loss_train: 0.22496948 + 0.00000000 = 0.22496948 loss_val = 0.22496948\n",
      "epoch: 12000/40000 loss_train: 0.14932201 + 0.00000000 = 0.14932201 loss_val = 0.14932201\n",
      "epoch: 15000/40000 loss_train: 0.10516036 + 0.00000000 = 0.10516036 loss_val = 0.10516036\n",
      "epoch: 18000/40000 loss_train: 0.08162815 + 0.00000000 = 0.08162815 loss_val = 0.08162815\n",
      "epoch: 21000/40000 loss_train: 0.06825445 + 0.00000000 = 0.06825445 loss_val = 0.06825445\n",
      "epoch: 24000/40000 loss_train: 0.05965216 + 0.00000000 = 0.05965216 loss_val = 0.05965216\n",
      "epoch: 27000/40000 loss_train: 0.05344563 + 0.00000000 = 0.05344563 loss_val = 0.05344563\n",
      "epoch: 30000/40000 loss_train: 0.04841074 + 0.00000000 = 0.04841074 loss_val = 0.04841074\n",
      "epoch: 33000/40000 loss_train: 0.04434755 + 0.00000000 = 0.04434755 loss_val = 0.04434755\n",
      "epoch: 36000/40000 loss_train: 0.04092314 + 0.00000000 = 0.04092314 loss_val = 0.04092314\n",
      "epoch: 39000/40000 loss_train: 0.03791869 + 0.00000000 = 0.03791869 loss_val = 0.03791869\n",
      "To 7 neurons, 2 layer(s),  0.001 learning rate, 0 batch size, 0 dropout rate, we have: \u001b[1m Accuracy: 97.62% \u001b[0m\n",
      "epoch:    0/40000 loss_train: 0.75696880 + 0.00000000 = 0.75696880 loss_val = 0.75696880\n",
      "epoch: 3000/40000 loss_train: 0.65294314 + 0.00000000 = 0.65294314 loss_val = 0.65294314\n",
      "epoch: 6000/40000 loss_train: 0.62241299 + 0.00000000 = 0.62241299 loss_val = 0.62241299\n",
      "epoch: 9000/40000 loss_train: 0.56351011 + 0.00000000 = 0.56351011 loss_val = 0.56351011\n",
      "epoch: 12000/40000 loss_train: 0.44248269 + 0.00000000 = 0.44248269 loss_val = 0.44248269\n",
      "epoch: 15000/40000 loss_train: 0.32150295 + 0.00000000 = 0.32150295 loss_val = 0.32150295\n",
      "epoch: 18000/40000 loss_train: 0.23087897 + 0.00000000 = 0.23087897 loss_val = 0.23087897\n",
      "epoch: 21000/40000 loss_train: 0.17358876 + 0.00000000 = 0.17358876 loss_val = 0.17358876\n",
      "epoch: 24000/40000 loss_train: 0.12641783 + 0.00000000 = 0.12641783 loss_val = 0.12641783\n",
      "epoch: 27000/40000 loss_train: 0.09414124 + 0.00000000 = 0.09414124 loss_val = 0.09414124\n",
      "epoch: 30000/40000 loss_train: 0.07387935 + 0.00000000 = 0.07387935 loss_val = 0.07387935\n",
      "epoch: 33000/40000 loss_train: 0.05978166 + 0.00000000 = 0.05978166 loss_val = 0.05978166\n",
      "epoch: 36000/40000 loss_train: 0.04774912 + 0.00000000 = 0.04774912 loss_val = 0.04774912\n",
      "epoch: 39000/40000 loss_train: 0.04020505 + 0.00000000 = 0.04020505 loss_val = 0.04020505\n",
      "To 7 neurons, 2 layer(s),  0.001 learning rate, 0 batch size, 0.1 dropout rate, we have: \u001b[1m Accuracy: 100.00% \u001b[0m\n",
      "epoch:    0/40000 loss_train: 0.74812001 + 0.00000000 = 0.74812001 loss_val = 0.74812001\n",
      "epoch: 3000/40000 loss_train: 0.64765332 + 0.00000000 = 0.64765332 loss_val = 0.64765332\n",
      "epoch: 6000/40000 loss_train: 0.51119842 + 0.00000000 = 0.51119842 loss_val = 0.51119842\n",
      "epoch: 9000/40000 loss_train: 0.32530055 + 0.00000000 = 0.32530055 loss_val = 0.32530055\n",
      "epoch: 12000/40000 loss_train: 0.26377820 + 0.00000000 = 0.26377820 loss_val = 0.26377820\n",
      "epoch: 15000/40000 loss_train: 0.23837881 + 0.00000000 = 0.23837881 loss_val = 0.23837881\n",
      "epoch: 18000/40000 loss_train: 0.22031433 + 0.00000000 = 0.22031433 loss_val = 0.22031433\n",
      "epoch: 21000/40000 loss_train: 0.20575781 + 0.00000000 = 0.20575781 loss_val = 0.20575781\n",
      "epoch: 24000/40000 loss_train: 0.19227707 + 0.00000000 = 0.19227707 loss_val = 0.19227707\n",
      "epoch: 27000/40000 loss_train: 0.17625748 + 0.00000000 = 0.17625748 loss_val = 0.17625748\n",
      "epoch: 30000/40000 loss_train: 0.15803281 + 0.00000000 = 0.15803281 loss_val = 0.15803281\n",
      "epoch: 33000/40000 loss_train: 0.13701515 + 0.00000000 = 0.13701515 loss_val = 0.13701515\n",
      "epoch: 36000/40000 loss_train: 0.11908073 + 0.00000000 = 0.11908073 loss_val = 0.11908073\n",
      "epoch: 39000/40000 loss_train: 0.10326992 + 0.00000000 = 0.10326992 loss_val = 0.10326992\n",
      "To 7 neurons, 2 layer(s),  0.001 learning rate, 4 batch size, 0 dropout rate, we have: \u001b[1m Accuracy: 97.62% \u001b[0m\n",
      "epoch:    0/40000 loss_train: 0.76792075 + 0.00000000 = 0.76792075 loss_val = 0.76792075\n",
      "epoch: 3000/40000 loss_train: 0.56318947 + 0.00000000 = 0.56318947 loss_val = 0.56318947\n",
      "epoch: 6000/40000 loss_train: 0.44557730 + 0.00000000 = 0.44557730 loss_val = 0.44557730\n",
      "epoch: 9000/40000 loss_train: 0.35545480 + 0.00000000 = 0.35545480 loss_val = 0.35545480\n",
      "epoch: 12000/40000 loss_train: 0.29589951 + 0.00000000 = 0.29589951 loss_val = 0.29589951\n",
      "epoch: 15000/40000 loss_train: 0.26200106 + 0.00000000 = 0.26200106 loss_val = 0.26200106\n",
      "epoch: 18000/40000 loss_train: 0.24393879 + 0.00000000 = 0.24393879 loss_val = 0.24393879\n",
      "epoch: 21000/40000 loss_train: 0.23286981 + 0.00000000 = 0.23286981 loss_val = 0.23286981\n",
      "epoch: 24000/40000 loss_train: 0.22520047 + 0.00000000 = 0.22520047 loss_val = 0.22520047\n",
      "epoch: 27000/40000 loss_train: 0.21931328 + 0.00000000 = 0.21931328 loss_val = 0.21931328\n",
      "epoch: 30000/40000 loss_train: 0.21466306 + 0.00000000 = 0.21466306 loss_val = 0.21466306\n",
      "epoch: 33000/40000 loss_train: 0.21115992 + 0.00000000 = 0.21115992 loss_val = 0.21115992\n",
      "epoch: 36000/40000 loss_train: 0.20821699 + 0.00000000 = 0.20821699 loss_val = 0.20821699\n",
      "epoch: 39000/40000 loss_train: 0.20423350 + 0.00000000 = 0.20423350 loss_val = 0.20423350\n",
      "To 7 neurons, 2 layer(s),  0.001 learning rate, 4 batch size, 0.1 dropout rate, we have: \u001b[1m Accuracy: 88.10% \u001b[0m\n",
      "epoch:    0/40000 loss_train: 0.78214928 + 0.00000000 = 0.78214928 loss_val = 0.78214928\n",
      "epoch: 3000/40000 loss_train: 0.63295828 + 0.00000000 = 0.63295828 loss_val = 0.63295828\n",
      "epoch: 6000/40000 loss_train: 0.49552584 + 0.00000000 = 0.49552584 loss_val = 0.49552584\n",
      "epoch: 9000/40000 loss_train: 0.32919967 + 0.00000000 = 0.32919967 loss_val = 0.32919967\n",
      "epoch: 12000/40000 loss_train: 0.26919251 + 0.00000000 = 0.26919251 loss_val = 0.26919251\n",
      "epoch: 15000/40000 loss_train: 0.23748449 + 0.00000000 = 0.23748449 loss_val = 0.23748449\n",
      "epoch: 18000/40000 loss_train: 0.19321686 + 0.00000000 = 0.19321686 loss_val = 0.19321686\n",
      "epoch: 21000/40000 loss_train: 0.15670461 + 0.00000000 = 0.15670461 loss_val = 0.15670461\n",
      "epoch: 24000/40000 loss_train: 0.12707257 + 0.00000000 = 0.12707257 loss_val = 0.12707257\n",
      "epoch: 27000/40000 loss_train: 0.10548139 + 0.00000000 = 0.10548139 loss_val = 0.10548139\n",
      "epoch: 30000/40000 loss_train: 0.08926093 + 0.00000000 = 0.08926093 loss_val = 0.08926093\n",
      "epoch: 33000/40000 loss_train: 0.07612523 + 0.00000000 = 0.07612523 loss_val = 0.07612523\n",
      "epoch: 36000/40000 loss_train: 0.06473924 + 0.00000000 = 0.06473924 loss_val = 0.06473924\n",
      "epoch: 39000/40000 loss_train: 0.05546333 + 0.00000000 = 0.05546333 loss_val = 0.05546333\n",
      "To 7 neurons, 2 layer(s),  0.001 learning rate, 8 batch size, 0 dropout rate, we have: \u001b[1m Accuracy: 97.62% \u001b[0m\n",
      "epoch:    0/40000 loss_train: 0.80326860 + 0.00000000 = 0.80326860 loss_val = 0.80326860\n",
      "epoch: 3000/40000 loss_train: 0.63349906 + 0.00000000 = 0.63349906 loss_val = 0.63349906\n",
      "epoch: 6000/40000 loss_train: 0.55326276 + 0.00000000 = 0.55326276 loss_val = 0.55326276\n",
      "epoch: 9000/40000 loss_train: 0.47908870 + 0.00000000 = 0.47908870 loss_val = 0.47908870\n",
      "epoch: 12000/40000 loss_train: 0.34795119 + 0.00000000 = 0.34795119 loss_val = 0.34795119\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 15000/40000 loss_train: 0.20278952 + 0.00000000 = 0.20278952 loss_val = 0.20278952\n",
      "epoch: 18000/40000 loss_train: 0.13231186 + 0.00000000 = 0.13231186 loss_val = 0.13231186\n",
      "epoch: 21000/40000 loss_train: 0.10407182 + 0.00000000 = 0.10407182 loss_val = 0.10407182\n",
      "epoch: 24000/40000 loss_train: 0.09036703 + 0.00000000 = 0.09036703 loss_val = 0.09036703\n",
      "epoch: 27000/40000 loss_train: 0.08097571 + 0.00000000 = 0.08097571 loss_val = 0.08097571\n",
      "epoch: 30000/40000 loss_train: 0.07148854 + 0.00000000 = 0.07148854 loss_val = 0.07148854\n",
      "epoch: 33000/40000 loss_train: 0.06511202 + 0.00000000 = 0.06511202 loss_val = 0.06511202\n",
      "epoch: 36000/40000 loss_train: 0.05878833 + 0.00000000 = 0.05878833 loss_val = 0.05878833\n",
      "epoch: 39000/40000 loss_train: 0.05365536 + 0.00000000 = 0.05365536 loss_val = 0.05365536\n",
      "To 7 neurons, 2 layer(s),  0.001 learning rate, 8 batch size, 0.1 dropout rate, we have: \u001b[1m Accuracy: 97.62% \u001b[0m\n",
      "epoch:    0/40000 loss_train: 0.92974046 + 0.00000000 = 0.92974046 loss_val = 0.92974046\n",
      "epoch: 3000/40000 loss_train: 0.09333187 + 0.00000000 = 0.09333187 loss_val = 0.09333187\n",
      "epoch: 6000/40000 loss_train: 0.04818047 + 0.00000000 = 0.04818047 loss_val = 0.04818047\n",
      "epoch: 9000/40000 loss_train: 0.03501111 + 0.00000000 = 0.03501111 loss_val = 0.03501111\n",
      "epoch: 12000/40000 loss_train: 0.02623578 + 0.00000000 = 0.02623578 loss_val = 0.02623578\n",
      "epoch: 15000/40000 loss_train: 0.01858886 + 0.00000000 = 0.01858886 loss_val = 0.01858886\n",
      "epoch: 18000/40000 loss_train: 0.01311516 + 0.00000000 = 0.01311516 loss_val = 0.01311516\n",
      "epoch: 21000/40000 loss_train: 0.00934369 + 0.00000000 = 0.00934369 loss_val = 0.00934369\n",
      "epoch: 24000/40000 loss_train: 0.00674774 + 0.00000000 = 0.00674774 loss_val = 0.00674774\n",
      "epoch: 27000/40000 loss_train: 0.00500647 + 0.00000000 = 0.00500647 loss_val = 0.00500647\n",
      "epoch: 30000/40000 loss_train: 0.00382257 + 0.00000000 = 0.00382257 loss_val = 0.00382257\n",
      "epoch: 33000/40000 loss_train: 0.00300822 + 0.00000000 = 0.00300822 loss_val = 0.00300822\n",
      "epoch: 36000/40000 loss_train: 0.00242718 + 0.00000000 = 0.00242718 loss_val = 0.00242718\n",
      "epoch: 39000/40000 loss_train: 0.00200512 + 0.00000000 = 0.00200512 loss_val = 0.00200512\n",
      "To 7 neurons, 2 layer(s),  0.005 learning rate, 0 batch size, 0 dropout rate, we have: \u001b[1m Accuracy: 97.62% \u001b[0m\n",
      "epoch:    0/40000 loss_train: 1.03471027 + 0.00000000 = 1.03471027 loss_val = 1.03471027\n",
      "epoch: 3000/40000 loss_train: 0.14463960 + 0.00000000 = 0.14463960 loss_val = 0.14463960\n",
      "epoch: 6000/40000 loss_train: 0.05192311 + 0.00000000 = 0.05192311 loss_val = 0.05192311\n",
      "epoch: 9000/40000 loss_train: 0.02106983 + 0.00000000 = 0.02106983 loss_val = 0.02106983\n",
      "To 7 neurons, 2 layer(s),  0.005 learning rate, 0 batch size, 0.1 dropout rate, we have: \u001b[1m Accuracy: 100.00% \u001b[0m\n",
      "epoch:    0/40000 loss_train: 1.42943032 + 0.00000000 = 1.42943032 loss_val = 1.42943032\n",
      "epoch: 3000/40000 loss_train: 0.09532725 + 0.00000000 = 0.09532725 loss_val = 0.09532725\n",
      "epoch: 6000/40000 loss_train: 0.07178883 + 0.00000000 = 0.07178883 loss_val = 0.07178883\n",
      "epoch: 9000/40000 loss_train: 0.06502155 + 0.00000000 = 0.06502155 loss_val = 0.06502155\n",
      "epoch: 12000/40000 loss_train: 0.06151010 + 0.00000000 = 0.06151010 loss_val = 0.06151010\n",
      "epoch: 15000/40000 loss_train: 0.05905514 + 0.00000000 = 0.05905514 loss_val = 0.05905514\n",
      "epoch: 18000/40000 loss_train: 0.05697390 + 0.00000000 = 0.05697390 loss_val = 0.05697390\n",
      "epoch: 21000/40000 loss_train: 0.05240831 + 0.00000000 = 0.05240831 loss_val = 0.05240831\n",
      "epoch: 24000/40000 loss_train: 0.04595511 + 0.00000000 = 0.04595511 loss_val = 0.04595511\n",
      "epoch: 27000/40000 loss_train: 0.03546134 + 0.00000000 = 0.03546134 loss_val = 0.03546134\n",
      "epoch: 30000/40000 loss_train: 0.02255956 + 0.00000000 = 0.02255956 loss_val = 0.02255956\n",
      "epoch: 33000/40000 loss_train: 0.01350366 + 0.00000000 = 0.01350366 loss_val = 0.01350366\n",
      "epoch: 36000/40000 loss_train: 0.00857449 + 0.00000000 = 0.00857449 loss_val = 0.00857449\n",
      "epoch: 39000/40000 loss_train: 0.00591544 + 0.00000000 = 0.00591544 loss_val = 0.00591544\n",
      "To 7 neurons, 2 layer(s),  0.005 learning rate, 4 batch size, 0 dropout rate, we have: \u001b[1m Accuracy: 97.62% \u001b[0m\n",
      "epoch:    0/40000 loss_train: 0.76108732 + 0.00000000 = 0.76108732 loss_val = 0.76108732\n",
      "epoch: 3000/40000 loss_train: 0.22627773 + 0.00000000 = 0.22627773 loss_val = 0.22627773\n",
      "epoch: 6000/40000 loss_train: 0.06394906 + 0.00000000 = 0.06394906 loss_val = 0.06394906\n",
      "To 7 neurons, 2 layer(s),  0.005 learning rate, 4 batch size, 0.1 dropout rate, we have: \u001b[1m Accuracy: 97.62% \u001b[0m\n",
      "epoch:    0/40000 loss_train: 0.68609888 + 0.00000000 = 0.68609888 loss_val = 0.68609888\n",
      "epoch: 3000/40000 loss_train: 0.13011875 + 0.00000000 = 0.13011875 loss_val = 0.13011875\n",
      "epoch: 6000/40000 loss_train: 0.03235822 + 0.00000000 = 0.03235822 loss_val = 0.03235822\n",
      "epoch: 9000/40000 loss_train: 0.01769474 + 0.00000000 = 0.01769474 loss_val = 0.01769474\n",
      "epoch: 12000/40000 loss_train: 0.01141000 + 0.00000000 = 0.01141000 loss_val = 0.01141000\n",
      "epoch: 15000/40000 loss_train: 0.00794911 + 0.00000000 = 0.00794911 loss_val = 0.00794911\n",
      "epoch: 18000/40000 loss_train: 0.00571527 + 0.00000000 = 0.00571527 loss_val = 0.00571527\n",
      "epoch: 21000/40000 loss_train: 0.00409663 + 0.00000000 = 0.00409663 loss_val = 0.00409663\n",
      "epoch: 24000/40000 loss_train: 0.00315155 + 0.00000000 = 0.00315155 loss_val = 0.00315155\n",
      "epoch: 27000/40000 loss_train: 0.00251267 + 0.00000000 = 0.00251267 loss_val = 0.00251267\n",
      "epoch: 30000/40000 loss_train: 0.00205827 + 0.00000000 = 0.00205827 loss_val = 0.00205827\n",
      "epoch: 33000/40000 loss_train: 0.00172301 + 0.00000000 = 0.00172301 loss_val = 0.00172301\n",
      "epoch: 36000/40000 loss_train: 0.00147128 + 0.00000000 = 0.00147128 loss_val = 0.00147128\n",
      "epoch: 39000/40000 loss_train: 0.00127633 + 0.00000000 = 0.00127633 loss_val = 0.00127633\n",
      "To 7 neurons, 2 layer(s),  0.005 learning rate, 8 batch size, 0 dropout rate, we have: \u001b[1m Accuracy: 97.62% \u001b[0m\n",
      "epoch:    0/40000 loss_train: 0.78998761 + 0.00000000 = 0.78998761 loss_val = 0.78998761\n",
      "epoch: 3000/40000 loss_train: 0.24510774 + 0.00000000 = 0.24510774 loss_val = 0.24510774\n",
      "epoch: 6000/40000 loss_train: 0.12026564 + 0.00000000 = 0.12026564 loss_val = 0.12026564\n"
     ]
    }
   ],
   "source": [
    "lst_hyperP = []\n",
    "lst_ANN = []\n",
    "for num_hidden_layers in hidden_layers:\n",
    "    for num_neurons_layers in neurons:\n",
    "        for num_learning_rate in learning_rate:\n",
    "            for num_batch_size in batch_size:\n",
    "                for prob_dropout in dropout_rate:\n",
    "                    nn = setBestNeuralNetwork(num_hidden_layers,num_neurons_layers, num_learning_rate, prob_dropout, input_dim, output_dim)\n",
    "                    \n",
    "                    nn.fit(X_train, y_train, epochs=40000,batch_gen= rna.batch_shuffle, batch_size=num_batch_size, verbose=3000)\n",
    "                \n",
    "                    y_pred = nn.predict(X_val)\n",
    "                    accu = 100*accuracy_score(y_val, y_pred > 0.5)\n",
    "                    #accu = 100*roc_auc_score(y_val, y_pred > 0.5)\n",
    "                    lst_hyperP.append(hyperparametros(accu,num_hidden_layers,num_neurons_layers,num_learning_rate,num_batch_size,prob_dropout) )\n",
    "                    lst_ANN.append(ANN_accuracy(accu,nn))\n",
    "                    print('To {} neurons, {} layer(s),  {} learning rate, {} batch size, {} dropout rate, we have: \\033[1m Accuracy: {:.2f}% \\033[0m'.format(num_neurons_layers,num_hidden_layers,num_learning_rate,num_batch_size,prob_dropout, accu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hyper = max(lst_hyperP, key=lambda hyper: hyper.acuracia)\n",
    "best_of_best_hyper.append( hyperparametros(best_hyper.acuracia,best_hyper.camadas,best_hyper.neuronios, best_hyper.learning_rate,best_hyper.batch_size,best_hyper.dropout) )\n",
    "print('{:.2f}% was the best \\033[1m accuracy\\033[0m, having {} layers,{} neurons, {} of learning rate and {} of batch size as hyperparameters!'.format(best_hyper.acuracia,best_hyper.camadas,best_hyper.neuronios, best_hyper.learning_rate,best_hyper.batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_ann = max(lst_ANN, key=lambda hyper: hyper.acuracia)\n",
    "best_of_best_ANN.append(ANN_accuracy(best_ann.acuracia,best_ann.ann))\n",
    "print('{:.2f}% was the best\\033[1m accuracy\\033[0m, with ANN {} '.format(best_ann.acuracia,best_ann.ann))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "lst_hyperP.sort(key = lambda hyper: hyper.acuracia)\n",
    "#[hyper.neuronios for hyper in lst_hyperP]\n",
    "for best_hyper in lst_hyperP: \n",
    "  print('{:.2f}% was the best\\033[1m accuracy\\033[0m, having {} layers,{} neurons, {} of learning rate, {} of batch size and {} dropout rate as hyperparameters!'.format(best_hyper.acuracia,best_hyper.camadas,best_hyper.neuronios, best_hyper.learning_rate,best_hyper.batch_size,best_hyper.dropout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_hyperP = []\n",
    "lst_ANN = []\n",
    "\n",
    "#for num_learning_rate in learning_rate:\n",
    "#    for num_batch_size in batch_size:\n",
    "#        nn = setBestNeuralNetwork2(num_learning_rate, input_dim, output_dim)\n",
    "\n",
    "#        nn.fit(X_train, y_train, epochs=46000,batch_gen= rna.batch_shuffle, batch_size=num_batch_size, verbose=3000)\n",
    "\n",
    "#        y_pred = nn.predict(X_val)\n",
    "#        accu = 100*accuracy_score(y_val, y_pred > 0.5)\n",
    "        #accu = 100*roc_auc_score(y_val, y_pred > 0.5)\n",
    "#        lst_hyperP.append( hyperparametros(accu,2,10,num_learning_rate,num_batch_size,0) )\n",
    "#        lst_ANN.append(ANN_accuracy(accu,nn))\n",
    "#        print('Para  {} learning rate, {} batch size, temos: \\033[1m Acurácia: {:.2f}% \\033[0m'.format(num_learning_rate,num_batch_size, accu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#best_hyper = max(lst_hyperP, key=lambda hyper: hyper.acuracia)\n",
    "#best_of_best_hyper.append( hyperparametros(best_hyper.acuracia,best_hyper.camadas,best_hyper.neuronios, best_hyper.learning_rate,best_hyper.batch_size,best_hyper.dropout) )\n",
    "#print('{:.2f}% foi a melhor\\033[1m acurácia\\033[0m, obtida com {} camadas,{} neurônios, {} de learning rate e {} de batch size!'.format(best_hyper.acuracia,best_hyper.camadas,best_hyper.neuronios, best_hyper.learning_rate,best_hyper.batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#best_ann2 = max(lst_ANN, key=lambda hyper: hyper.acuracia)\n",
    "#best_of_best_ANN.append(ANN_accuracy(best_ann2.acuracia,best_ann2.ann))\n",
    "#print('{:.2f}% foi a melhor\\033[1m acurácia\\033[0m, obtida com a rede {} '.format(best_ann2.acuracia,best_ann2.ann))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lst_hyperP.sort(key = lambda hyper: hyper.acuracia)\n",
    "#[hyper.neuronios for hyper in lst_hyperP]\n",
    "#for best_hyper in lst_hyperP: \n",
    "#  print('{:.2f}% foi a melhor\\033[1m acurácia\\033[0m, obtida com {} camadas, {} de learning rate, {} de batch size e {} dropout rate!'.format(best_hyper.acuracia,best_hyper.camadas, best_hyper.learning_rate,best_hyper.batch_size,best_hyper.dropout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lst_hyperP = []\n",
    "#lst_ANN = []\n",
    "\n",
    "#for num_learning_rate in learning_rate:\n",
    "#    for num_batch_size in batch_size:\n",
    "#        nn = setBestNeuralNetwork3(num_learning_rate, input_dim, output_dim)\n",
    "\n",
    "#        nn.fit(X_train, y_train, epochs=36000,batch_gen= rna.batch_shuffle, batch_size=num_batch_size, verbose=3000)\n",
    "\n",
    "#        y_pred = nn.predict(X_val)\n",
    "#        accu = 100*accuracy_score(y_val, y_pred > 0.5)\n",
    "#        lst_hyperP.append( hyperparametros(accu,2,20,num_learning_rate,num_batch_size,0) )\n",
    "#        lst_ANN.append(ANN_accuracy(accu,nn))\n",
    "#        print('Para  {} learning rate, {} batch size, temos: \\033[1m Acurácia: {:.2f}% \\033[0m'.format(num_learning_rate,num_batch_size, accu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#best_hyper = max(lst_hyperP, key=lambda hyper: hyper.acuracia)\n",
    "#best_of_best_hyper.append( hyperparametros(best_hyper.acuracia,best_hyper.camadas,best_hyper.neuronios, best_hyper.learning_rate,best_hyper.batch_size,best_hyper.dropout) )\n",
    "#print('{:.2f}% foi a melhor\\033[1m acurácia\\033[0m, obtida com {} camadas,{} neurônios, {} de learning rate e {} de batch size!'.format(best_hyper.acuracia,best_hyper.camadas,best_hyper.neuronios, best_hyper.learning_rate,best_hyper.batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#best_ann3 = max(lst_ANN, key=lambda hyper: hyper.acuracia)\n",
    "#best_of_best_ANN.append(ANN_accuracy(best_ann3.acuracia,best_ann3.ann))\n",
    "#print('{:.2f}% foi a melhor\\033[1m acurácia\\033[0m, obtida com a rede {} '.format(best_ann3.acuracia,best_ann3.ann))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lst_hyperP.sort(key = lambda hyper: hyper.acuracia)\n",
    "#[hyper.neuronios for hyper in lst_hyperP]\n",
    "#for best_hyper in lst_hyperP: \n",
    "#  print('{:.2f}% foi a melhor\\033[1m acurácia\\033[0m, obtida com {} camadas, {} de learning rate, {} de batch size e {} dropout rate!'.format(best_hyper.acuracia,best_hyper.camadas, best_hyper.learning_rate,best_hyper.batch_size,best_hyper.dropout))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://medium.com/mdr-inc/from-sgd-to-adam-c9fce513c4bb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_of_best_ANN_Test = []\n",
    "for best_RNA in best_of_best_ANN: \n",
    "  bestNN = best_RNA.ann\n",
    "  yhat_probs = bestNN.predict(X_test)\n",
    "  yhat_classes = (yhat_probs > 0.5)\n",
    "  accu = 100*accuracy_score(y_test,yhat_classes)\n",
    "  accMedia = (accu + best_RNA.acuracia) / 2\n",
    "  print('{:.2f}% accu_test - {:.2f}% foi a melhor\\033[1m acurácia\\033[0m, {:.2f} acc média {} RNA!'.format(accu,best_RNA.acuracia,accMedia,best_RNA.ann))\n",
    "  accu = accMedia  \n",
    "  best_of_best_ANN_Test.append(ANN_accuracy(accu,bestNN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for best_hyper in best_of_best_hyper: \n",
    "  print('{:.2f}% foi a melhor\\033[1m acurácia\\033[0m, obtida com {} camadas,{} neurônios, {} de learning rate, {} de batch size e {} dropout rate!'.format(best_hyper.acuracia,best_hyper.camadas,best_hyper.neuronios, best_hyper.learning_rate,best_hyper.batch_size,best_hyper.dropout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestNN_Test = max(best_of_best_ANN_Test, key=lambda hyper: hyper.acuracia)\n",
    "bestNN_Test = bestNN_Test.ann"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "yhat_probs = bestNN_Test.predict(X_test)\n",
    "yhat_classes = (yhat_probs > 0.5)\n",
    "accu = 100*accuracy_score(y_test,yhat_classes)\n",
    "print('{:.2f}%, acurácia de teste !'.format(accu))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Segunda abordagem - configura uma nova rede com os parametros obtidos da otimização por força bruta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestNN = max(best_of_best_ANN_Test, key=lambda hyper: hyper.acuracia)\n",
    "bestNN = bestNN.ann"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestHyper = max(best_of_best_hyper, key=lambda hyper: hyper.acuracia)\n",
    "print('{:.2f}% foi a melhor\\033[1m acurácia\\033[0m, obtida com {} camadas, {} de learning rate, {} de batch size e {} dropout rate!'.format(bestHyper.acuracia,bestHyper.camadas, bestHyper.learning_rate,bestHyper.batch_size,bestHyper.dropout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_probs = bestNN.predict(X_test)\n",
    "yhat_classes = (yhat_probs > 0.5)\n",
    "accu = 100*accuracy_score(y_test,yhat_classes)\n",
    "print('{:.2f}%, acurácia de teste !'.format(accu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "bstANN = bestNN\n",
    "#bst_hyper = bstANN.hyper\n",
    "print('{:.2f}% foi a melhor\\033[1m acurácia\\033[0m, obtida com {} camadas, {} de learning rate, {} de batch size e {} dropout rate!'.format(best_hyper.acuracia,best_hyper.camadas, best_hyper.learning_rate,best_hyper.batch_size,best_hyper.dropout))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving the best ANN architeture\n",
    " - Given the best set of hyperparameters, trained ANN is saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if accu >= 85:\n",
    "    print('{:.2f}%, excellent accuracy for test sample, ANN saved!'.format(accu))\n",
    "    bestNN.save('bestANN\\suscetibilidadeNN_guaruja.pkl')\n",
    "else:\n",
    "    print('Accuracy: {:.2f}% testing sample accuracy under .85!'.format(accu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = bestNN.predict(X_train)\n",
    "print('Accuracy: {:.2f}%'.format(100*accuracy_score(y_train, y_pred > 0.5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = bestNN.predict(X_val)\n",
    "print('Accuracy: {:.2f}%'.format(100*accuracy_score(y_val, y_pred > 0.5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = bestNN.predict(X_test)\n",
    "print('Accuracy: {:.2f}%'.format(100*accuracy_score(y_test, y_pred > 0.5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = bestNN.predict(X)\n",
    "print('Accuracy: {:.2f}%'.format(100*accuracy_score(y, y_pred > 0.5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Quando obtermos uma boa acurácia com os dados de teste, salvamos a rede devidamente treinada__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_probs = bestNN.predict(X_train)\n",
    "yhat_classes = (yhat_probs > 0.5)\n",
    "accu = 100*accuracy_score(y_train, yhat_classes)\n",
    "print('Acurácia: {:.2f}%'.format(accu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NetworkPerformance(y_train, yhat_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_probs = bestNN.predict(X_test)\n",
    "yhat_classes = (yhat_probs > 0.5)\n",
    "accu = 100*accuracy_score(y_test,yhat_classes)\n",
    "print('Acurácia: {:.2f}%'.format(accu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NetworkPerformance(y_test, yhat_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_probs = bestNN.predict(X_val)\n",
    "yhat_classes = (yhat_probs > 0.5)\n",
    "accu = 100*accuracy_score(y_val,yhat_classes)\n",
    "print('Acurácia: {:.2f}%'.format(accu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NetworkPerformance(y_val, yhat_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_probs = bestNN.predict(X)\n",
    "yhat_classes = (yhat_probs > 0.5)\n",
    "accu = 100*accuracy_score(y,yhat_classes)\n",
    "print('Acurácia: {:.2f}%'.format(accu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "NetworkPerformance(y, yhat_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printAUROC(y,yhat_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cf_matrix = confusion_matrix(y,(yhat_probs > 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "ax = sns.heatmap(cf_matrix, annot=True, cmap='Blues')\n",
    "\n",
    "ax.set_title('Confusion Matrix\\n\\n')\n",
    "ax.set_xlabel('Predicted occurrences\\nAcurácia={:0.2f}%'.format(100*accuracy_score(y, y_pred > 0.5)))\n",
    "ax.set_ylabel('Actual occurrences ')\n",
    "\n",
    "\n",
    "## Ticket labels - List must be in alphabetical order\n",
    "ax.xaxis.set_ticklabels(['False','True'])\n",
    "ax.yaxis.set_ticklabels(['False','True'])\n",
    "\n",
    "## Display the visualization of the Confusion Matrix.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_proprietario = None\n",
    "output_proprietario = dataset_original\n",
    "output_proprietario['score'] = yhat_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes = output_proprietario['slope']*10\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(14, 9))\n",
    "\n",
    "axes[0].scatter(output_proprietario['slope'], output_proprietario['elevation'],\n",
    "            c=(output_proprietario['class']), cmap='rainbow', alpha=0.3,\n",
    "            s=sizes, edgecolors='none')\n",
    "legend1 = axes[0].legend(*scatter.legend_elements(),\n",
    "                    loc=\"upper right\", title=\"landslide\")\n",
    "\n",
    "axes[1].scatter(output_proprietario['slope'], output_proprietario['elevation'],\n",
    "            c=(output_proprietario['score'] > 0.5), cmap='rainbow', alpha=0.3,\n",
    "            s=sizes, edgecolors='none')\n",
    "legend1 = axes[1].legend(*scatter.legend_elements(),\n",
    "                    loc=\"upper right\", title=\"landslide\")\n",
    "\n",
    "fig.tight_layout()\n",
    "text = 'deslizamentos preditos ' + 'acurácia de: {:.2f}%'.format(100*accuracy_score(y, y_pred > 0.5))\n",
    "axes[0].title.set_text('deslizamentos reais')\n",
    "axes[1].title.set_text(text)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora considerar __RepeatedStratifiedKfold__ para avaliação da rede\n",
    "\n",
    "ref. https://www.geeksforgeeks.org/stratified-k-fold-cross-validation/\n",
    "\n",
    "ref. https://medium.com/@venkatasujit272/overview-of-cross-validation-3785d5414ece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Teste com objeto StratifiedKFold.\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "import statistics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X = dataset.drop('class',axis=1)\n",
    "#y= dataset['class']\n",
    "#X = X.to_numpy()    #converts dataframe into array to be used at NN\n",
    "#y = y.to_numpy()    #converts dataframe into array to be used at NN\n",
    "#y = y.reshape(-1,1) #reorganiza o array em um array 1 x 1\n",
    "\n",
    "#normalização do dataset\n",
    "#minmax = MinMaxScaler(feature_range=(-1, 1))\n",
    "#minmax = MinMaxScaler()\n",
    "#X = minmax.fit_transform(X.astype(np.float64))\n",
    "print(X.min(axis=0), X.max(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n",
    "lst_accu_stratified = []\n",
    "\n",
    "rskf = RepeatedStratifiedKFold(n_splits=5, n_repeats=2,random_state=36851234)\n",
    "\n",
    "print ('Numero de Splits_stratified de X: ',rskf.get_n_splits(X, y),'\\n')\n",
    "\n",
    "# Prin_stratifiedting the Train & Test Indices of splits\n",
    "for train_index, test_index in rskf.split(X, y): \n",
    "    #print (\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train_stratified, X_test_stratified = X[train_index], X[test_index]\n",
    "    y_train_stratified, y_test_stratified = y[train_index], y[test_index]\n",
    "    print(X_train_stratified.shape, y_train_stratified.shape)\n",
    "    \n",
    "    yhat_probs = bestNN.predict(X_test_stratified)\n",
    "    lst_accu_stratified.append(100*accuracy_score(y_test_stratified, yhat_probs > 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Lista de acurácias obtidas:', lst_accu_stratified)\n",
    "print('\\nAcurácia máxima obtida:',\n",
    "      max(lst_accu_stratified), '%')\n",
    "print('\\nAcurácia mínima:',\n",
    "      min(lst_accu_stratified), '%')\n",
    "print('\\nAcurácia média:',\n",
    "      statistics.mean(lst_accu_stratified), '%')\n",
    "print('\\nDesvio Padrão:', statistics.stdev(lst_accu_stratified))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recupera modelo otimizado saldo anteriormente e o testa com diferentes conjuntos de dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_pred = bestNN.predict(X)\n",
    "yhat_classes = (y_pred > 0.5)\n",
    "print('Acurácia: {:.2f}%'.format(100*accuracy_score(y, y_pred > 0.5)))\n",
    "output_proprietario = None\n",
    "output_proprietario = dataset_original\n",
    "output_proprietario['score'] = y_pred\n",
    "output_proprietario.to_csv('bestANN/mapaSuscetibilidade_proprietaria.csv', encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NetworkPerformance(y, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_proprietario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot\n",
    "ns_auc = roc_auc_score(y, y_pred)\n",
    "# summarize scores\n",
    "print('ROC AUC=%.3f' % (ns_auc))\n",
    "# calculate roc curves\n",
    "ns_fpr, ns_tpr, _ = roc_curve(y, y_pred)\n",
    "# plot the roc curve for the model\n",
    "pyplot.plot(ns_fpr, ns_tpr, linestyle='--', label='ROC AUC')\n",
    "# axis labels\n",
    "pyplot.xlabel('False Positive Rate')\n",
    "pyplot.ylabel('True Positive Rate')\n",
    "# show the legend\n",
    "pyplot.legend()\n",
    "# show the plot\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ref. https://machinelearningmastery.com/roc-curves-and-precision-recall-curves-for-classification-in-python/\n",
    "- ref. https://stackoverflow.com/questions/25009284/how-to-plot-roc-curve-in-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes = output_proprietario['elevation']*10\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(14, 9))\n",
    "\n",
    "axes[0].scatter(output_proprietario['slope'], output_proprietario['elevation'],\n",
    "            c=(output_proprietario['class']), cmap='rainbow', alpha=0.3,\n",
    "            s=sizes, edgecolors='none')\n",
    "legend1 = axes[0].legend(*scatter.legend_elements(),\n",
    "                    loc=\"upper right\", title=\"landslide\")\n",
    "\n",
    "axes[1].scatter(output_proprietario['slope'], output_proprietario['elevation'],\n",
    "            c=(output_proprietario['score'] > 0.5), cmap='rainbow', alpha=0.3,\n",
    "            s=sizes, edgecolors='none')\n",
    "legend1 = axes[1].legend(*scatter.legend_elements(),\n",
    "                    loc=\"upper right\", title=\"landslide\")\n",
    "\n",
    "fig.tight_layout()\n",
    "text = 'deslizamentos preditos ' + 'acurácia de: {:.2f}%'.format(100*accuracy_score(y, y_pred > 0.5))\n",
    "axes[0].title.set_text('deslizamentos reais')\n",
    "axes[1].title.set_text(text)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataSet = output_proprietario.sample(n = 50)\n",
    "#dataSet = dataSet.drop('FID',axis=1)\n",
    "#dataSet = dataSet.drop('X',axis=1)\n",
    "#dataSet = dataSet.drop('Y',axis=1)\n",
    "plotSwarmChart(dataSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perguntar para Ale como retornar os dados para ela, se em um csv com cada posição e as respectivas probabilidades?\n",
    "# se for, basta incorporar essa variável y_pred à uma nova coluna do dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end = datetime.datetime.now()\n",
    "print(end-start)\n",
    "quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras neural network\n",
    "__Problema de classificação, considerar as melhores práticas:__\n",
    "- Ajuste dos hiperparametros\n",
    "- Baseline para implementação customizada\n",
    "\n",
    "ref. https://machinelearningmastery.com/data-preparation-without-data-leakage/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora, configura a rede com os melhores parâmetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(X_train), 'train examples')\n",
    "print(len(X_val), 'validation examples')\n",
    "print(len(X_test), 'test examples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#{'num_hidden_layers': 1, 'num_units': 4, 'dropout_rate': 0.5, 'learning_rate': 0.004682800657889146}       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HyperParameters: {'num_hidden_layers': 2, 'num_units': 12, 'dropout_rate': 0.2821478566400208, 'learning_rate': 0.006549533067877217}\n",
    "#conjunto de hyperparametros para o Guaruja 87% acuracia dados de teste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypermodel = keras.Sequential()\n",
    "hypermodel.add(Dense(24, activation='relu', input_dim=input_dim))\n",
    "\n",
    "hypermodel.add(Dense(24, activation='relu'))\n",
    "hypermodel.add(Dropout(0.43719886068153724))\n",
    "\n",
    "#hypermodel.add(Dense(24, activation='relu'))\n",
    "#hypermodel.add(Dropout(0.5))\n",
    "#hypermodel.add(Dense(24, activation='relu'))\n",
    "#hypermodel.add(Dropout(0.5))\n",
    "#hypermodel.add(Dense(24, activation='relu'))\n",
    "#hypermodel.add(Dropout(0.5))\n",
    "#hypermodel.add(Dense(24, activation='relu'))\n",
    "#hypermodel.add(Dropout(0.5))\n",
    "#hypermodel.add(Dense(24, activation='relu'))\n",
    "#hypermodel.add(Dropout(0.5))\n",
    "#hypermodel.add(Dense(24, activation='relu'))\n",
    "#hypermodel.add(Dropout(0.5))\n",
    "#hypermodel.add(Dense(24, activation='relu'))\n",
    "#hypermodel.add(Dropout(0.5))\n",
    "\n",
    "hypermodel.add(Dense(1, activation='sigmoid'))\n",
    "learning_rate= 0.01\n",
    "myOptimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "hypermodel.compile(optimizer=myOptimizer, loss=\"binary_crossentropy\", metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.fit(X_train, y_train, epochs=100, batch_size=5)\n",
    "history           = hypermodel.fit(X_train, y_train, epochs=500, batch_size=5)\n",
    "val_acc_per_epoch = history.history['accuracy']\n",
    "best_epoch        = val_acc_per_epoch.index(max(val_acc_per_epoch)) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Best epoch: %d' % (best_epoch,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# avaliação via keras\n",
    "_, accuracy = hypermodel.evaluate(X_test, y_test)\n",
    "print('Accuracy: %.2f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrain the model\n",
    "history = hypermodel.fit(X_train, y_train, epochs=best_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# avaliação via keras\n",
    "_, accuracy = hypermodel.evaluate(X_test, y_test)\n",
    "print('Accuracy: %.2f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# rankdir='LR' is used to make the graph horizontal.\n",
    "tf.keras.utils.plot_model(hypermodel, show_shapes=True, rankdir=\"LR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if accuracy >= .80:\n",
    "    print('ótima acurácia, rede salva!')\n",
    "    hypermodel.save('bestANN\\suscetibilidadeKeras_guaruja.pkl')\n",
    "else:\n",
    "    print('acurácia abaixo de .85')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X = dataset.drop('class',axis=1)\n",
    "#y = dataset['class']\n",
    "#X = X.to_numpy()    #converts dataframe into array to be used at NN\n",
    "#y = y.to_numpy()    #converts dataframe into array to be used at NN\n",
    "#y = y.reshape(-1,1) #reorganiza o array em um array 1 x 1\n",
    "\n",
    "#normalização do dataset\n",
    "#minmax = MinMaxScaler(feature_range=(-1, 1))\n",
    "#X = minmax.fit_transform(X.astype(np.float64))\n",
    "#print(X.min(axis=0), X.max(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n",
    "lst_accu_stratified = []\n",
    "\n",
    "rskf = RepeatedStratifiedKFold(n_splits=5, n_repeats=2,random_state=36851234)\n",
    "\n",
    "print ('Numero de Splits de X: ',rskf.get_n_splits(X, y),'\\n')\n",
    "\n",
    "# Printing the Train & Test Indices of splits\n",
    "for train_index, test_index in rskf.split(X, y): \n",
    "    #print (\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train_stratified_keras, X_test_stratified_keras = X[train_index], X[test_index]\n",
    "    y_train_stratified_keras, y_test_stratified_keras = y[train_index], y[test_index]\n",
    "    #print('novo dataset')\n",
    "    #print(X_train_stratified_keras.shape, y_train_stratified_keras.shape)\n",
    "    \n",
    "    yhat_probs = hypermodel.predict(X_test_stratified_keras)\n",
    "    lst_accu_stratified.append(100*accuracy_score(y_test_stratified_keras, yhat_probs > 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Lista de acurácias obtidas:', lst_accu_stratified)\n",
    "print('\\nAcurácia máxima obtida:',\n",
    "      max(lst_accu_stratified), '%')\n",
    "print('\\nAcurácia mínima:',\n",
    "      min(lst_accu_stratified), '%')\n",
    "print('\\nAcurácia média:',\n",
    "      statistics.mean(lst_accu_stratified), '%')\n",
    "print('\\nDesvio Padrão:', statistics.stdev(lst_accu_stratified))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = hypermodel.predict(X_test)\n",
    "#print('Predições:', y_pred, sep='\\n')\n",
    "print('Acurácia: {:.2f}%'.format(100*accuracy_score(y_test, y_pred > 0.5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_pred = hypermodel.predict(X_train)\n",
    "print('Acurácia: {:.2f}%'.format(100*accuracy_score(y_train, y_pred > 0.5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = hypermodel.predict(X_val)\n",
    "print('Acurácia: {:.2f}%'.format(100*accuracy_score(y_val, y_pred > 0.5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = hypermodel.predict(X)\n",
    "print('Acurácia: {:.2f}%'.format(100*accuracy_score(y, y_pred > 0.5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "output_keras = None\n",
    "output_keras = dataset_original\n",
    "output_keras['score'] = y_pred\n",
    "output_keras.to_csv('bestANN/mapaSuscetibilidade_keras.csv', encoding=\"utf-8\")\n",
    "output_keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ns_auc = roc_auc_score(y, y_pred)\n",
    "# summarize scores\n",
    "print('ROC AUC=%.3f' % (ns_auc))\n",
    "# calculate roc curves\n",
    "ns_fpr, ns_tpr, _ = roc_curve(y, y_pred)\n",
    "# plot the roc curve for the model\n",
    "pyplot.plot(ns_fpr, ns_tpr, linestyle='--', label='ROC AUC')\n",
    "# axis labels\n",
    "pyplot.xlabel('False Positive Rate')\n",
    "pyplot.ylabel('True Positive Rate')\n",
    "# show the legend\n",
    "pyplot.legend()\n",
    "# show the plot\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes = output_proprietario['elevation']*10\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(14, 9))\n",
    "\n",
    "axes[0].scatter(output_proprietario['slope'], output_proprietario['elevation'],\n",
    "            c=(output_proprietario['class']), cmap='rainbow', alpha=0.3,\n",
    "            s=sizes, edgecolors='none')\n",
    "legend1 = axes[0].legend(*scatter.legend_elements(),\n",
    "                    loc=\"upper right\", title=\"landslide\")\n",
    "\n",
    "axes[1].scatter(output_proprietario['slope'], output_proprietario['elevation'],\n",
    "            c=(output_proprietario['score'] > 0.5), cmap='rainbow', alpha=0.3,\n",
    "            s=sizes, edgecolors='none')\n",
    "legend1 = axes[1].legend(*scatter.legend_elements(),\n",
    "                    loc=\"upper right\", title=\"landslide\")\n",
    "\n",
    "fig.tight_layout()\n",
    "text = 'deslizamentos preditos ' + 'acurácia de: {:.2f}%'.format(100*accuracy_score(y, y_pred > 0.5))\n",
    "axes[0].title.set_text('deslizamentos reais')\n",
    "axes[1].title.set_text(text)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataSet = output_keras.sample(n = 50)\n",
    "#dataSet = dataSet.drop('FID',axis=1)\n",
    "#dataSet = dataSet.drop('X',axis=1)\n",
    "#dataSet = dataSet.drop('Y',axis=1)\n",
    "plotSwarmChart(dataSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end = datetime.datetime.now()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "- ref.1: https://www.tensorflow.org/tutorials/structured_data/preprocessing_layers\n",
    "- ref.2: https://www.tensorflow.org/tutorials/structured_data/feature_columns\n",
    "- ref.3: https://realpython.com/train-test-split-python-data/\n",
    "- ref.4: https://machinelearningmastery.com/data-preparation-without-data-leakage/\n",
    "- ref.5: https://towardsdatascience.com/data-leakage-in-machine-learning-10bdd3eec742\n",
    "- ref.6: https://analyticsindiamag.com/what-is-data-leakage-in-ml-why-should-you-be-concerned/\n",
    "- ref.7: https://www.section.io/engineering-education/data-leakage/\n",
    "- ref.8: https://medium.com/analytics-vidhya/overfitting-vs-data-leakage-in-machine-learning-ec59baa603e1\n",
    "- ref.9: https://www.analyticsvidhya.com/blog/2020/06/auc-roc-curve-machine-learning/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b7e61c40c58399eabfdc57275b655b0084201ad0151cf37bc45bab8193ea43c1"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
