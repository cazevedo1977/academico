{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Treino e testes da rede neural__\n",
    "- Dataset: Guarujá (cap. 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Copyright 2021 Caio Azevedo - ensaios de uma dissertação. \n",
    "\n",
    "13 de junho de 2021 - última atualização __todo santo dia__\n",
    "- ref. https://www.tensorflow.org/tutorials/structured_data/preprocessing_layers\n",
    "- ref. https://www.tensorflow.org/tutorials/structured_data/feature_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Objetivos__:\n",
    "\n",
    "- Tratamento dos dados para aplicação da rede neural: \n",
    "   - remoção de features irrelevantes;\n",
    "   - normalização do dataset;\n",
    "   - separação do dados de treino e teste da rede neural\n",
    "\n",
    "- Treinamento e avaliação das redes neurais:\n",
    "   - implementação keras\n",
    "   - implementação customizada\n",
    "\n",
    "- Automação do setup dos hiperparâmetros para melhor desempenho (critério inicial acurácia).\n",
    "  - em __15-08-2021__ - Uso Keras Tuning para otimização dos hyperparameters Keras (vide arquivo .ipynb)\n",
    "  - em __29-08-2021__ - Força Bruta para otimização dos hiperparametros na rede customizada\n",
    "\n",
    "- Avaliar performance da rede com e sem 'features irrelevantes'.\n",
    "\n",
    "__Sobre a implementação da Rede Neural Proprietária__:\n",
    "\n",
    "1. ref. https://whimsical.com/artificial-neural-network-4cTMNjQBkkCwJHZhUy7BTV\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparando predição com rede do keras\n",
    "- ref. https://machinelearningmastery.com/tutorial-first-neural-network-python-keras/\n",
    "\n",
    "Tunning neural netowrks\n",
    "- ref. http://karpathy.github.io/2019/04/25/recipe/\n",
    "\n",
    "Sobre as métricas\n",
    "- ref. https://neptune.ai/blog/f1-score-accuracy-roc-auc-pr-auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "start = datetime.datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cazev\\miniconda3\\envs\\cashme\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.0\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import Rede_Neural as rna\n",
    "from Rede_Neural import NeuralNetwork\n",
    "from Rede_Neural import Layer\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import pyplot\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from utils import plot\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gráfico que relacionada cada uma das features com o valor predito\n",
    "def plotSwarmChart(dataSet):\n",
    "    dataSet['Constante'] = 0 #feature inútil para referência do gráfico\n",
    "\n",
    "    f, axes = plt.subplots(1, 7, figsize=(35, 17), sharex=False)\n",
    "    f.subplots_adjust(hspace=0.2, wspace=0.7)\n",
    "    \n",
    "    sns.catplot(x=dataSet.columns[6], y='score', kind=\"swarm\", hue='score', data=dataSet,ax=axes[6])\n",
    "    \n",
    "    for i in range(7):\n",
    "        col = dataSet.columns[i] \n",
    "        ax = sns.swarmplot(x=dataSet['Constante'],y=dataSet[col].values,hue=(dataSet['score']>0.5),ax=axes[i])\n",
    "        ax.set_title(col)\n",
    "        \n",
    "    plt.close(2)\n",
    "    plt.close(3)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NetworkPerformance(y_real, y_prob):\n",
    "    y_predict = (y_prob > 0.5)\n",
    "    # accuracy: (tp + tn) / (p + n)\n",
    "    accuracy = accuracy_score(y_real, y_predict)\n",
    "    print('Accuracy: %f' % accuracy)\n",
    "    # precision tp / (tp + fp)\n",
    "    precision = precision_score(y_real, y_predict)\n",
    "    print('Precision: %f' % precision)\n",
    "    # recall: tp / (tp + fn)\n",
    "    recall = recall_score(y_real, y_predict)\n",
    "    print('Recall: %f' % recall)\n",
    "    # f1: 2 tp / (2 tp + fp + fn)\n",
    "    f1 = f1_score(y_real, y_predict)\n",
    "    print('F1 score: %f' % f1)\n",
    "    # kappa\n",
    "    kappa = cohen_kappa_score(y_real, y_predict)\n",
    "    print('Cohens kappa: %f' % kappa)\n",
    "    # ROC AUC\n",
    "    auc = roc_auc_score(y_real, y_prob)\n",
    "    print('ROC AUC: %f' % auc)\n",
    "    # confusion matrix\n",
    "    matrix = confusion_matrix(y_real, y_predict)\n",
    "    print(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.analyticsvidhya.com/blog/2020/06/auc-roc-curve-machine-learning/\n",
    "def printAUROC(y_real,y_predict):\n",
    "    ns_auc = roc_auc_score(y_real, y_predict)\n",
    "    # summarize scores\n",
    "    print('Área curva ROC=%.4f' % (ns_auc))\n",
    "    # calculate roc curves\n",
    "    lr_fpr, lr_tpr, _ = roc_curve(y_real, y_predict)\n",
    "    random_probs = [0 for i in range(len(y_test))]\n",
    "    p_fpr, p_tpr, _ = roc_curve(y_test, random_probs, pos_label=1)\n",
    "    # plot the roc curve for the model\n",
    "    plt.plot(p_fpr, p_tpr, linestyle='--', label='aleatório', color='orange')\n",
    "    pyplot.plot(lr_fpr, lr_tpr, marker='.', label='RNA', color='blue')\n",
    "    \n",
    "    # axis labels\n",
    "    pyplot.xlabel('Taxa de falso positivo')\n",
    "    pyplot.ylabel('Taxa de verdadeiro positivo')\n",
    "    # show the legend\n",
    "    pyplot.legend()\n",
    "    # show the plot\n",
    "    pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FID</th>\n",
       "      <th>X</th>\n",
       "      <th>Y</th>\n",
       "      <th>TWI</th>\n",
       "      <th>Curvatura</th>\n",
       "      <th>Declividade</th>\n",
       "      <th>Elevacao</th>\n",
       "      <th>Aspecto</th>\n",
       "      <th>Geologia</th>\n",
       "      <th>Uso</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>98</td>\n",
       "      <td>381913.2155</td>\n",
       "      <td>7358368.235</td>\n",
       "      <td>27.338095</td>\n",
       "      <td>0.002735</td>\n",
       "      <td>27.338095</td>\n",
       "      <td>120.943680</td>\n",
       "      <td>330.186584</td>\n",
       "      <td>65</td>\n",
       "      <td>90</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>152</td>\n",
       "      <td>375711.0000</td>\n",
       "      <td>7348857.000</td>\n",
       "      <td>5.155329</td>\n",
       "      <td>0.006004</td>\n",
       "      <td>24.772690</td>\n",
       "      <td>17.397917</td>\n",
       "      <td>113.472549</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>95</td>\n",
       "      <td>383008.5927</td>\n",
       "      <td>7357558.608</td>\n",
       "      <td>24.757074</td>\n",
       "      <td>0.000368</td>\n",
       "      <td>24.757074</td>\n",
       "      <td>207.428345</td>\n",
       "      <td>121.768433</td>\n",
       "      <td>65</td>\n",
       "      <td>90</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>106</td>\n",
       "      <td>371074.0000</td>\n",
       "      <td>7345111.000</td>\n",
       "      <td>6.688554</td>\n",
       "      <td>-0.010576</td>\n",
       "      <td>29.801399</td>\n",
       "      <td>15.330963</td>\n",
       "      <td>71.881042</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>82</td>\n",
       "      <td>375019.5230</td>\n",
       "      <td>7353140.890</td>\n",
       "      <td>0.221845</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>0.221845</td>\n",
       "      <td>2.162373</td>\n",
       "      <td>185.550385</td>\n",
       "      <td>2</td>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   FID            X            Y        TWI  Curvatura  Declividade  \\\n",
       "0   98  381913.2155  7358368.235  27.338095   0.002735    27.338095   \n",
       "1  152  375711.0000  7348857.000   5.155329   0.006004    24.772690   \n",
       "2   95  383008.5927  7357558.608  24.757074   0.000368    24.757074   \n",
       "3  106  371074.0000  7345111.000   6.688554  -0.010576    29.801399   \n",
       "4   82  375019.5230  7353140.890   0.221845  -0.000012     0.221845   \n",
       "\n",
       "     Elevacao     Aspecto  Geologia  Uso  class  \n",
       "0  120.943680  330.186584        65   90      0  \n",
       "1   17.397917  113.472549         2   20      1  \n",
       "2  207.428345  121.768433        65   90      0  \n",
       "3   15.330963   71.881042         2   20      1  \n",
       "4    2.162373  185.550385         2   70      0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, sys\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "APP_PATH = os.getcwd()\n",
    "\n",
    "#file = \"Input_Guaruja_50m_shuffled.csv\"\n",
    "#file = \"Input_Guaruja_100m_shuffled.csv\"\n",
    "#file = \"Input_Guaruja_150m_shuffled.csv\"\n",
    "#file = \"Input_Guaruja_200m_shuffled.csv\"\n",
    "\n",
    "#file = \"pt_nocorrencias_1km_shuffled.csv\"\n",
    "#file = \"pt_nocorrencias_2km_shuffled.csv\"\n",
    "#file = \"pt_nocorrencias_3km_shuffled.csv\"\n",
    "#file = \"pt_nocorrencias_4km_shuffled.csv\"\n",
    "\n",
    "file = \"ptos_aleatorios_shuffled.csv\"\n",
    "\n",
    "\n",
    "dataset = pd.read_csv(os.path.join(APP_PATH, os.path.join(\"data\", file))) \n",
    "#dataset = pd.read_csv(os.path.join(APP_PATH, os.path.join(\"data\", \"ptos_aleatorios.csv\")))\n",
    "\n",
    "# load the dataset\n",
    "#dataset = dataset.sample(frac = 1) #embaralha os registros\n",
    "#dataset.to_csv(os.path.join(APP_PATH, os.path.join(\"data\", \"ptos_aleatorios_shuffled.csv\")))\n",
    "dataset_original = dataset.copy()\n",
    "dataset.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset[\"eng_1\"] = dataset[\"Elevacao\"] / dataset[\"Declividade\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>slope</th>\n",
       "      <th>aspect</th>\n",
       "      <th>elevation</th>\n",
       "      <th>uso_solo</th>\n",
       "      <th>lito</th>\n",
       "      <th>twi</th>\n",
       "      <th>curv</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>27.338095</td>\n",
       "      <td>330.186584</td>\n",
       "      <td>120.943680</td>\n",
       "      <td>90</td>\n",
       "      <td>65</td>\n",
       "      <td>27.338095</td>\n",
       "      <td>0.002735</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>24.772690</td>\n",
       "      <td>113.472549</td>\n",
       "      <td>17.397917</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>5.155329</td>\n",
       "      <td>0.006004</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>24.757074</td>\n",
       "      <td>121.768433</td>\n",
       "      <td>207.428345</td>\n",
       "      <td>90</td>\n",
       "      <td>65</td>\n",
       "      <td>24.757074</td>\n",
       "      <td>0.000368</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>29.801399</td>\n",
       "      <td>71.881042</td>\n",
       "      <td>15.330963</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>6.688554</td>\n",
       "      <td>-0.010576</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.221845</td>\n",
       "      <td>185.550385</td>\n",
       "      <td>2.162373</td>\n",
       "      <td>70</td>\n",
       "      <td>2</td>\n",
       "      <td>0.221845</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       slope      aspect   elevation  uso_solo  lito        twi      curv  \\\n",
       "0  27.338095  330.186584  120.943680        90    65  27.338095  0.002735   \n",
       "1  24.772690  113.472549   17.397917        20     2   5.155329  0.006004   \n",
       "2  24.757074  121.768433  207.428345        90    65  24.757074  0.000368   \n",
       "3  29.801399   71.881042   15.330963        20     2   6.688554 -0.010576   \n",
       "4   0.221845  185.550385    2.162373        70     2   0.221845 -0.000012   \n",
       "\n",
       "   class  \n",
       "0      0  \n",
       "1      1  \n",
       "2      0  \n",
       "3      1  \n",
       "4      0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.rename(columns={'TWI': 'twi','Curvatura': 'curv', 'Declividade': 'slope','Elevacao': 'elevation','Aspecto': 'aspect', 'Geologia': 'lito', 'Uso': 'uso_solo'}, inplace=True)\n",
    "dataset = dataset[['slope','aspect','elevation','uso_solo','lito','twi','curv','class']]\n",
    "#dataset = dataset[['aspect','uso_solo','lito','twi','curv','class','eng_1']]\n",
    "dataset.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 8)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>slope</th>\n",
       "      <th>aspect</th>\n",
       "      <th>elevation</th>\n",
       "      <th>uso_solo</th>\n",
       "      <th>lito</th>\n",
       "      <th>twi</th>\n",
       "      <th>curv</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>200.00</td>\n",
       "      <td>200.00</td>\n",
       "      <td>200.00</td>\n",
       "      <td>200.00</td>\n",
       "      <td>200.00</td>\n",
       "      <td>200.00</td>\n",
       "      <td>200.00</td>\n",
       "      <td>200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>23.94</td>\n",
       "      <td>209.79</td>\n",
       "      <td>64.32</td>\n",
       "      <td>75.80</td>\n",
       "      <td>32.56</td>\n",
       "      <td>14.42</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>11.79</td>\n",
       "      <td>103.92</td>\n",
       "      <td>56.08</td>\n",
       "      <td>26.72</td>\n",
       "      <td>31.56</td>\n",
       "      <td>13.62</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.09</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.23</td>\n",
       "      <td>20.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.09</td>\n",
       "      <td>-0.59</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>18.08</td>\n",
       "      <td>121.04</td>\n",
       "      <td>14.33</td>\n",
       "      <td>90.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>4.29</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>25.33</td>\n",
       "      <td>202.74</td>\n",
       "      <td>54.48</td>\n",
       "      <td>90.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>6.13</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>30.77</td>\n",
       "      <td>316.40</td>\n",
       "      <td>94.71</td>\n",
       "      <td>90.00</td>\n",
       "      <td>65.00</td>\n",
       "      <td>26.33</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>69.73</td>\n",
       "      <td>359.51</td>\n",
       "      <td>226.06</td>\n",
       "      <td>90.00</td>\n",
       "      <td>65.00</td>\n",
       "      <td>69.73</td>\n",
       "      <td>4.30</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        slope  aspect  elevation  uso_solo    lito     twi    curv  class\n",
       "count  200.00  200.00     200.00    200.00  200.00  200.00  200.00  200.0\n",
       "mean    23.94  209.79      64.32     75.80   32.56   14.42    0.03    0.5\n",
       "std     11.79  103.92      56.08     26.72   31.56   13.62    0.41    0.5\n",
       "min      0.09    0.04       0.23     20.00    2.00    0.09   -0.59    0.0\n",
       "25%     18.08  121.04      14.33     90.00    2.00    4.29   -0.01    0.0\n",
       "50%     25.33  202.74      54.48     90.00    2.00    6.13   -0.00    0.0\n",
       "75%     30.77  316.40      94.71     90.00   65.00   26.33    0.00    1.0\n",
       "max     69.73  359.51     226.06     90.00   65.00   69.73    4.30    1.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#describing the dataframe to seek for distribution information\n",
    "dataset.describe().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x936 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(15,13))\n",
    "sizes = dataset['elevation']*10\n",
    "scatter = ax.scatter(dataset['slope'], dataset['elevation'], \n",
    "           c=(dataset['class']), cmap='rainbow', alpha=0.5,\n",
    "           s=sizes, edgecolors='none')\n",
    "\n",
    "legend1 = ax.legend(*scatter.legend_elements(),\n",
    "                    loc=\"upper right\", title=\"landslide\")\n",
    "\n",
    "ax.title.set_text(\"{}{}\".format(len(dataset.index), ' amostras de escorregamentos')) \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparação dos dados (Remoção das features irrelevantes & Normalização)\n",
    "__Cuidado com data leakage, considerar as melhores práticas:__\n",
    "- Train-Test Evaluation With Correct Data Preparation\n",
    "- ref. https://realpython.com/train-test-split-python-data/\n",
    "- Data Preparation With k-fold Cross-Validation\n",
    "\n",
    "- ref. https://machinelearningmastery.com/data-preparation-without-data-leakage/\n",
    "- ref. https://towardsdatascience.com/data-leakage-in-machine-learning-10bdd3eec742\n",
    "- ref. https://analyticsindiamag.com/what-is-data-leakage-in-ml-why-should-you-be-concerned/\n",
    "- ref. https://www.section.io/engineering-education/data-leakage/\n",
    "- ref. https://medium.com/analytics-vidhya/overfitting-vs-data-leakage-in-machine-learning-ec59baa603e1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " __Segue abaixo, a ordem de relevância das variáveis, bem como a identificação daquelas que podem ser removidas (vide analise análise da features).__\n",
    "  - elevation | elevation (ok) \n",
    "  - uso_solo | uso_solo (ok)\n",
    "  - twi | curvatura (ok)\n",
    "  - curvatura | twi (ok)\n",
    "  - lito | aspect   __(to be tested)__\n",
    "  - aspect | lito   __(to be tested)__\n",
    "  - slope | slope   __(to be tested)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>slope</th>\n",
       "      <th>aspect</th>\n",
       "      <th>elevation</th>\n",
       "      <th>uso_solo</th>\n",
       "      <th>lito</th>\n",
       "      <th>twi</th>\n",
       "      <th>curv</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>27.338095</td>\n",
       "      <td>330.186584</td>\n",
       "      <td>120.943680</td>\n",
       "      <td>90</td>\n",
       "      <td>65</td>\n",
       "      <td>27.338095</td>\n",
       "      <td>0.002735</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>24.772690</td>\n",
       "      <td>113.472549</td>\n",
       "      <td>17.397917</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>5.155329</td>\n",
       "      <td>0.006004</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>24.757074</td>\n",
       "      <td>121.768433</td>\n",
       "      <td>207.428345</td>\n",
       "      <td>90</td>\n",
       "      <td>65</td>\n",
       "      <td>24.757074</td>\n",
       "      <td>0.000368</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>29.801399</td>\n",
       "      <td>71.881042</td>\n",
       "      <td>15.330963</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>6.688554</td>\n",
       "      <td>-0.010576</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.221845</td>\n",
       "      <td>185.550385</td>\n",
       "      <td>2.162373</td>\n",
       "      <td>70</td>\n",
       "      <td>2</td>\n",
       "      <td>0.221845</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       slope      aspect   elevation  uso_solo  lito        twi      curv  \\\n",
       "0  27.338095  330.186584  120.943680        90    65  27.338095  0.002735   \n",
       "1  24.772690  113.472549   17.397917        20     2   5.155329  0.006004   \n",
       "2  24.757074  121.768433  207.428345        90    65  24.757074  0.000368   \n",
       "3  29.801399   71.881042   15.330963        20     2   6.688554 -0.010576   \n",
       "4   0.221845  185.550385    2.162373        70     2   0.221845 -0.000012   \n",
       "\n",
       "   class  \n",
       "0      0  \n",
       "1      1  \n",
       "2      0  \n",
       "3      1  \n",
       "4      0  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#features menos relevantes guaruja\n",
    "#dataset = dataset.drop('elevation',axis=1)\n",
    "#dataset = dataset.drop('curv',axis=1)\n",
    "#dataset = dataset.drop('lito',axis=1)\n",
    "#dataset = dataset.drop('uso_solo',axis=1)\n",
    "\n",
    "# de fato, essas 3 features são irrelevantes pois a acurácia é similar sem elas no dataset\n",
    "dataset.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#divide o dataset em base de treino e teste\n",
    "def dataPreparation(dataSet):\n",
    "    X = dataSet.drop('class',axis=1)\n",
    "    y= dataSet['class']\n",
    "    \n",
    "    X = X.to_numpy()    #converts dataframe into array to be used at NN\n",
    "    y = y.to_numpy()    #converts dataframe into array to be used at NN\n",
    "    y = y.reshape(-1,1) #reorganiza o array em um array 1 x 1\n",
    "    \n",
    "    # split data into training and testing sets\n",
    "    seed = 7\n",
    "    test_size = 0.30\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=seed)\n",
    "    \n",
    "    # define the scaler\n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1)) \n",
    "    #scaler = MinMaxScaler() \n",
    "    # fit on the training dataset\n",
    "    scaler.fit(X_train)\n",
    "    # scale the training dataset\n",
    "    X_train = scaler.transform(X_train)\n",
    "    \n",
    "    # scale the test dataset\n",
    "    X_test = scaler.transform(X_test)\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#divide o dataset em treino-teste-validação\n",
    "def dataPreparation2(dataSet):\n",
    "    X = dataSet.drop('class',axis=1)\n",
    "    y= dataSet['class']\n",
    "    \n",
    "    X = X.to_numpy()    #converts dataframe into array to be used at NN\n",
    "    y = y.to_numpy()    #converts dataframe into array to be used at NN\n",
    "    y = y.reshape(-1,1) #reorganiza o array em um array 1 x 1\n",
    "    \n",
    "    # split data into training, validation and testing sets\n",
    "    seed = 7\n",
    "    test_size = 0.30\n",
    "    \n",
    "    train_x, test_x, train_y, test_y = train_test_split(X, y, test_size=test_size, shuffle=False)\n",
    "    train_x, val_x, train_y, val_y   = train_test_split(train_x,train_y, test_size=test_size, shuffle=False)\n",
    "\n",
    "    # define the scaler\n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1)) \n",
    "    #scaler = MinMaxScaler() \n",
    "    # fit on the training dataset\n",
    "    scaler.fit(train_x)\n",
    "    # scale the training dataset\n",
    "    train_x = scaler.transform(train_x)\n",
    "    # scale the test dataset\n",
    "    test_x = scaler.transform(test_x)\n",
    "    # scale the test dataset\n",
    "    val_x = scaler.transform(val_x)\n",
    "    return train_x, test_x, val_x, train_y, test_y, val_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, X_val, y_train, y_test, y_val = dataPreparation2(dataset)\n",
    "#X_train, X_test, y_train, y_test = dataPreparation(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98 train examples\n",
      "42 validation examples\n",
      "60 test examples\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train), 'train examples')\n",
    "print(len(X_val), 'validation examples')\n",
    "print(len(X_test), 'test examples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1. -1. -1. -1. -1. -1. -1.] [ 1.63025317  0.948116    1.03804377  1.          1.          1.35762042\n",
      " -0.98499636]\n"
     ]
    }
   ],
   "source": [
    "print(X_train.min(axis=0), X_test.max(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.concatenate((X_train, X_val))\n",
    "X = np.concatenate((X, X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 7)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.concatenate((y_train, y_val))\n",
    "y = np.concatenate((y, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 1)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.         -1.00147197 -1.00818162 -1.         -1.         -1.\n",
      " -1.26211285] [1.78731424 1.00048694 1.03804377 1.         1.         1.78731424\n",
      " 1.        ]\n"
     ]
    }
   ],
   "source": [
    "print(X.min(axis=0), X.max(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X = dataset.drop('class',axis=1)\n",
    "#y= dataset['class']\n",
    "#X = X.to_numpy()    #converts dataframe into array to be used at NN\n",
    "#y = y.to_numpy()    #converts dataframe into array to be used at NN\n",
    "#y = y.reshape(-1,1) #reorganiza o array em um array 1 x 1\n",
    "\n",
    "#normalização do dataset\n",
    "#minmax = MinMaxScaler(feature_range=(-1, 1))\n",
    "#minmax = MinMaxScaler()\n",
    "#X = minmax.fit_transform(X.astype(np.float64))\n",
    "#print(X.min(axis=0), X.max(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Computa o número de variáveis de entrada (features) e saída (sempre 1)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim, output_dim = X_train.shape[1], y_train.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rede Neural Customizada\n",
    "### Força Bruta para tuning dos hiperparametros \n",
    "__Problema de classificação, considerar as melhores práticas:__\n",
    "- Ajuste dos hiperparametros\n",
    "- Baseline para implementação customizada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class hyperparametros: \n",
    "    def __init__(self, acuracia, camadas,neuronios,learning_rate,batch_size,dropout): \n",
    "        self.acuracia = acuracia \n",
    "        self.camadas = camadas\n",
    "        self.neuronios = neuronios \n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.dropout = dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANN_accuracy: \n",
    "    def __init__(self, acuracia, _neuralNetwork): \n",
    "        self.acuracia = acuracia \n",
    "        self.ann = _neuralNetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#patience = early stopping\n",
    "def setBestNeuralNetwork(hidden_layers,neurons, num_learning_rate, droput, input_dim, output_dim):\n",
    "    nn = NeuralNetwork(cost_func = rna.binary_cross_entropy, learning_rate = num_learning_rate, momentum=0.1, patience=50) \n",
    "\n",
    "    #camada de entrada\n",
    "    nn.layers.append(Layer(input_dim=input_dim, output_dim=neurons,activation= rna.relu, weights_initializer=rna.glorot_normal))\n",
    "\n",
    "    for num_hidden_layers in range(1,hidden_layers+1,1):\n",
    "        #nn.layers.append(Layer(input_dim=neurons, output_dim=neurons,activation=rna.relu,reg_func=rna.l2_regularization,reg_strength=1e-2))\n",
    "        nn.layers.append(Layer(input_dim=neurons, output_dim=neurons,activation=rna.relu,dropout_prob = droput,weights_initializer=rna.glorot_normal,biases_initializer=rna.glorot_normal))\n",
    "\n",
    "#regularizacao por dropout e com inicialização de pesos e bias\n",
    "#nn.layers.append(Layer(input_dim=neurons, output_dim=neurons,dropout_prob = droput,activation=rna.relu,weights_initializer=rna.glorot_normal,biases_initializer=rna.glorot_normal))\n",
    "\n",
    "#regularizacao por L2 e sem inicialização de pesos e bias\n",
    "#nn.layers.append(Layer(input_dim=neurons, output_dim=neurons,reg_func=l2_regularization,reg_strength=1e-2,activation=rna.relu))\n",
    "#nn.layers.append(Layer(input_dim=neurons, output_dim=neurons,dropout_prob = droput,activation=rna.relu,weights_initializer=rna.glorot_normal,biases_initializer=rna.glorot_normal))        \n",
    "    #camada de saída\n",
    "    nn.layers.append(Layer(input_dim=neurons, output_dim=output_dim,activation=rna.sigmoid,weights_initializer=rna.glorot_normal))  \n",
    "\n",
    "    return nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def setBestNeuralNetwork2(num_learning_rate,input_dim, output_dim):\n",
    "#    nn = NeuralNetwork(cost_func = rna.binary_cross_entropy, learning_rate = num_learning_rate, patience=10) \n",
    "\n",
    "    #camada de entrada \n",
    "#    nn.layers.append(Layer(input_dim=input_dim, output_dim=20,activation= rna.relu,weights_initializer=rna.glorot_normal,biases_initializer=rna.glorot_normal))\n",
    "\n",
    "#    nn.layers.append(Layer(input_dim=20, output_dim=10,activation=rna.relu,weights_initializer=rna.glorot_normal,biases_initializer=rna.glorot_normal))\n",
    "    #nn.layers.append(Layer(input_dim=10, output_dim=10,activation=rna.relu,weights_initializer=rna.glorot_normal,biases_initializer=rna.glorot_normal))\n",
    " \n",
    "    #camadad de saída\n",
    "#    nn.layers.append(Layer(input_dim=10, output_dim=output_dim,activation=rna.sigmoid))  \n",
    "\n",
    "#    return nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def setBestNeuralNetwork3(num_learning_rate,input_dim, output_dim):\n",
    "#    nn = NeuralNetwork(cost_func = rna.binary_cross_entropy, learning_rate = num_learning_rate,momentum=0.25, patience=10) \n",
    "\n",
    "    #camada de entrada - considerar tanh na segunda camada e com dropout 0.2821478566400208\n",
    "#    nn.layers.append(Layer(input_dim=input_dim, output_dim=7,activation= rna.relu, weights_initializer=rna.glorot_normal))\n",
    "\n",
    "#    nn.layers.append(Layer(input_dim=7, output_dim=20 ,activation=rna.relu,weights_initializer=rna.glorot_normal,biases_initializer=rna.glorot_normal))\n",
    "    #nn.layers.append(Layer(input_dim=7, output_dim=14,activation=rna.relu,weights_initializer=rna.glorot_normal,biases_initializer=rna.glorot_normal))\n",
    "#    nn.layers.append(Layer(input_dim=20, output_dim=7,activation=rna.relu,weights_initializer=rna.glorot_normal,biases_initializer=rna.glorot_normal))\n",
    " \n",
    "    #camada de saída\n",
    "#    nn.layers.append(Layer(input_dim=7, output_dim=output_dim,activation=rna.sigmoid, weights_initializer=rna.glorot_normal))  \n",
    "\n",
    "#    return nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (batch_size) implementa o mini-batch que acelera o tempo de treinamento, requer método de inicialização do mini-batch. \n",
    "#ideal que o batch_size seja multiplo do tamanho do conjunto de dados de treinamento, que aqui é de 128 \n",
    "\n",
    "neurons = [7,8,12] \n",
    "# preciso encontrar uma forma de implementar a alteração do numero de neuronios na camada oculta variando \n",
    "# de n-2 até n+6\n",
    "hidden_layers = [1,2,3]\n",
    "learning_rate = [0.001,0.005,0.01]\n",
    "batch_size = [0,4,8] \n",
    "dropout_rate = [0,0.1]\n",
    "best_of_best_ANN = []\n",
    "best_of_best_hyper = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setTestNeuralNetwork(hidden_layers,neurons, num_learning_rate, droput, input_dim, output_dim):\n",
    "    #nn = NeuralNetwork(cost_func = rna.binary_cross_entropy, learning_rate = num_learning_rate, momentum=0.25, patience=50) \n",
    "\n",
    "    #camada de entrada\n",
    "    print('\\033[1m Camada de Entrada: {} neurônios inputs  {} neurônios saída {} learning rate \\033[0m'.format(input_dim, neurons,num_learning_rate))\n",
    "    #nn.layers.append(Layer(input_dim=input_dim, output_dim=neurons,activation= rna.relu, weights_initializer=rna.glorot_normal))\n",
    "\n",
    "    for num_hidden_layers in range(1,hidden_layers+1,1):\n",
    "        print(' _camada oculta {} neurônios {} camadas '.format(neurons,hidden_layers))\n",
    "        #nn.layers.append(Layer(input_dim=neurons, output_dim=neurons,activation=rna.relu,reg_func=rna.l2_regularization,reg_strength=1e-2))\n",
    "        #nn.layers.append(Layer(input_dim=neurons, output_dim=neurons,activation=rna.relu,dropout_prob = droput,weights_initializer=rna.glorot_normal))\n",
    "    \n",
    "    #nn.layers.append(Layer(input_dim=neurons, output_dim=output_dim,activation=rna.sigmoid,weights_initializer=rna.glorot_normal))  \n",
    "    print('\\033[1m {} neurônios da ultima camada oculta, {} neurônios na de saída \\033[0m'.format(neurons,output_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m Camada de Entrada: 7 neurônios inputs  7 neurônios saída 0.001 learning rate \u001b[0m\n",
      " _camada oculta 7 neurônios 1 camadas \n",
      "\u001b[1m 7 neurônios da ultima camada oculta, 1 neurônios na de saída \u001b[0m\n",
      "\u001b[1m Camada de Entrada: 7 neurônios inputs  7 neurônios saída 0.001 learning rate \u001b[0m\n",
      " _camada oculta 7 neurônios 1 camadas \n",
      "\u001b[1m 7 neurônios da ultima camada oculta, 1 neurônios na de saída \u001b[0m\n",
      "\u001b[1m Camada de Entrada: 7 neurônios inputs  7 neurônios saída 0.001 learning rate \u001b[0m\n",
      " _camada oculta 7 neurônios 1 camadas \n",
      "\u001b[1m 7 neurônios da ultima camada oculta, 1 neurônios na de saída \u001b[0m\n",
      "\u001b[1m Camada de Entrada: 7 neurônios inputs  7 neurônios saída 0.001 learning rate \u001b[0m\n",
      " _camada oculta 7 neurônios 1 camadas \n",
      "\u001b[1m 7 neurônios da ultima camada oculta, 1 neurônios na de saída \u001b[0m\n",
      "\u001b[1m Camada de Entrada: 7 neurônios inputs  7 neurônios saída 0.001 learning rate \u001b[0m\n",
      " _camada oculta 7 neurônios 1 camadas \n",
      "\u001b[1m 7 neurônios da ultima camada oculta, 1 neurônios na de saída \u001b[0m\n",
      "\u001b[1m Camada de Entrada: 7 neurônios inputs  7 neurônios saída 0.001 learning rate \u001b[0m\n",
      " _camada oculta 7 neurônios 1 camadas \n",
      "\u001b[1m 7 neurônios da ultima camada oculta, 1 neurônios na de saída \u001b[0m\n",
      "\u001b[1m Camada de Entrada: 7 neurônios inputs  7 neurônios saída 0.005 learning rate \u001b[0m\n",
      " _camada oculta 7 neurônios 1 camadas \n",
      "\u001b[1m 7 neurônios da ultima camada oculta, 1 neurônios na de saída \u001b[0m\n",
      "\u001b[1m Camada de Entrada: 7 neurônios inputs  7 neurônios saída 0.005 learning rate \u001b[0m\n",
      " _camada oculta 7 neurônios 1 camadas \n",
      "\u001b[1m 7 neurônios da ultima camada oculta, 1 neurônios na de saída \u001b[0m\n",
      "\u001b[1m Camada de Entrada: 7 neurônios inputs  7 neurônios saída 0.005 learning rate \u001b[0m\n",
      " _camada oculta 7 neurônios 1 camadas \n",
      "\u001b[1m 7 neurônios da ultima camada oculta, 1 neurônios na de saída \u001b[0m\n",
      "\u001b[1m Camada de Entrada: 7 neurônios inputs  7 neurônios saída 0.005 learning rate \u001b[0m\n",
      " _camada oculta 7 neurônios 1 camadas \n",
      "\u001b[1m 7 neurônios da ultima camada oculta, 1 neurônios na de saída \u001b[0m\n",
      "\u001b[1m Camada de Entrada: 7 neurônios inputs  7 neurônios saída 0.005 learning rate \u001b[0m\n",
      " _camada oculta 7 neurônios 1 camadas \n",
      "\u001b[1m 7 neurônios da ultima camada oculta, 1 neurônios na de saída \u001b[0m\n",
      "\u001b[1m Camada de Entrada: 7 neurônios inputs  7 neurônios saída 0.005 learning rate \u001b[0m\n",
      " _camada oculta 7 neurônios 1 camadas \n",
      "\u001b[1m 7 neurônios da ultima camada oculta, 1 neurônios na de saída \u001b[0m\n",
      "\u001b[1m Camada de Entrada: 7 neurônios inputs  7 neurônios saída 0.01 learning rate \u001b[0m\n",
      " _camada oculta 7 neurônios 1 camadas \n",
      "\u001b[1m 7 neurônios da ultima camada oculta, 1 neurônios na de saída \u001b[0m\n",
      "\u001b[1m Camada de Entrada: 7 neurônios inputs  7 neurônios saída 0.01 learning rate \u001b[0m\n",
      " _camada oculta 7 neurônios 1 camadas \n",
      "\u001b[1m 7 neurônios da ultima camada oculta, 1 neurônios na de saída \u001b[0m\n",
      "\u001b[1m Camada de Entrada: 7 neurônios inputs  7 neurônios saída 0.01 learning rate \u001b[0m\n",
      " _camada oculta 7 neurônios 1 camadas \n",
      "\u001b[1m 7 neurônios da ultima camada oculta, 1 neurônios na de saída \u001b[0m\n",
      "\u001b[1m Camada de Entrada: 7 neurônios inputs  7 neurônios saída 0.01 learning rate \u001b[0m\n",
      " _camada oculta 7 neurônios 1 camadas \n",
      "\u001b[1m 7 neurônios da ultima camada oculta, 1 neurônios na de saída \u001b[0m\n",
      "\u001b[1m Camada de Entrada: 7 neurônios inputs  7 neurônios saída 0.01 learning rate \u001b[0m\n",
      " _camada oculta 7 neurônios 1 camadas \n",
      "\u001b[1m 7 neurônios da ultima camada oculta, 1 neurônios na de saída \u001b[0m\n",
      "\u001b[1m Camada de Entrada: 7 neurônios inputs  7 neurônios saída 0.01 learning rate \u001b[0m\n",
      " _camada oculta 7 neurônios 1 camadas \n",
      "\u001b[1m 7 neurônios da ultima camada oculta, 1 neurônios na de saída \u001b[0m\n",
      "\u001b[1m Camada de Entrada: 7 neurônios inputs  8 neurônios saída 0.001 learning rate \u001b[0m\n",
      " _camada oculta 8 neurônios 1 camadas \n",
      "\u001b[1m 8 neurônios da ultima camada oculta, 1 neurônios na de saída \u001b[0m\n",
      "\u001b[1m Camada de Entrada: 7 neurônios inputs  8 neurônios saída 0.001 learning rate \u001b[0m\n",
      " _camada oculta 8 neurônios 1 camadas \n",
      "\u001b[1m 8 neurônios da ultima camada oculta, 1 neurônios na de saída \u001b[0m\n",
      "\u001b[1m Camada de Entrada: 7 neurônios inputs  8 neurônios saída 0.001 learning rate \u001b[0m\n",
      " _camada oculta 8 neurônios 1 camadas \n",
      "\u001b[1m 8 neurônios da ultima camada oculta, 1 neurônios na de saída \u001b[0m\n",
      "\u001b[1m Camada de Entrada: 7 neurônios inputs  8 neurônios saída 0.001 learning rate \u001b[0m\n",
      " _camada oculta 8 neurônios 1 camadas \n",
      "\u001b[1m 8 neurônios da ultima camada oculta, 1 neurônios na de saída \u001b[0m\n",
      "\u001b[1m Camada de Entrada: 7 neurônios inputs  8 neurônios saída 0.001 learning rate \u001b[0m\n",
      " _camada oculta 8 neurônios 1 camadas \n",
      "\u001b[1m 8 neurônios da ultima camada oculta, 1 neurônios na de saída \u001b[0m\n",
      "\u001b[1m Camada de Entrada: 7 neurônios inputs  8 neurônios saída 0.001 learning rate \u001b[0m\n",
      " _camada oculta 8 neurônios 1 camadas \n",
      "\u001b[1m 8 neurônios da ultima camada oculta, 1 neurônios na de saída \u001b[0m\n",
      "\u001b[1m Camada de Entrada: 7 neurônios inputs  8 neurônios saída 0.005 learning rate \u001b[0m\n",
      " _camada oculta 8 neurônios 1 camadas \n",
      "\u001b[1m 8 neurônios da ultima camada oculta, 1 neurônios na de saída \u001b[0m\n",
      "\u001b[1m Camada de Entrada: 7 neurônios inputs  8 neurônios saída 0.005 learning rate \u001b[0m\n",
      " _camada oculta 8 neurônios 1 camadas \n",
      "\u001b[1m 8 neurônios da ultima camada oculta, 1 neurônios na de saída \u001b[0m\n",
      "\u001b[1m Camada de Entrada: 7 neurônios inputs  8 neurônios saída 0.005 learning rate \u001b[0m\n",
      " _camada oculta 8 neurônios 1 camadas \n",
      "\u001b[1m 8 neurônios da ultima camada oculta, 1 neurônios na de saída \u001b[0m\n",
      "\u001b[1m Camada de Entrada: 7 neurônios inputs  8 neurônios saída 0.005 learning rate \u001b[0m\n",
      " _camada oculta 8 neurônios 1 camadas \n",
      "\u001b[1m 8 neurônios da ultima camada oculta, 1 neurônios na de saída \u001b[0m\n",
      "\u001b[1m Camada de Entrada: 7 neurônios inputs  8 neurônios saída 0.005 learning rate \u001b[0m\n",
      " _camada oculta 8 neurônios 1 camadas \n",
      "\u001b[1m 8 neurônios da ultima camada oculta, 1 neurônios na de saída \u001b[0m\n",
      "\u001b[1m Camada de Entrada: 7 neurônios inputs  8 neurônios saída 0.005 learning rate \u001b[0m\n",
      " _camada oculta 8 neurônios 1 camadas \n",
      "\u001b[1m 8 neurônios da ultima camada oculta, 1 neurônios na de saída \u001b[0m\n",
      "\u001b[1m Camada de Entrada: 7 neurônios inputs  8 neurônios saída 0.01 learning rate \u001b[0m\n",
      " _camada oculta 8 neurônios 1 camadas \n",
      "\u001b[1m 8 neurônios da ultima camada oculta, 1 neurônios na de saída \u001b[0m\n",
      "\u001b[1m Camada de Entrada: 7 neurônios inputs  8 neurônios saída 0.01 learning rate \u001b[0m\n",
      " _camada oculta 8 neurônios 1 camadas \n",
      "\u001b[1m 8 neurônios da ultima camada oculta, 1 neurônios na de saída \u001b[0m\n",
      "\u001b[1m Camada de Entrada: 7 neurônios inputs  8 neurônios saída 0.01 learning rate \u001b[0m\n",
      " _camada oculta 8 neurônios 1 camadas \n",
      "\u001b[1m 8 neurônios da ultima camada oculta, 1 neurônios na de saída \u001b[0m\n",
      "\u001b[1m Camada de Entrada: 7 neurônios inputs  8 neurônios saída 0.01 learning rate \u001b[0m\n",
      " _camada oculta 8 neurônios 1 camadas \n",
      "\u001b[1m 8 neurônios da ultima camada oculta, 1 neurônios na de saída \u001b[0m\n",
      "\u001b[1m Camada de Entrada: 7 neurônios inputs  8 neurônios saída 0.01 learning rate \u001b[0m\n",
      " _camada oculta 8 neurônios 1 camadas \n",
      "\u001b[1m 8 neurônios da ultima camada oculta, 1 neurônios na de saída \u001b[0m\n",
      "\u001b[1m Camada de Entrada: 7 neurônios inputs  8 neurônios saída 0.01 learning rate \u001b[0m\n",
      " _camada oculta 8 neurônios 1 camadas \n",
      "\u001b[1m 8 neurônios da ultima camada oculta, 1 neurônios na de saída \u001b[0m\n",
      "\u001b[1m Camada de Entrada: 7 neurônios inputs  12 neurônios saída 0.001 learning rate \u001b[0m\n",
      " _camada oculta 12 neurônios 1 camadas \n",
      "\u001b[1m 12 neurônios da ultima camada oculta, 1 neurônios na de saída \u001b[0m\n",
      "\u001b[1m Camada de Entrada: 7 neurônios inputs  12 neurônios saída 0.001 learning rate \u001b[0m\n",
      " _camada oculta 12 neurônios 1 camadas \n",
      "\u001b[1m 12 neurônios da ultima camada oculta, 1 neurônios na de saída \u001b[0m\n",
      "\u001b[1m Camada de Entrada: 7 neurônios inputs  12 neurônios saída 0.001 learning rate \u001b[0m\n",
      " _camada oculta 12 neurônios 1 camadas \n",
      "\u001b[1m 12 neurônios da ultima camada oculta, 1 neurônios na de saída \u001b[0m\n",
      "\u001b[1m Camada de Entrada: 7 neurônios inputs  12 neurônios saída 0.001 learning rate \u001b[0m\n",
      " _camada oculta 12 neurônios 1 camadas \n",
      "\u001b[1m 12 neurônios da ultima camada oculta, 1 neurônios na de saída \u001b[0m\n",
      "\u001b[1m Camada de Entrada: 7 neurônios inputs  12 neurônios saída 0.001 learning rate \u001b[0m\n",
      " _camada oculta 12 neurônios 1 camadas \n",
      "\u001b[1m 12 neurônios da ultima camada oculta, 1 neurônios na de saída \u001b[0m\n",
      "\u001b[1m Camada de Entrada: 7 neurônios inputs  12 neurônios saída 0.001 learning rate \u001b[0m\n",
      " _camada oculta 12 neurônios 1 camadas \n",
      "\u001b[1m 12 neurônios da ultima camada oculta, 1 neurônios na de saída \u001b[0m\n",
      "\u001b[1m Camada de Entrada: 7 neurônios inputs  12 neurônios saída 0.005 learning rate \u001b[0m\n",
      " _camada oculta 12 neurônios 1 camadas \n",
      "\u001b[1m 12 neurônios da ultima camada oculta, 1 neurônios na de saída \u001b[0m\n",
      "\u001b[1m Camada de Entrada: 7 neurônios inputs  12 neurônios saída 0.005 learning rate \u001b[0m\n",
      " _camada oculta 12 neurônios 1 camadas \n",
      "\u001b[1m 12 neurônios da ultima camada oculta, 1 neurônios na de saída \u001b[0m\n",
      "\u001b[1m Camada de Entrada: 7 neurônios inputs  12 neurônios saída 0.005 learning rate \u001b[0m\n",
      " _camada oculta 12 neurônios 1 camadas \n",
      "\u001b[1m 12 neurônios da ultima camada oculta, 1 neurônios na de saída \u001b[0m\n",
      "\u001b[1m Camada de Entrada: 7 neurônios inputs  12 neurônios saída 0.005 learning rate \u001b[0m\n",
      " _camada oculta 12 neurônios 1 camadas \n",
      "\u001b[1m 12 neurônios da ultima camada oculta, 1 neurônios na de saída \u001b[0m\n",
      "\u001b[1m Camada de Entrada: 7 neurônios inputs  12 neurônios saída 0.005 learning rate \u001b[0m\n",
      " _camada oculta 12 neurônios 1 camadas \n",
      "\u001b[1m 12 neurônios da ultima camada oculta, 1 neurônios na de saída \u001b[0m\n",
      "\u001b[1m Camada de Entrada: 7 neurônios inputs  12 neurônios saída 0.005 learning rate \u001b[0m\n",
      " _camada oculta 12 neurônios 1 camadas \n",
      "\u001b[1m 12 neurônios da ultima camada oculta, 1 neurônios na de saída \u001b[0m\n",
      "\u001b[1m Camada de Entrada: 7 neurônios inputs  12 neurônios saída 0.01 learning rate \u001b[0m\n",
      " _camada oculta 12 neurônios 1 camadas \n",
      "\u001b[1m 12 neurônios da ultima camada oculta, 1 neurônios na de saída \u001b[0m\n",
      "\u001b[1m Camada de Entrada: 7 neurônios inputs  12 neurônios saída 0.01 learning rate \u001b[0m\n",
      " _camada oculta 12 neurônios 1 camadas \n",
      "\u001b[1m 12 neurônios da ultima camada oculta, 1 neurônios na de saída \u001b[0m\n",
      "\u001b[1m Camada de Entrada: 7 neurônios inputs  12 neurônios saída 0.01 learning rate \u001b[0m\n",
      " _camada oculta 12 neurônios 1 camadas \n",
      "\u001b[1m 12 neurônios da ultima camada oculta, 1 neurônios na de saída \u001b[0m\n",
      "\u001b[1m Camada de Entrada: 7 neurônios inputs  12 neurônios saída 0.01 learning rate \u001b[0m\n",
      " _camada oculta 12 neurônios 1 camadas \n",
      "\u001b[1m 12 neurônios da ultima camada oculta, 1 neurônios na de saída \u001b[0m\n",
      "\u001b[1m Camada de Entrada: 7 neurônios inputs  12 neurônios saída 0.01 learning rate \u001b[0m\n",
      " _camada oculta 12 neurônios 1 camadas \n",
      "\u001b[1m 12 neurônios da ultima camada oculta, 1 neurônios na de saída \u001b[0m\n",
      "\u001b[1m Camada de Entrada: 7 neurônios inputs  12 neurônios saída 0.01 learning rate \u001b[0m\n",
      " _camada oculta 12 neurônios 1 camadas \n",
      "\u001b[1m 12 neurônios da ultima camada oculta, 1 neurônios na de saída \u001b[0m\n",
      "\u001b[1m Camada de Entrada: 7 neurônios inputs  7 neurônios saída 0.001 learning rate \u001b[0m\n",
      " _camada oculta 7 neurônios 2 camadas \n",
      " _camada oculta 7 neurônios 2 camadas \n",
      "\u001b[1m 7 neurônios da ultima camada oculta, 1 neurônios na de saída \u001b[0m\n",
      "\u001b[1m Camada de Entrada: 7 neurônios inputs  7 neurônios saída 0.001 learning rate \u001b[0m\n",
      " _camada oculta 7 neurônios 2 camadas \n",
      " _camada oculta 7 neurônios 2 camadas \n",
      "\u001b[1m 7 neurônios da ultima camada oculta, 1 neurônios na de saída \u001b[0m\n",
      "\u001b[1m Camada de Entrada: 7 neurônios inputs  7 neurônios saída 0.001 learning rate \u001b[0m\n",
      " _camada oculta 7 neurônios 2 camadas \n",
      " _camada oculta 7 neurônios 2 camadas \n",
      "\u001b[1m 7 neurônios da ultima camada oculta, 1 neurônios na de saída \u001b[0m\n",
      "\u001b[1m Camada de Entrada: 7 neurônios inputs  7 neurônios saída 0.001 learning rate \u001b[0m\n",
      " _camada oculta 7 neurônios 2 camadas \n",
      " _camada oculta 7 neurônios 2 camadas \n",
      "\u001b[1m 7 neurônios da ultima camada oculta, 1 neurônios na de saída \u001b[0m\n",
      "\u001b[1m Camada de Entrada: 7 neurônios inputs  7 neurônios saída 0.001 learning rate \u001b[0m\n",
      " _camada oculta 7 neurônios 2 camadas \n",
      " _camada oculta 7 neurônios 2 camadas \n",
      "\u001b[1m 7 neurônios da ultima camada oculta, 1 neurônios na de saída \u001b[0m\n",
      "\u001b[1m Camada de Entrada: 7 neurônios inputs  7 neurônios saída 0.001 learning rate \u001b[0m\n",
      " _camada oculta 7 neurônios 2 camadas \n",
      " _camada oculta 7 neurônios 2 camadas \n",
      "\u001b[1m 7 neurônios da ultima camada oculta, 1 neurônios na de saída \u001b[0m\n",
      "\u001b[1m Camada de Entrada: 7 neurônios inputs  7 neurônios saída 0.005 learning rate \u001b[0m\n",
      " _camada oculta 7 neurônios 2 camadas \n",
      " _camada oculta 7 neurônios 2 camadas \n",
      "\u001b[1m 7 neurônios da ultima camada oculta, 1 neurônios na de saída \u001b[0m\n",
      "\u001b[1m Camada de Entrada: 7 neurônios inputs  7 neurônios saída 0.005 learning rate \u001b[0m\n",
      " _camada oculta 7 neurônios 2 camadas \n",
      " _camada oculta 7 neurônios 2 camadas \n",
      "\u001b[1m 7 neurônios da ultima camada oculta, 1 neurônios na de saída \u001b[0m\n",
      "\u001b[1m Camada de Entrada: 7 neurônios inputs  7 neurônios saída 0.005 learning rate \u001b[0m\n",
      " _camada oculta 7 neurônios 2 camadas \n",
      " _camada oculta 7 neurônios 2 camadas \n",
      "\u001b[1m 7 neurônios da ultima camada oculta, 1 neurônios na de saída \u001b[0m\n",
      "\u001b[1m Camada de Entrada: 7 neurônios inputs  7 neurônios saída 0.005 learning rate \u001b[0m\n",
      " _camada oculta 7 neurônios 2 camadas \n",
      " _camada oculta 7 neurônios 2 camadas \n",
      "\u001b[1m 7 neurônios da ultima camada oculta, 1 neurônios na de saída \u001b[0m\n",
      "\u001b[1m Camada de Entrada: 7 neurônios inputs  7 neurônios saída 0.005 learning rate \u001b[0m\n",
      " _camada oculta 7 neurônios 2 camadas \n",
      " _camada oculta 7 neurônios 2 camadas \n",
      "\u001b[1m 7 neurônios da ultima camada oculta, 1 neurônios na de saída \u001b[0m\n",
      "\u001b[1m Camada de Entrada: 7 neurônios inputs  7 neurônios saída 0.005 learning rate \u001b[0m\n",
      " _camada oculta 7 neurônios 2 camadas \n",
      " _camada oculta 7 neurônios 2 camadas \n",
      "\u001b[1m 7 neurônios da ultima camada oculta, 1 neurônios na de saída \u001b[0m\n",
      "\u001b[1m Camada de Entrada: 7 neurônios inputs  7 neurônios saída 0.01 learning rate \u001b[0m\n",
      " _camada oculta 7 neurônios 2 camadas \n",
      " _camada oculta 7 neurônios 2 camadas \n",
      "\u001b[1m 7 neurônios da ultima camada oculta, 1 neurônios na de saída \u001b[0m\n",
      "\u001b[1m Camada de Entrada: 7 neurônios inputs  7 neurônios saída 0.01 learning rate \u001b[0m\n",
      " _camada oculta 7 neurônios 2 camadas \n",
      " _camada oculta 7 neurônios 2 camadas \n",
      "\u001b[1m 7 neurônios da ultima camada oculta, 1 neurônios na de saída \u001b[0m\n",
      "\u001b[1m Camada de Entrada: 7 neurônios inputs  7 neurônios saída 0.01 learning rate \u001b[0m\n",
      " _camada oculta 7 neurônios 2 camadas \n",
      " _camada oculta 7 neurônios 2 camadas \n",
      "\u001b[1m 7 neurônios da ultima camada oculta, 1 neurônios na de saída \u001b[0m\n",
      "\u001b[1m Camada de Entrada: 7 neurônios inputs  7 neurônios saída 0.01 learning rate \u001b[0m\n",
      " _camada oculta 7 neurônios 2 camadas \n",
      " _camada oculta 7 neurônios 2 camadas \n",
      "\u001b[1m 7 neurônios da ultima camada oculta, 1 neurônios na de saída \u001b[0m\n",
      "\u001b[1m Camada de Entrada: 7 neurônios inputs  7 neurônios saída 0.01 learning rate \u001b[0m\n",
      " _camada oculta 7 neurônios 2 camadas \n",
      " _camada oculta 7 neurônios 2 camadas \n",
      "\u001b[1m 7 neurônios da ultima camada oculta, 1 neurônios na de saída \u001b[0m\n",
      "\u001b[1m Camada de Entrada: 7 neurônios inputs  7 neurônios saída 0.01 learning rate \u001b[0m\n",
      " _camada oculta 7 neurônios 2 camadas \n",
      " _camada oculta 7 neurônios 2 camadas \n",
      "\u001b[1m 7 neurônios da ultima camada oculta, 1 neurônios na de saída \u001b[0m\n",
      "\u001b[1m Camada de Entrada: 7 neurônios inputs  8 neurônios saída 0.001 learning rate \u001b[0m\n",
      " _camada oculta 8 neurônios 2 camadas \n",
      " _camada oculta 8 neurônios 2 camadas \n",
      "\u001b[1m 8 neurônios da ultima camada oculta, 1 neurônios na de saída \u001b[0m\n",
      "\u001b[1m Camada de Entrada: 7 neurônios inputs  8 neurônios saída 0.001 learning rate \u001b[0m\n",
      " _camada oculta 8 neurônios 2 camadas \n",
      " _camada oculta 8 neurônios 2 camadas \n",
      "\u001b[1m 8 neurônios da ultima camada oculta, 1 neurônios na de saída \u001b[0m\n",
      "\u001b[1m Camada de Entrada: 7 neurônios inputs  8 neurônios saída 0.001 learning rate \u001b[0m\n",
      " _camada oculta 8 neurônios 2 camadas \n",
      " _camada oculta 8 neurônios 2 camadas \n",
      "\u001b[1m 8 neurônios da ultima camada oculta, 1 neurônios na de saída \u001b[0m\n",
      "\u001b[1m Camada de Entrada: 7 neurônios inputs  8 neurônios saída 0.001 learning rate \u001b[0m\n",
      " _camada oculta 8 neurônios 2 camadas \n",
      " _camada oculta 8 neurônios 2 camadas \n",
      "\u001b[1m 8 neurônios da ultima camada oculta, 1 neurônios na de saída \u001b[0m\n",
      "\u001b[1m Camada de Entrada: 7 neurônios inputs  8 neurônios saída 0.001 learning rate \u001b[0m\n",
      " _camada oculta 8 neurônios 2 camadas \n",
      " _camada oculta 8 neurônios 2 camadas \n",
      "\u001b[1m 8 neurônios da ultima camada oculta, 1 neurônios na de saída \u001b[0m\n",
      "\u001b[1m Camada de Entrada: 7 neurônios inputs  8 neurônios saída 0.001 learning rate \u001b[0m\n",
      " _camada oculta 8 neurônios 2 camadas \n",
      " _camada oculta 8 neurônios 2 camadas \n",
      "\u001b[1m 8 neurônios da ultima camada oculta, 1 neurônios na de saída \u001b[0m\n",
      "\u001b[1m Camada de Entrada: 7 neurônios inputs  8 neurônios saída 0.005 learning rate \u001b[0m\n",
      " _camada oculta 8 neurônios 2 camadas \n",
      " _camada oculta 8 neurônios 2 camadas \n",
      "\u001b[1m 8 neurônios da ultima camada oculta, 1 neurônios na de saída \u001b[0m\n",
      "\u001b[1m Camada de Entrada: 7 neurônios inputs  8 neurônios saída 0.005 learning rate \u001b[0m\n",
      " _camada oculta 8 neurônios 2 camadas \n",
      " _camada oculta 8 neurônios 2 camadas \n",
      "\u001b[1m 8 neurônios da ultima camada oculta, 1 neurônios na de saída \u001b[0m\n",
      "\u001b[1m Camada de Entrada: 7 neurônios inputs  8 neurônios saída 0.005 learning rate \u001b[0m\n",
      " _camada oculta 8 neurônios 2 camadas \n",
      " _camada oculta 8 neurônios 2 camadas \n",
      "\u001b[1m 8 neurônios da ultima camada oculta, 1 neurônios na de saída \u001b[0m\n",
      "\u001b[1m Camada de Entrada: 7 neurônios inputs  8 neurônios saída 0.005 learning rate \u001b[0m\n",
      " _camada oculta 8 neurônios 2 camadas \n",
      " _camada oculta 8 neurônios 2 camadas \n",
      "\u001b[1m 8 neurônios da ultima camada oculta, 1 neurônios na de saída \u001b[0m\n",
      "\u001b[1m Camada de Entrada: 7 neurônios inputs  8 neurônios saída 0.005 learning rate \u001b[0m\n",
      " _camada oculta 8 neurônios 2 camadas \n",
      " _camada oculta 8 neurônios 2 camadas \n",
      "\u001b[1m 8 neurônios da ultima camada oculta, 1 neurônios na de saída \u001b[0m\n",
      "\u001b[1m Camada de Entrada: 7 neurônios inputs  8 neurônios saída 0.005 learning rate \u001b[0m\n",
      " _camada oculta 8 neurônios 2 camadas \n",
      " _camada oculta 8 neurônios 2 camadas \n",
      "\u001b[1m 8 neurônios da ultima camada oculta, 1 neurônios na de saída \u001b[0m\n",
      "\u001b[1m Camada de Entrada: 7 neurônios inputs  8 neurônios saída 0.01 learning rate \u001b[0m\n",
      " _camada oculta 8 neurônios 2 camadas \n",
      " _camada oculta 8 neurônios 2 camadas \n",
      "\u001b[1m 8 neurônios da ultima camada oculta, 1 neurônios na de saída \u001b[0m\n",
      "\u001b[1m Camada de Entrada: 7 neurônios inputs  8 neurônios saída 0.01 learning rate \u001b[0m\n",
      " _camada oculta 8 neurônios 2 camadas \n",
      " _camada oculta 8 neurônios 2 camadas \n",
      "\u001b[1m 8 neurônios da ultima camada oculta, 1 neurônios na de saída \u001b[0m\n",
      "\u001b[1m Camada de Entrada: 7 neurônios inputs  8 neurônios saída 0.01 learning rate \u001b[0m\n",
      " _camada oculta 8 neurônios 2 camadas \n",
      " _camada oculta 8 neurônios 2 camadas \n",
      "\u001b[1m 8 neurônios da ultima camada oculta, 1 neurônios na de saída \u001b[0m\n",
      "\u001b[1m Camada de Entrada: 7 neurônios inputs  8 neurônios saída 0.01 learning rate \u001b[0m\n",
      " _camada oculta 8 neurônios 2 camadas \n",
      " _camada oculta 8 neurônios 2 camadas \n",
      "\u001b[1m 8 neurônios da ultima camada oculta, 1 neurônios na de saída \u001b[0m\n",
      "\u001b[1m Camada de Entrada: 7 neurônios inputs  8 neurônios saída 0.01 learning rate \u001b[0m\n",
      " _camada oculta 8 neurônios 2 camadas \n",
      " _camada oculta 8 neurônios 2 camadas \n",
      "\u001b[1m 8 neurônios da ultima camada oculta, 1 neurônios na de saída \u001b[0m\n",
      "\u001b[1m Camada de Entrada: 7 neurônios inputs  8 neurônios saída 0.01 learning rate \u001b[0m\n",
      " _camada oculta 8 neurônios 2 camadas \n",
      " _camada oculta 8 neurônios 2 camadas \n",
      "\u001b[1m 8 neurônios da ultima camada oculta, 1 neurônios na de saída \u001b[0m\n",
      "\u001b[1m Camada de Entrada: 7 neurônios inputs  12 neurônios saída 0.001 learning rate \u001b[0m\n",
      " _camada oculta 12 neurônios 2 camadas \n",
      " _camada oculta 12 neurônios 2 camadas \n",
      "\u001b[1m 12 neurônios da ultima camada oculta, 1 neurônios na de saída \u001b[0m\n",
      "\u001b[1m Camada de Entrada: 7 neurônios inputs  12 neurônios saída 0.001 learning rate \u001b[0m\n",
      " _camada oculta 12 neurônios 2 camadas \n",
      " _camada oculta 12 neurônios 2 camadas \n",
      "\u001b[1m 12 neurônios da ultima camada oculta, 1 neurônios na de saída \u001b[0m\n",
      "\u001b[1m Camada de Entrada: 7 neurônios inputs  12 neurônios saída 0.001 learning rate \u001b[0m\n",
      " _camada oculta 12 neurônios 2 camadas \n",
      " _camada oculta 12 neurônios 2 camadas \n",
      "\u001b[1m 12 neurônios da ultima camada oculta, 1 neurônios na de saída \u001b[0m\n",
      "\u001b[1m Camada de Entrada: 7 neurônios inputs  12 neurônios saída 0.001 learning rate \u001b[0m\n",
      " _camada oculta 12 neurônios 2 camadas \n",
      " _camada oculta 12 neurônios 2 camadas \n",
      "\u001b[1m 12 neurônios da ultima camada oculta, 1 neurônios na de saída \u001b[0m\n",
      "\u001b[1m Camada de Entrada: 7 neurônios inputs  12 neurônios saída 0.001 learning rate \u001b[0m\n",
      " _camada oculta 12 neurônios 2 camadas \n",
      " _camada oculta 12 neurônios 2 camadas \n",
      "\u001b[1m 12 neurônios da ultima camada oculta, 1 neurônios na de saída \u001b[0m\n",
      "\u001b[1m Camada de Entrada: 7 neurônios inputs  12 neurônios saída 0.001 learning rate \u001b[0m\n",
      " _camada oculta 12 neurônios 2 camadas \n",
      " _camada oculta 12 neurônios 2 camadas \n",
      "\u001b[1m 12 neurônios da ultima camada oculta, 1 neurônios na de saída \u001b[0m\n",
      "\u001b[1m Camada de Entrada: 7 neurônios inputs  12 neurônios saída 0.005 learning rate \u001b[0m\n",
      " _camada oculta 12 neurônios 2 camadas \n",
      " _camada oculta 12 neurônios 2 camadas \n",
      "\u001b[1m 12 neurônios da ultima camada oculta, 1 neurônios na de saída \u001b[0m\n",
      "\u001b[1m Camada de Entrada: 7 neurônios inputs  12 neurônios saída 0.005 learning rate \u001b[0m\n",
      " _camada oculta 12 neurônios 2 camadas \n",
      " _camada oculta 12 neurônios 2 camadas \n",
      "\u001b[1m 12 neurônios da ultima camada oculta, 1 neurônios na de saída \u001b[0m\n",
      "\u001b[1m Camada de Entrada: 7 neurônios inputs  12 neurônios saída 0.005 learning rate \u001b[0m\n",
      " _camada oculta 12 neurônios 2 camadas \n",
      " _camada oculta 12 neurônios 2 camadas \n",
      "\u001b[1m 12 neurônios da ultima camada oculta, 1 neurônios na de saída \u001b[0m\n",
      "\u001b[1m Camada de Entrada: 7 neurônios inputs  12 neurônios saída 0.005 learning rate \u001b[0m\n",
      " _camada oculta 12 neurônios 2 camadas \n",
      " _camada oculta 12 neurônios 2 camadas \n",
      "\u001b[1m 12 neurônios da ultima camada oculta, 1 neurônios na de saída \u001b[0m\n",
      "\u001b[1m Camada de Entrada: 7 neurônios inputs  12 neurônios saída 0.005 learning rate \u001b[0m\n",
      " _camada oculta 12 neurônios 2 camadas \n",
      " _camada oculta 12 neurônios 2 camadas \n",
      "\u001b[1m 12 neurônios da ultima camada oculta, 1 neurônios na de saída \u001b[0m\n",
      "\u001b[1m Camada de Entrada: 7 neurônios inputs  12 neurônios saída 0.005 learning rate \u001b[0m\n",
      " _camada oculta 12 neurônios 2 camadas \n",
      " _camada oculta 12 neurônios 2 camadas \n",
      "\u001b[1m 12 neurônios da ultima camada oculta, 1 neurônios na de saída \u001b[0m\n",
      "\u001b[1m Camada de Entrada: 7 neurônios inputs  12 neurônios saída 0.01 learning rate \u001b[0m\n",
      " _camada oculta 12 neurônios 2 camadas \n",
      " _camada oculta 12 neurônios 2 camadas \n",
      "\u001b[1m 12 neurônios da ultima camada oculta, 1 neurônios na de saída \u001b[0m\n",
      "\u001b[1m Camada de Entrada: 7 neurônios inputs  12 neurônios saída 0.01 learning rate \u001b[0m\n",
      " _camada oculta 12 neurônios 2 camadas \n",
      " _camada oculta 12 neurônios 2 camadas \n",
      "\u001b[1m 12 neurônios da ultima camada oculta, 1 neurônios na de saída \u001b[0m\n",
      "\u001b[1m Camada de Entrada: 7 neurônios inputs  12 neurônios saída 0.01 learning rate \u001b[0m\n",
      " _camada oculta 12 neurônios 2 camadas \n",
      " _camada oculta 12 neurônios 2 camadas \n",
      "\u001b[1m 12 neurônios da ultima camada oculta, 1 neurônios na de saída \u001b[0m\n",
      "\u001b[1m Camada de Entrada: 7 neurônios inputs  12 neurônios saída 0.01 learning rate \u001b[0m\n",
      " _camada oculta 12 neurônios 2 camadas \n",
      " _camada oculta 12 neurônios 2 camadas \n",
      "\u001b[1m 12 neurônios da ultima camada oculta, 1 neurônios na de saída \u001b[0m\n",
      "\u001b[1m Camada de Entrada: 7 neurônios inputs  12 neurônios saída 0.01 learning rate \u001b[0m\n",
      " _camada oculta 12 neurônios 2 camadas \n",
      " _camada oculta 12 neurônios 2 camadas \n",
      "\u001b[1m 12 neurônios da ultima camada oculta, 1 neurônios na de saída \u001b[0m\n",
      "\u001b[1m Camada de Entrada: 7 neurônios inputs  12 neurônios saída 0.01 learning rate \u001b[0m\n",
      " _camada oculta 12 neurônios 2 camadas \n",
      " _camada oculta 12 neurônios 2 camadas \n",
      "\u001b[1m 12 neurônios da ultima camada oculta, 1 neurônios na de saída \u001b[0m\n",
      "\u001b[1m Camada de Entrada: 7 neurônios inputs  7 neurônios saída 0.001 learning rate \u001b[0m\n",
      " _camada oculta 7 neurônios 3 camadas \n",
      " _camada oculta 7 neurônios 3 camadas \n",
      " _camada oculta 7 neurônios 3 camadas \n",
      "\u001b[1m 7 neurônios da ultima camada oculta, 1 neurônios na de saída \u001b[0m\n",
      "\u001b[1m Camada de Entrada: 7 neurônios inputs  7 neurônios saída 0.001 learning rate \u001b[0m\n",
      " _camada oculta 7 neurônios 3 camadas \n",
      " _camada oculta 7 neurônios 3 camadas \n",
      " _camada oculta 7 neurônios 3 camadas \n",
      "\u001b[1m 7 neurônios da ultima camada oculta, 1 neurônios na de saída \u001b[0m\n",
      "\u001b[1m Camada de Entrada: 7 neurônios inputs  7 neurônios saída 0.001 learning rate \u001b[0m\n",
      " _camada oculta 7 neurônios 3 camadas \n",
      " _camada oculta 7 neurônios 3 camadas \n",
      " _camada oculta 7 neurônios 3 camadas \n",
      "\u001b[1m 7 neurônios da ultima camada oculta, 1 neurônios na de saída \u001b[0m\n",
      "\u001b[1m Camada de Entrada: 7 neurônios inputs  7 neurônios saída 0.001 learning rate \u001b[0m\n",
      " _camada oculta 7 neurônios 3 camadas \n",
      " _camada oculta 7 neurônios 3 camadas \n",
      " _camada oculta 7 neurônios 3 camadas \n",
      "\u001b[1m 7 neurônios da ultima camada oculta, 1 neurônios na de saída \u001b[0m\n",
      "\u001b[1m Camada de Entrada: 7 neurônios inputs  7 neurônios saída 0.001 learning rate \u001b[0m\n",
      " _camada oculta 7 neurônios 3 camadas \n",
      " _camada oculta 7 neurônios 3 camadas \n",
      " _camada oculta 7 neurônios 3 camadas \n",
      "\u001b[1m 7 neurônios da ultima camada oculta, 1 neurônios na de saída \u001b[0m\n",
      "\u001b[1m Camada de Entrada: 7 neurônios inputs  7 neurônios saída 0.001 learning rate \u001b[0m\n",
      " _camada oculta 7 neurônios 3 camadas \n",
      " _camada oculta 7 neurônios 3 camadas \n",
      " _camada oculta 7 neurônios 3 camadas \n",
      "\u001b[1m 7 neurônios da ultima camada oculta, 1 neurônios na de saída \u001b[0m\n",
      "\u001b[1m Camada de Entrada: 7 neurônios inputs  7 neurônios saída 0.005 learning rate \u001b[0m\n",
      " _camada oculta 7 neurônios 3 camadas \n",
      " _camada oculta 7 neurônios 3 camadas \n",
      " _camada oculta 7 neurônios 3 camadas \n",
      "\u001b[1m 7 neurônios da ultima camada oculta, 1 neurônios na de saída \u001b[0m\n",
      "\u001b[1m Camada de Entrada: 7 neurônios inputs  7 neurônios saída 0.005 learning rate \u001b[0m\n",
      " _camada oculta 7 neurônios 3 camadas \n",
      " _camada oculta 7 neurônios 3 camadas \n",
      " _camada oculta 7 neurônios 3 camadas \n",
      "\u001b[1m 7 neurônios da ultima camada oculta, 1 neurônios na de saída \u001b[0m\n",
      "\u001b[1m Camada de Entrada: 7 neurônios inputs  7 neurônios saída 0.005 learning rate \u001b[0m\n",
      " _camada oculta 7 neurônios 3 camadas \n",
      " _camada oculta 7 neurônios 3 camadas \n",
      " _camada oculta 7 neurônios 3 camadas \n",
      "\u001b[1m 7 neurônios da ultima camada oculta, 1 neurônios na de saída \u001b[0m\n",
      "\u001b[1m Camada de Entrada: 7 neurônios inputs  7 neurônios saída 0.005 learning rate \u001b[0m\n",
      " _camada oculta 7 neurônios 3 camadas \n",
      " _camada oculta 7 neurônios 3 camadas \n",
      " _camada oculta 7 neurônios 3 camadas \n",
      "\u001b[1m 7 neurônios da ultima camada oculta, 1 neurônios na de saída \u001b[0m\n",
      "\u001b[1m Camada de Entrada: 7 neurônios inputs  7 neurônios saída 0.005 learning rate \u001b[0m\n",
      " _camada oculta 7 neurônios 3 camadas \n",
      " _camada oculta 7 neurônios 3 camadas \n",
      " _camada oculta 7 neurônios 3 camadas \n",
      "\u001b[1m 7 neurônios da ultima camada oculta, 1 neurônios na de saída \u001b[0m\n",
      "\u001b[1m Camada de Entrada: 7 neurônios inputs  7 neurônios saída 0.005 learning rate \u001b[0m\n",
      " _camada oculta 7 neurônios 3 camadas \n",
      " _camada oculta 7 neurônios 3 camadas \n",
      " _camada oculta 7 neurônios 3 camadas \n",
      "\u001b[1m 7 neurônios da ultima camada oculta, 1 neurônios na de saída \u001b[0m\n",
      "\u001b[1m Camada de Entrada: 7 neurônios inputs  7 neurônios saída 0.01 learning rate \u001b[0m\n",
      " _camada oculta 7 neurônios 3 camadas \n",
      " _camada oculta 7 neurônios 3 camadas \n",
      " _camada oculta 7 neurônios 3 camadas \n",
      "\u001b[1m 7 neurônios da ultima camada oculta, 1 neurônios na de saída \u001b[0m\n",
      "\u001b[1m Camada de Entrada: 7 neurônios inputs  7 neurônios saída 0.01 learning rate \u001b[0m\n",
      " _camada oculta 7 neurônios 3 camadas \n",
      " _camada oculta 7 neurônios 3 camadas \n",
      " _camada oculta 7 neurônios 3 camadas \n",
      "\u001b[1m 7 neurônios da ultima camada oculta, 1 neurônios na de saída \u001b[0m\n",
      "\u001b[1m Camada de Entrada: 7 neurônios inputs  7 neurônios saída 0.01 learning rate \u001b[0m\n",
      " _camada oculta 7 neurônios 3 camadas \n",
      " _camada oculta 7 neurônios 3 camadas \n",
      " _camada oculta 7 neurônios 3 camadas \n",
      "\u001b[1m 7 neurônios da ultima camada oculta, 1 neurônios na de saída \u001b[0m\n",
      "\u001b[1m Camada de Entrada: 7 neurônios inputs  7 neurônios saída 0.01 learning rate \u001b[0m\n",
      " _camada oculta 7 neurônios 3 camadas \n",
      " _camada oculta 7 neurônios 3 camadas \n",
      " _camada oculta 7 neurônios 3 camadas \n",
      "\u001b[1m 7 neurônios da ultima camada oculta, 1 neurônios na de saída \u001b[0m\n",
      "\u001b[1m Camada de Entrada: 7 neurônios inputs  7 neurônios saída 0.01 learning rate \u001b[0m\n",
      " _camada oculta 7 neurônios 3 camadas \n",
      " _camada oculta 7 neurônios 3 camadas \n",
      " _camada oculta 7 neurônios 3 camadas \n",
      "\u001b[1m 7 neurônios da ultima camada oculta, 1 neurônios na de saída \u001b[0m\n",
      "\u001b[1m Camada de Entrada: 7 neurônios inputs  7 neurônios saída 0.01 learning rate \u001b[0m\n",
      " _camada oculta 7 neurônios 3 camadas \n",
      " _camada oculta 7 neurônios 3 camadas \n",
      " _camada oculta 7 neurônios 3 camadas \n",
      "\u001b[1m 7 neurônios da ultima camada oculta, 1 neurônios na de saída \u001b[0m\n",
      "\u001b[1m Camada de Entrada: 7 neurônios inputs  8 neurônios saída 0.001 learning rate \u001b[0m\n",
      " _camada oculta 8 neurônios 3 camadas \n",
      " _camada oculta 8 neurônios 3 camadas \n",
      " _camada oculta 8 neurônios 3 camadas \n",
      "\u001b[1m 8 neurônios da ultima camada oculta, 1 neurônios na de saída \u001b[0m\n",
      "\u001b[1m Camada de Entrada: 7 neurônios inputs  8 neurônios saída 0.001 learning rate \u001b[0m\n",
      " _camada oculta 8 neurônios 3 camadas \n",
      " _camada oculta 8 neurônios 3 camadas \n",
      " _camada oculta 8 neurônios 3 camadas \n",
      "\u001b[1m 8 neurônios da ultima camada oculta, 1 neurônios na de saída \u001b[0m\n",
      "\u001b[1m Camada de Entrada: 7 neurônios inputs  8 neurônios saída 0.001 learning rate \u001b[0m\n",
      " _camada oculta 8 neurônios 3 camadas \n",
      " _camada oculta 8 neurônios 3 camadas \n",
      " _camada oculta 8 neurônios 3 camadas \n",
      "\u001b[1m 8 neurônios da ultima camada oculta, 1 neurônios na de saída \u001b[0m\n",
      "\u001b[1m Camada de Entrada: 7 neurônios inputs  8 neurônios saída 0.001 learning rate \u001b[0m\n",
      " _camada oculta 8 neurônios 3 camadas \n",
      " _camada oculta 8 neurônios 3 camadas \n",
      " _camada oculta 8 neurônios 3 camadas \n",
      "\u001b[1m 8 neurônios da ultima camada oculta, 1 neurônios na de saída \u001b[0m\n",
      "\u001b[1m Camada de Entrada: 7 neurônios inputs  8 neurônios saída 0.001 learning rate \u001b[0m\n",
      " _camada oculta 8 neurônios 3 camadas \n",
      " _camada oculta 8 neurônios 3 camadas \n",
      " _camada oculta 8 neurônios 3 camadas \n",
      "\u001b[1m 8 neurônios da ultima camada oculta, 1 neurônios na de saída \u001b[0m\n",
      "\u001b[1m Camada de Entrada: 7 neurônios inputs  8 neurônios saída 0.001 learning rate \u001b[0m\n",
      " _camada oculta 8 neurônios 3 camadas \n",
      " _camada oculta 8 neurônios 3 camadas \n",
      " _camada oculta 8 neurônios 3 camadas \n",
      "\u001b[1m 8 neurônios da ultima camada oculta, 1 neurônios na de saída \u001b[0m\n",
      "\u001b[1m Camada de Entrada: 7 neurônios inputs  8 neurônios saída 0.005 learning rate \u001b[0m\n",
      " _camada oculta 8 neurônios 3 camadas \n",
      " _camada oculta 8 neurônios 3 camadas \n",
      " _camada oculta 8 neurônios 3 camadas \n",
      "\u001b[1m 8 neurônios da ultima camada oculta, 1 neurônios na de saída \u001b[0m\n",
      "\u001b[1m Camada de Entrada: 7 neurônios inputs  8 neurônios saída 0.005 learning rate \u001b[0m\n",
      " _camada oculta 8 neurônios 3 camadas \n",
      " _camada oculta 8 neurônios 3 camadas \n",
      " _camada oculta 8 neurônios 3 camadas \n",
      "\u001b[1m 8 neurônios da ultima camada oculta, 1 neurônios na de saída \u001b[0m\n",
      "\u001b[1m Camada de Entrada: 7 neurônios inputs  8 neurônios saída 0.005 learning rate \u001b[0m\n",
      " _camada oculta 8 neurônios 3 camadas \n",
      " _camada oculta 8 neurônios 3 camadas \n",
      " _camada oculta 8 neurônios 3 camadas \n",
      "\u001b[1m 8 neurônios da ultima camada oculta, 1 neurônios na de saída \u001b[0m\n",
      "\u001b[1m Camada de Entrada: 7 neurônios inputs  8 neurônios saída 0.005 learning rate \u001b[0m\n",
      " _camada oculta 8 neurônios 3 camadas \n",
      " _camada oculta 8 neurônios 3 camadas \n",
      " _camada oculta 8 neurônios 3 camadas \n",
      "\u001b[1m 8 neurônios da ultima camada oculta, 1 neurônios na de saída \u001b[0m\n",
      "\u001b[1m Camada de Entrada: 7 neurônios inputs  8 neurônios saída 0.005 learning rate \u001b[0m\n",
      " _camada oculta 8 neurônios 3 camadas \n",
      " _camada oculta 8 neurônios 3 camadas \n",
      " _camada oculta 8 neurônios 3 camadas \n",
      "\u001b[1m 8 neurônios da ultima camada oculta, 1 neurônios na de saída \u001b[0m\n",
      "\u001b[1m Camada de Entrada: 7 neurônios inputs  8 neurônios saída 0.005 learning rate \u001b[0m\n",
      " _camada oculta 8 neurônios 3 camadas \n",
      " _camada oculta 8 neurônios 3 camadas \n",
      " _camada oculta 8 neurônios 3 camadas \n",
      "\u001b[1m 8 neurônios da ultima camada oculta, 1 neurônios na de saída \u001b[0m\n",
      "\u001b[1m Camada de Entrada: 7 neurônios inputs  8 neurônios saída 0.01 learning rate \u001b[0m\n",
      " _camada oculta 8 neurônios 3 camadas \n",
      " _camada oculta 8 neurônios 3 camadas \n",
      " _camada oculta 8 neurônios 3 camadas \n",
      "\u001b[1m 8 neurônios da ultima camada oculta, 1 neurônios na de saída \u001b[0m\n",
      "\u001b[1m Camada de Entrada: 7 neurônios inputs  8 neurônios saída 0.01 learning rate \u001b[0m\n",
      " _camada oculta 8 neurônios 3 camadas \n",
      " _camada oculta 8 neurônios 3 camadas \n",
      " _camada oculta 8 neurônios 3 camadas \n",
      "\u001b[1m 8 neurônios da ultima camada oculta, 1 neurônios na de saída \u001b[0m\n",
      "\u001b[1m Camada de Entrada: 7 neurônios inputs  8 neurônios saída 0.01 learning rate \u001b[0m\n",
      " _camada oculta 8 neurônios 3 camadas \n",
      " _camada oculta 8 neurônios 3 camadas \n",
      " _camada oculta 8 neurônios 3 camadas \n",
      "\u001b[1m 8 neurônios da ultima camada oculta, 1 neurônios na de saída \u001b[0m\n",
      "\u001b[1m Camada de Entrada: 7 neurônios inputs  8 neurônios saída 0.01 learning rate \u001b[0m\n",
      " _camada oculta 8 neurônios 3 camadas \n",
      " _camada oculta 8 neurônios 3 camadas \n",
      " _camada oculta 8 neurônios 3 camadas \n",
      "\u001b[1m 8 neurônios da ultima camada oculta, 1 neurônios na de saída \u001b[0m\n",
      "\u001b[1m Camada de Entrada: 7 neurônios inputs  8 neurônios saída 0.01 learning rate \u001b[0m\n",
      " _camada oculta 8 neurônios 3 camadas \n",
      " _camada oculta 8 neurônios 3 camadas \n",
      " _camada oculta 8 neurônios 3 camadas \n",
      "\u001b[1m 8 neurônios da ultima camada oculta, 1 neurônios na de saída \u001b[0m\n",
      "\u001b[1m Camada de Entrada: 7 neurônios inputs  8 neurônios saída 0.01 learning rate \u001b[0m\n",
      " _camada oculta 8 neurônios 3 camadas \n",
      " _camada oculta 8 neurônios 3 camadas \n",
      " _camada oculta 8 neurônios 3 camadas \n",
      "\u001b[1m 8 neurônios da ultima camada oculta, 1 neurônios na de saída \u001b[0m\n",
      "\u001b[1m Camada de Entrada: 7 neurônios inputs  12 neurônios saída 0.001 learning rate \u001b[0m\n",
      " _camada oculta 12 neurônios 3 camadas \n",
      " _camada oculta 12 neurônios 3 camadas \n",
      " _camada oculta 12 neurônios 3 camadas \n",
      "\u001b[1m 12 neurônios da ultima camada oculta, 1 neurônios na de saída \u001b[0m\n",
      "\u001b[1m Camada de Entrada: 7 neurônios inputs  12 neurônios saída 0.001 learning rate \u001b[0m\n",
      " _camada oculta 12 neurônios 3 camadas \n",
      " _camada oculta 12 neurônios 3 camadas \n",
      " _camada oculta 12 neurônios 3 camadas \n",
      "\u001b[1m 12 neurônios da ultima camada oculta, 1 neurônios na de saída \u001b[0m\n",
      "\u001b[1m Camada de Entrada: 7 neurônios inputs  12 neurônios saída 0.001 learning rate \u001b[0m\n",
      " _camada oculta 12 neurônios 3 camadas \n",
      " _camada oculta 12 neurônios 3 camadas \n",
      " _camada oculta 12 neurônios 3 camadas \n",
      "\u001b[1m 12 neurônios da ultima camada oculta, 1 neurônios na de saída \u001b[0m\n",
      "\u001b[1m Camada de Entrada: 7 neurônios inputs  12 neurônios saída 0.001 learning rate \u001b[0m\n",
      " _camada oculta 12 neurônios 3 camadas \n",
      " _camada oculta 12 neurônios 3 camadas \n",
      " _camada oculta 12 neurônios 3 camadas \n",
      "\u001b[1m 12 neurônios da ultima camada oculta, 1 neurônios na de saída \u001b[0m\n",
      "\u001b[1m Camada de Entrada: 7 neurônios inputs  12 neurônios saída 0.001 learning rate \u001b[0m\n",
      " _camada oculta 12 neurônios 3 camadas \n",
      " _camada oculta 12 neurônios 3 camadas \n",
      " _camada oculta 12 neurônios 3 camadas \n",
      "\u001b[1m 12 neurônios da ultima camada oculta, 1 neurônios na de saída \u001b[0m\n",
      "\u001b[1m Camada de Entrada: 7 neurônios inputs  12 neurônios saída 0.001 learning rate \u001b[0m\n",
      " _camada oculta 12 neurônios 3 camadas \n",
      " _camada oculta 12 neurônios 3 camadas \n",
      " _camada oculta 12 neurônios 3 camadas \n",
      "\u001b[1m 12 neurônios da ultima camada oculta, 1 neurônios na de saída \u001b[0m\n",
      "\u001b[1m Camada de Entrada: 7 neurônios inputs  12 neurônios saída 0.005 learning rate \u001b[0m\n",
      " _camada oculta 12 neurônios 3 camadas \n",
      " _camada oculta 12 neurônios 3 camadas \n",
      " _camada oculta 12 neurônios 3 camadas \n",
      "\u001b[1m 12 neurônios da ultima camada oculta, 1 neurônios na de saída \u001b[0m\n",
      "\u001b[1m Camada de Entrada: 7 neurônios inputs  12 neurônios saída 0.005 learning rate \u001b[0m\n",
      " _camada oculta 12 neurônios 3 camadas \n",
      " _camada oculta 12 neurônios 3 camadas \n",
      " _camada oculta 12 neurônios 3 camadas \n",
      "\u001b[1m 12 neurônios da ultima camada oculta, 1 neurônios na de saída \u001b[0m\n",
      "\u001b[1m Camada de Entrada: 7 neurônios inputs  12 neurônios saída 0.005 learning rate \u001b[0m\n",
      " _camada oculta 12 neurônios 3 camadas \n",
      " _camada oculta 12 neurônios 3 camadas \n",
      " _camada oculta 12 neurônios 3 camadas \n",
      "\u001b[1m 12 neurônios da ultima camada oculta, 1 neurônios na de saída \u001b[0m\n",
      "\u001b[1m Camada de Entrada: 7 neurônios inputs  12 neurônios saída 0.005 learning rate \u001b[0m\n",
      " _camada oculta 12 neurônios 3 camadas \n",
      " _camada oculta 12 neurônios 3 camadas \n",
      " _camada oculta 12 neurônios 3 camadas \n",
      "\u001b[1m 12 neurônios da ultima camada oculta, 1 neurônios na de saída \u001b[0m\n",
      "\u001b[1m Camada de Entrada: 7 neurônios inputs  12 neurônios saída 0.005 learning rate \u001b[0m\n",
      " _camada oculta 12 neurônios 3 camadas \n",
      " _camada oculta 12 neurônios 3 camadas \n",
      " _camada oculta 12 neurônios 3 camadas \n",
      "\u001b[1m 12 neurônios da ultima camada oculta, 1 neurônios na de saída \u001b[0m\n",
      "\u001b[1m Camada de Entrada: 7 neurônios inputs  12 neurônios saída 0.005 learning rate \u001b[0m\n",
      " _camada oculta 12 neurônios 3 camadas \n",
      " _camada oculta 12 neurônios 3 camadas \n",
      " _camada oculta 12 neurônios 3 camadas \n",
      "\u001b[1m 12 neurônios da ultima camada oculta, 1 neurônios na de saída \u001b[0m\n",
      "\u001b[1m Camada de Entrada: 7 neurônios inputs  12 neurônios saída 0.01 learning rate \u001b[0m\n",
      " _camada oculta 12 neurônios 3 camadas \n",
      " _camada oculta 12 neurônios 3 camadas \n",
      " _camada oculta 12 neurônios 3 camadas \n",
      "\u001b[1m 12 neurônios da ultima camada oculta, 1 neurônios na de saída \u001b[0m\n",
      "\u001b[1m Camada de Entrada: 7 neurônios inputs  12 neurônios saída 0.01 learning rate \u001b[0m\n",
      " _camada oculta 12 neurônios 3 camadas \n",
      " _camada oculta 12 neurônios 3 camadas \n",
      " _camada oculta 12 neurônios 3 camadas \n",
      "\u001b[1m 12 neurônios da ultima camada oculta, 1 neurônios na de saída \u001b[0m\n",
      "\u001b[1m Camada de Entrada: 7 neurônios inputs  12 neurônios saída 0.01 learning rate \u001b[0m\n",
      " _camada oculta 12 neurônios 3 camadas \n",
      " _camada oculta 12 neurônios 3 camadas \n",
      " _camada oculta 12 neurônios 3 camadas \n",
      "\u001b[1m 12 neurônios da ultima camada oculta, 1 neurônios na de saída \u001b[0m\n",
      "\u001b[1m Camada de Entrada: 7 neurônios inputs  12 neurônios saída 0.01 learning rate \u001b[0m\n",
      " _camada oculta 12 neurônios 3 camadas \n",
      " _camada oculta 12 neurônios 3 camadas \n",
      " _camada oculta 12 neurônios 3 camadas \n",
      "\u001b[1m 12 neurônios da ultima camada oculta, 1 neurônios na de saída \u001b[0m\n",
      "\u001b[1m Camada de Entrada: 7 neurônios inputs  12 neurônios saída 0.01 learning rate \u001b[0m\n",
      " _camada oculta 12 neurônios 3 camadas \n",
      " _camada oculta 12 neurônios 3 camadas \n",
      " _camada oculta 12 neurônios 3 camadas \n",
      "\u001b[1m 12 neurônios da ultima camada oculta, 1 neurônios na de saída \u001b[0m\n",
      "\u001b[1m Camada de Entrada: 7 neurônios inputs  12 neurônios saída 0.01 learning rate \u001b[0m\n",
      " _camada oculta 12 neurônios 3 camadas \n",
      " _camada oculta 12 neurônios 3 camadas \n",
      " _camada oculta 12 neurônios 3 camadas \n",
      "\u001b[1m 12 neurônios da ultima camada oculta, 1 neurônios na de saída \u001b[0m\n",
      "5846006549323611672814739330865132078623730171904\n"
     ]
    }
   ],
   "source": [
    "tot = 1\n",
    "for num_hidden_layers in hidden_layers:\n",
    "    for num_neurons_layers in neurons:\n",
    "        for num_learning_rate in learning_rate:\n",
    "            for num_batch_size in batch_size:\n",
    "                for prob_dropout in dropout_rate:\n",
    "                    setTestNeuralNetwork(num_hidden_layers,num_neurons_layers, num_learning_rate, prob_dropout, input_dim, output_dim)\n",
    "                    tot += tot\n",
    "                    #print('Para {} neurônios, {} camadas,  {} learning rate, {} batch size, {} dropout rate, temos: \\033[1m Acurácia: {:.2f}% \\033[0m'.format(num_neurons_layers,num_hidden_layers,num_learning_rate,num_batch_size,prob_dropout, accu))\n",
    "\n",
    "print(tot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:    0/40000 loss_train: 0.84490023 + 0.00000000 = 0.84490023 loss_val = 0.84490023\n",
      "epoch: 3000/40000 loss_train: 0.61791103 + 0.00000000 = 0.61791103 loss_val = 0.61791103\n",
      "epoch: 6000/40000 loss_train: 0.49647476 + 0.00000000 = 0.49647476 loss_val = 0.49647476\n",
      "epoch: 9000/40000 loss_train: 0.38543520 + 0.00000000 = 0.38543520 loss_val = 0.38543520\n",
      "epoch: 12000/40000 loss_train: 0.29875299 + 0.00000000 = 0.29875299 loss_val = 0.29875299\n",
      "epoch: 15000/40000 loss_train: 0.23619341 + 0.00000000 = 0.23619341 loss_val = 0.23619341\n",
      "epoch: 18000/40000 loss_train: 0.18882290 + 0.00000000 = 0.18882290 loss_val = 0.18882290\n",
      "epoch: 21000/40000 loss_train: 0.15404901 + 0.00000000 = 0.15404901 loss_val = 0.15404901\n",
      "epoch: 24000/40000 loss_train: 0.12861807 + 0.00000000 = 0.12861807 loss_val = 0.12861807\n",
      "epoch: 27000/40000 loss_train: 0.10554305 + 0.00000000 = 0.10554305 loss_val = 0.10554305\n",
      "epoch: 30000/40000 loss_train: 0.09085746 + 0.00000000 = 0.09085746 loss_val = 0.09085746\n",
      "epoch: 33000/40000 loss_train: 0.08122290 + 0.00000000 = 0.08122290 loss_val = 0.08122290\n",
      "epoch: 36000/40000 loss_train: 0.07402573 + 0.00000000 = 0.07402573 loss_val = 0.07402573\n",
      "epoch: 39000/40000 loss_train: 0.06828554 + 0.00000000 = 0.06828554 loss_val = 0.06828554\n",
      "Para 7 neurônios, 1 camadas,  0.001 learning rate, 0 batch size, 0 dropout rate, temos: \u001b[1m Acurácia: 100.00% \u001b[0m\n",
      "epoch:    0/40000 loss_train: 0.93504814 + 0.00000000 = 0.93504814 loss_val = 0.93504814\n",
      "epoch: 3000/40000 loss_train: 0.60198059 + 0.00000000 = 0.60198059 loss_val = 0.60198059\n",
      "epoch: 6000/40000 loss_train: 0.46164330 + 0.00000000 = 0.46164330 loss_val = 0.46164330\n",
      "epoch: 9000/40000 loss_train: 0.34596514 + 0.00000000 = 0.34596514 loss_val = 0.34596514\n",
      "epoch: 12000/40000 loss_train: 0.26957070 + 0.00000000 = 0.26957070 loss_val = 0.26957070\n",
      "epoch: 15000/40000 loss_train: 0.22063925 + 0.00000000 = 0.22063925 loss_val = 0.22063925\n",
      "epoch: 18000/40000 loss_train: 0.18757558 + 0.00000000 = 0.18757558 loss_val = 0.18757558\n",
      "epoch: 21000/40000 loss_train: 0.16366932 + 0.00000000 = 0.16366932 loss_val = 0.16366932\n",
      "epoch: 24000/40000 loss_train: 0.14536225 + 0.00000000 = 0.14536225 loss_val = 0.14536225\n",
      "epoch: 27000/40000 loss_train: 0.13144441 + 0.00000000 = 0.13144441 loss_val = 0.13144441\n",
      "epoch: 30000/40000 loss_train: 0.12077203 + 0.00000000 = 0.12077203 loss_val = 0.12077203\n",
      "epoch: 33000/40000 loss_train: 0.11224372 + 0.00000000 = 0.11224372 loss_val = 0.11224372\n",
      "epoch: 36000/40000 loss_train: 0.10512536 + 0.00000000 = 0.10512536 loss_val = 0.10512536\n",
      "epoch: 39000/40000 loss_train: 0.09920885 + 0.00000000 = 0.09920885 loss_val = 0.09920885\n",
      "Para 7 neurônios, 1 camadas,  0.001 learning rate, 0 batch size, 0.1 dropout rate, temos: \u001b[1m Acurácia: 97.62% \u001b[0m\n",
      "epoch:    0/40000 loss_train: 1.43047018 + 0.00000000 = 1.43047018 loss_val = 1.43047018\n",
      "epoch: 3000/40000 loss_train: 0.65032773 + 0.00000000 = 0.65032773 loss_val = 0.65032773\n",
      "epoch: 6000/40000 loss_train: 0.52715763 + 0.00000000 = 0.52715763 loss_val = 0.52715763\n",
      "epoch: 9000/40000 loss_train: 0.39275104 + 0.00000000 = 0.39275104 loss_val = 0.39275104\n",
      "epoch: 12000/40000 loss_train: 0.24934221 + 0.00000000 = 0.24934221 loss_val = 0.24934221\n",
      "epoch: 15000/40000 loss_train: 0.15429406 + 0.00000000 = 0.15429406 loss_val = 0.15429406\n",
      "epoch: 18000/40000 loss_train: 0.10877535 + 0.00000000 = 0.10877535 loss_val = 0.10877535\n",
      "epoch: 21000/40000 loss_train: 0.08677551 + 0.00000000 = 0.08677551 loss_val = 0.08677551\n",
      "epoch: 24000/40000 loss_train: 0.07426110 + 0.00000000 = 0.07426110 loss_val = 0.07426110\n",
      "epoch: 27000/40000 loss_train: 0.06594082 + 0.00000000 = 0.06594082 loss_val = 0.06594082\n",
      "epoch: 30000/40000 loss_train: 0.05983042 + 0.00000000 = 0.05983042 loss_val = 0.05983042\n",
      "epoch: 33000/40000 loss_train: 0.05505567 + 0.00000000 = 0.05505567 loss_val = 0.05505567\n",
      "epoch: 36000/40000 loss_train: 0.05117815 + 0.00000000 = 0.05117815 loss_val = 0.05117815\n",
      "epoch: 39000/40000 loss_train: 0.04794280 + 0.00000000 = 0.04794280 loss_val = 0.04794280\n",
      "Para 7 neurônios, 1 camadas,  0.001 learning rate, 4 batch size, 0 dropout rate, temos: \u001b[1m Acurácia: 97.62% \u001b[0m\n",
      "epoch:    0/40000 loss_train: 0.74047480 + 0.00000000 = 0.74047480 loss_val = 0.74047480\n",
      "epoch: 3000/40000 loss_train: 0.57560549 + 0.00000000 = 0.57560549 loss_val = 0.57560549\n",
      "epoch: 6000/40000 loss_train: 0.39702136 + 0.00000000 = 0.39702136 loss_val = 0.39702136\n",
      "epoch: 9000/40000 loss_train: 0.29363664 + 0.00000000 = 0.29363664 loss_val = 0.29363664\n",
      "epoch: 12000/40000 loss_train: 0.23402135 + 0.00000000 = 0.23402135 loss_val = 0.23402135\n",
      "epoch: 15000/40000 loss_train: 0.19477421 + 0.00000000 = 0.19477421 loss_val = 0.19477421\n",
      "epoch: 18000/40000 loss_train: 0.16851504 + 0.00000000 = 0.16851504 loss_val = 0.16851504\n",
      "epoch: 21000/40000 loss_train: 0.14940637 + 0.00000000 = 0.14940637 loss_val = 0.14940637\n",
      "epoch: 24000/40000 loss_train: 0.13553660 + 0.00000000 = 0.13553660 loss_val = 0.13553660\n",
      "epoch: 27000/40000 loss_train: 0.12427468 + 0.00000000 = 0.12427468 loss_val = 0.12427468\n",
      "epoch: 30000/40000 loss_train: 0.11456683 + 0.00000000 = 0.11456683 loss_val = 0.11456683\n",
      "epoch: 33000/40000 loss_train: 0.10690417 + 0.00000000 = 0.10690417 loss_val = 0.10690417\n",
      "epoch: 36000/40000 loss_train: 0.10024684 + 0.00000000 = 0.10024684 loss_val = 0.10024684\n",
      "epoch: 39000/40000 loss_train: 0.09471754 + 0.00000000 = 0.09471754 loss_val = 0.09471754\n",
      "Para 7 neurônios, 1 camadas,  0.001 learning rate, 4 batch size, 0.1 dropout rate, temos: \u001b[1m Acurácia: 97.62% \u001b[0m\n",
      "epoch:    0/40000 loss_train: 0.85939000 + 0.00000000 = 0.85939000 loss_val = 0.85939000\n",
      "epoch: 3000/40000 loss_train: 0.64044909 + 0.00000000 = 0.64044909 loss_val = 0.64044909\n",
      "epoch: 6000/40000 loss_train: 0.57442356 + 0.00000000 = 0.57442356 loss_val = 0.57442356\n",
      "epoch: 9000/40000 loss_train: 0.48807135 + 0.00000000 = 0.48807135 loss_val = 0.48807135\n",
      "epoch: 12000/40000 loss_train: 0.38339893 + 0.00000000 = 0.38339893 loss_val = 0.38339893\n",
      "epoch: 15000/40000 loss_train: 0.29936975 + 0.00000000 = 0.29936975 loss_val = 0.29936975\n",
      "epoch: 18000/40000 loss_train: 0.25170995 + 0.00000000 = 0.25170995 loss_val = 0.25170995\n",
      "epoch: 21000/40000 loss_train: 0.22702518 + 0.00000000 = 0.22702518 loss_val = 0.22702518\n",
      "epoch: 24000/40000 loss_train: 0.20968770 + 0.00000000 = 0.20968770 loss_val = 0.20968770\n",
      "epoch: 27000/40000 loss_train: 0.19059927 + 0.00000000 = 0.19059927 loss_val = 0.19059927\n",
      "epoch: 30000/40000 loss_train: 0.17005528 + 0.00000000 = 0.17005528 loss_val = 0.17005528\n",
      "epoch: 33000/40000 loss_train: 0.14315346 + 0.00000000 = 0.14315346 loss_val = 0.14315346\n",
      "epoch: 36000/40000 loss_train: 0.10635448 + 0.00000000 = 0.10635448 loss_val = 0.10635448\n",
      "epoch: 39000/40000 loss_train: 0.08100893 + 0.00000000 = 0.08100893 loss_val = 0.08100893\n",
      "Para 7 neurônios, 1 camadas,  0.001 learning rate, 8 batch size, 0 dropout rate, temos: \u001b[1m Acurácia: 97.62% \u001b[0m\n",
      "epoch:    0/40000 loss_train: 0.73157038 + 0.00000000 = 0.73157038 loss_val = 0.73157038\n",
      "epoch: 3000/40000 loss_train: 0.45577827 + 0.00000000 = 0.45577827 loss_val = 0.45577827\n",
      "epoch: 6000/40000 loss_train: 0.31632488 + 0.00000000 = 0.31632488 loss_val = 0.31632488\n",
      "epoch: 9000/40000 loss_train: 0.24290689 + 0.00000000 = 0.24290689 loss_val = 0.24290689\n",
      "epoch: 12000/40000 loss_train: 0.19844773 + 0.00000000 = 0.19844773 loss_val = 0.19844773\n",
      "epoch: 15000/40000 loss_train: 0.16771806 + 0.00000000 = 0.16771806 loss_val = 0.16771806\n",
      "epoch: 18000/40000 loss_train: 0.14313846 + 0.00000000 = 0.14313846 loss_val = 0.14313846\n",
      "epoch: 21000/40000 loss_train: 0.12406245 + 0.00000000 = 0.12406245 loss_val = 0.12406245\n",
      "epoch: 24000/40000 loss_train: 0.10944258 + 0.00000000 = 0.10944258 loss_val = 0.10944258\n",
      "epoch: 27000/40000 loss_train: 0.09801876 + 0.00000000 = 0.09801876 loss_val = 0.09801876\n",
      "epoch: 30000/40000 loss_train: 0.08862941 + 0.00000000 = 0.08862941 loss_val = 0.08862941\n",
      "epoch: 33000/40000 loss_train: 0.08060114 + 0.00000000 = 0.08060114 loss_val = 0.08060114\n",
      "epoch: 36000/40000 loss_train: 0.07371151 + 0.00000000 = 0.07371151 loss_val = 0.07371151\n",
      "epoch: 39000/40000 loss_train: 0.06821169 + 0.00000000 = 0.06821169 loss_val = 0.06821169\n",
      "Para 7 neurônios, 1 camadas,  0.001 learning rate, 8 batch size, 0.1 dropout rate, temos: \u001b[1m Acurácia: 100.00% \u001b[0m\n",
      "epoch:    0/40000 loss_train: 0.77819844 + 0.00000000 = 0.77819844 loss_val = 0.77819844\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3000/40000 loss_train: 0.17592744 + 0.00000000 = 0.17592744 loss_val = 0.17592744\n",
      "epoch: 6000/40000 loss_train: 0.09042227 + 0.00000000 = 0.09042227 loss_val = 0.09042227\n",
      "epoch: 9000/40000 loss_train: 0.07387703 + 0.00000000 = 0.07387703 loss_val = 0.07387703\n",
      "epoch: 12000/40000 loss_train: 0.06518340 + 0.00000000 = 0.06518340 loss_val = 0.06518340\n",
      "epoch: 15000/40000 loss_train: 0.05976690 + 0.00000000 = 0.05976690 loss_val = 0.05976690\n",
      "epoch: 18000/40000 loss_train: 0.05624571 + 0.00000000 = 0.05624571 loss_val = 0.05624571\n",
      "epoch: 21000/40000 loss_train: 0.05325584 + 0.00000000 = 0.05325584 loss_val = 0.05325584\n",
      "epoch: 24000/40000 loss_train: 0.04945497 + 0.00000000 = 0.04945497 loss_val = 0.04945497\n",
      "epoch: 27000/40000 loss_train: 0.04650064 + 0.00000000 = 0.04650064 loss_val = 0.04650064\n",
      "epoch: 30000/40000 loss_train: 0.04388610 + 0.00000000 = 0.04388610 loss_val = 0.04388610\n",
      "epoch: 33000/40000 loss_train: 0.04090960 + 0.00000000 = 0.04090960 loss_val = 0.04090960\n",
      "epoch: 36000/40000 loss_train: 0.03536244 + 0.00000000 = 0.03536244 loss_val = 0.03536244\n",
      "epoch: 39000/40000 loss_train: 0.03136009 + 0.00000000 = 0.03136009 loss_val = 0.03136009\n",
      "Para 7 neurônios, 1 camadas,  0.005 learning rate, 0 batch size, 0 dropout rate, temos: \u001b[1m Acurácia: 100.00% \u001b[0m\n",
      "epoch:    0/40000 loss_train: 1.17044176 + 0.00000000 = 1.17044176 loss_val = 1.17044176\n",
      "epoch: 3000/40000 loss_train: 0.30377502 + 0.00000000 = 0.30377502 loss_val = 0.30377502\n",
      "epoch: 6000/40000 loss_train: 0.09951686 + 0.00000000 = 0.09951686 loss_val = 0.09951686\n",
      "epoch: 9000/40000 loss_train: 0.06512817 + 0.00000000 = 0.06512817 loss_val = 0.06512817\n",
      "epoch: 12000/40000 loss_train: 0.05133618 + 0.00000000 = 0.05133618 loss_val = 0.05133618\n",
      "epoch: 15000/40000 loss_train: 0.04231761 + 0.00000000 = 0.04231761 loss_val = 0.04231761\n",
      "epoch: 18000/40000 loss_train: 0.03575836 + 0.00000000 = 0.03575836 loss_val = 0.03575836\n",
      "epoch: 21000/40000 loss_train: 0.03108949 + 0.00000000 = 0.03108949 loss_val = 0.03108949\n",
      "epoch: 24000/40000 loss_train: 0.02739971 + 0.00000000 = 0.02739971 loss_val = 0.02739971\n",
      "epoch: 27000/40000 loss_train: 0.02387426 + 0.00000000 = 0.02387426 loss_val = 0.02387426\n",
      "epoch: 30000/40000 loss_train: 0.02084850 + 0.00000000 = 0.02084850 loss_val = 0.02084850\n",
      "epoch: 33000/40000 loss_train: 0.01817666 + 0.00000000 = 0.01817666 loss_val = 0.01817666\n",
      "epoch: 36000/40000 loss_train: 0.01552233 + 0.00000000 = 0.01552233 loss_val = 0.01552233\n",
      "epoch: 39000/40000 loss_train: 0.01329953 + 0.00000000 = 0.01329953 loss_val = 0.01329953\n",
      "Para 7 neurônios, 1 camadas,  0.005 learning rate, 0 batch size, 0.1 dropout rate, temos: \u001b[1m Acurácia: 100.00% \u001b[0m\n",
      "epoch:    0/40000 loss_train: 1.11038691 + 0.00000000 = 1.11038691 loss_val = 1.11038691\n",
      "epoch: 3000/40000 loss_train: 0.51507541 + 0.00000000 = 0.51507541 loss_val = 0.51507541\n",
      "epoch: 6000/40000 loss_train: 0.11430433 + 0.00000000 = 0.11430433 loss_val = 0.11430433\n",
      "epoch: 9000/40000 loss_train: 0.08283286 + 0.00000000 = 0.08283286 loss_val = 0.08283286\n",
      "epoch: 12000/40000 loss_train: 0.07314194 + 0.00000000 = 0.07314194 loss_val = 0.07314194\n",
      "epoch: 15000/40000 loss_train: 0.06724546 + 0.00000000 = 0.06724546 loss_val = 0.06724546\n",
      "epoch: 18000/40000 loss_train: 0.06222102 + 0.00000000 = 0.06222102 loss_val = 0.06222102\n",
      "epoch: 21000/40000 loss_train: 0.05873572 + 0.00000000 = 0.05873572 loss_val = 0.05873572\n",
      "epoch: 24000/40000 loss_train: 0.05609988 + 0.00000000 = 0.05609988 loss_val = 0.05609988\n",
      "epoch: 27000/40000 loss_train: 0.05396609 + 0.00000000 = 0.05396609 loss_val = 0.05396609\n",
      "epoch: 30000/40000 loss_train: 0.05208398 + 0.00000000 = 0.05208398 loss_val = 0.05208398\n",
      "epoch: 33000/40000 loss_train: 0.05027214 + 0.00000000 = 0.05027214 loss_val = 0.05027214\n",
      "epoch: 36000/40000 loss_train: 0.04845072 + 0.00000000 = 0.04845072 loss_val = 0.04845072\n",
      "epoch: 39000/40000 loss_train: 0.04657729 + 0.00000000 = 0.04657729 loss_val = 0.04657729\n",
      "Para 7 neurônios, 1 camadas,  0.005 learning rate, 4 batch size, 0 dropout rate, temos: \u001b[1m Acurácia: 95.24% \u001b[0m\n",
      "epoch:    0/40000 loss_train: 0.74501734 + 0.00000000 = 0.74501734 loss_val = 0.74501734\n",
      "epoch: 3000/40000 loss_train: 0.24520063 + 0.00000000 = 0.24520063 loss_val = 0.24520063\n",
      "epoch: 6000/40000 loss_train: 0.19095258 + 0.00000000 = 0.19095258 loss_val = 0.19095258\n",
      "epoch: 9000/40000 loss_train: 0.10420797 + 0.00000000 = 0.10420797 loss_val = 0.10420797\n",
      "epoch: 12000/40000 loss_train: 0.07075112 + 0.00000000 = 0.07075112 loss_val = 0.07075112\n",
      "epoch: 15000/40000 loss_train: 0.05752982 + 0.00000000 = 0.05752982 loss_val = 0.05752982\n",
      "Para 7 neurônios, 1 camadas,  0.005 learning rate, 4 batch size, 0.1 dropout rate, temos: \u001b[1m Acurácia: 97.62% \u001b[0m\n",
      "epoch:    0/40000 loss_train: 2.52415824 + 0.00000000 = 2.52415824 loss_val = 2.52415824\n",
      "epoch: 3000/40000 loss_train: 0.11658487 + 0.00000000 = 0.11658487 loss_val = 0.11658487\n",
      "epoch: 6000/40000 loss_train: 0.09037963 + 0.00000000 = 0.09037963 loss_val = 0.09037963\n",
      "epoch: 9000/40000 loss_train: 0.08254562 + 0.00000000 = 0.08254562 loss_val = 0.08254562\n",
      "epoch: 12000/40000 loss_train: 0.07688764 + 0.00000000 = 0.07688764 loss_val = 0.07688764\n",
      "epoch: 15000/40000 loss_train: 0.07338459 + 0.00000000 = 0.07338459 loss_val = 0.07338459\n",
      "epoch: 18000/40000 loss_train: 0.06877968 + 0.00000000 = 0.06877968 loss_val = 0.06877968\n",
      "epoch: 21000/40000 loss_train: 0.06345359 + 0.00000000 = 0.06345359 loss_val = 0.06345359\n",
      "epoch: 24000/40000 loss_train: 0.05806395 + 0.00000000 = 0.05806395 loss_val = 0.05806395\n",
      "epoch: 27000/40000 loss_train: 0.05365071 + 0.00000000 = 0.05365071 loss_val = 0.05365071\n",
      "epoch: 30000/40000 loss_train: 0.04946896 + 0.00000000 = 0.04946896 loss_val = 0.04946896\n",
      "epoch: 33000/40000 loss_train: 0.04540509 + 0.00000000 = 0.04540509 loss_val = 0.04540509\n",
      "epoch: 36000/40000 loss_train: 0.04099789 + 0.00000000 = 0.04099789 loss_val = 0.04099789\n",
      "epoch: 39000/40000 loss_train: 0.03606287 + 0.00000000 = 0.03606287 loss_val = 0.03606287\n",
      "Para 7 neurônios, 1 camadas,  0.005 learning rate, 8 batch size, 0 dropout rate, temos: \u001b[1m Acurácia: 100.00% \u001b[0m\n",
      "epoch:    0/40000 loss_train: 0.95944327 + 0.00000000 = 0.95944327 loss_val = 0.95944327\n",
      "epoch: 3000/40000 loss_train: 0.23453824 + 0.00000000 = 0.23453824 loss_val = 0.23453824\n",
      "epoch: 6000/40000 loss_train: 0.11641019 + 0.00000000 = 0.11641019 loss_val = 0.11641019\n",
      "epoch: 9000/40000 loss_train: 0.09101348 + 0.00000000 = 0.09101348 loss_val = 0.09101348\n",
      "epoch: 12000/40000 loss_train: 0.07742298 + 0.00000000 = 0.07742298 loss_val = 0.07742298\n",
      "Para 7 neurônios, 1 camadas,  0.005 learning rate, 8 batch size, 0.1 dropout rate, temos: \u001b[1m Acurácia: 97.62% \u001b[0m\n",
      "epoch:    0/40000 loss_train: 0.85196264 + 0.00000000 = 0.85196264 loss_val = 0.85196264\n",
      "epoch: 3000/40000 loss_train: 0.04905423 + 0.00000000 = 0.04905423 loss_val = 0.04905423\n",
      "epoch: 6000/40000 loss_train: 0.02777902 + 0.00000000 = 0.02777902 loss_val = 0.02777902\n",
      "epoch: 9000/40000 loss_train: 0.01905867 + 0.00000000 = 0.01905867 loss_val = 0.01905867\n",
      "epoch: 12000/40000 loss_train: 0.01375284 + 0.00000000 = 0.01375284 loss_val = 0.01375284\n",
      "epoch: 15000/40000 loss_train: 0.01032847 + 0.00000000 = 0.01032847 loss_val = 0.01032847\n",
      "epoch: 18000/40000 loss_train: 0.00798296 + 0.00000000 = 0.00798296 loss_val = 0.00798296\n",
      "epoch: 21000/40000 loss_train: 0.00633241 + 0.00000000 = 0.00633241 loss_val = 0.00633241\n",
      "epoch: 24000/40000 loss_train: 0.00513859 + 0.00000000 = 0.00513859 loss_val = 0.00513859\n",
      "epoch: 27000/40000 loss_train: 0.00425199 + 0.00000000 = 0.00425199 loss_val = 0.00425199\n",
      "epoch: 30000/40000 loss_train: 0.00357823 + 0.00000000 = 0.00357823 loss_val = 0.00357823\n",
      "epoch: 33000/40000 loss_train: 0.00305708 + 0.00000000 = 0.00305708 loss_val = 0.00305708\n",
      "epoch: 36000/40000 loss_train: 0.00264607 + 0.00000000 = 0.00264607 loss_val = 0.00264607\n",
      "epoch: 39000/40000 loss_train: 0.00231592 + 0.00000000 = 0.00231592 loss_val = 0.00231592\n",
      "Para 7 neurônios, 1 camadas,  0.01 learning rate, 0 batch size, 0 dropout rate, temos: \u001b[1m Acurácia: 97.62% \u001b[0m\n",
      "epoch:    0/40000 loss_train: 0.68772594 + 0.00000000 = 0.68772594 loss_val = 0.68772594\n",
      "epoch: 3000/40000 loss_train: 0.08777228 + 0.00000000 = 0.08777228 loss_val = 0.08777228\n",
      "epoch: 6000/40000 loss_train: 0.03398610 + 0.00000000 = 0.03398610 loss_val = 0.03398610\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Para 7 neurônios, 1 camadas,  0.01 learning rate, 0 batch size, 0.1 dropout rate, temos: \u001b[1m Acurácia: 100.00% \u001b[0m\n",
      "epoch:    0/40000 loss_train: 0.88019539 + 0.00000000 = 0.88019539 loss_val = 0.88019539\n",
      "epoch: 3000/40000 loss_train: 0.06880689 + 0.00000000 = 0.06880689 loss_val = 0.06880689\n",
      "epoch: 6000/40000 loss_train: 0.03859362 + 0.00000000 = 0.03859362 loss_val = 0.03859362\n",
      "epoch: 9000/40000 loss_train: 0.02500158 + 0.00000000 = 0.02500158 loss_val = 0.02500158\n",
      "epoch: 12000/40000 loss_train: 0.01678448 + 0.00000000 = 0.01678448 loss_val = 0.01678448\n",
      "epoch: 15000/40000 loss_train: 0.01178272 + 0.00000000 = 0.01178272 loss_val = 0.01178272\n",
      "epoch: 18000/40000 loss_train: 0.00868066 + 0.00000000 = 0.00868066 loss_val = 0.00868066\n",
      "epoch: 21000/40000 loss_train: 0.00666388 + 0.00000000 = 0.00666388 loss_val = 0.00666388\n",
      "epoch: 24000/40000 loss_train: 0.00529181 + 0.00000000 = 0.00529181 loss_val = 0.00529181\n",
      "epoch: 27000/40000 loss_train: 0.00431985 + 0.00000000 = 0.00431985 loss_val = 0.00431985\n",
      "epoch: 30000/40000 loss_train: 0.00360751 + 0.00000000 = 0.00360751 loss_val = 0.00360751\n",
      "epoch: 33000/40000 loss_train: 0.00306889 + 0.00000000 = 0.00306889 loss_val = 0.00306889\n",
      "epoch: 36000/40000 loss_train: 0.00265150 + 0.00000000 = 0.00265150 loss_val = 0.00265150\n",
      "epoch: 39000/40000 loss_train: 0.00232103 + 0.00000000 = 0.00232103 loss_val = 0.00232103\n",
      "Para 7 neurônios, 1 camadas,  0.01 learning rate, 4 batch size, 0 dropout rate, temos: \u001b[1m Acurácia: 100.00% \u001b[0m\n",
      "epoch:    0/40000 loss_train: 0.72716430 + 0.00000000 = 0.72716430 loss_val = 0.72716430\n",
      "epoch: 3000/40000 loss_train: 0.09014684 + 0.00000000 = 0.09014684 loss_val = 0.09014684\n",
      "Para 7 neurônios, 1 camadas,  0.01 learning rate, 4 batch size, 0.1 dropout rate, temos: \u001b[1m Acurácia: 95.24% \u001b[0m\n",
      "epoch:    0/40000 loss_train: 1.00568310 + 0.00000000 = 1.00568310 loss_val = 1.00568310\n",
      "epoch: 3000/40000 loss_train: 0.06816561 + 0.00000000 = 0.06816561 loss_val = 0.06816561\n",
      "epoch: 6000/40000 loss_train: 0.03636253 + 0.00000000 = 0.03636253 loss_val = 0.03636253\n",
      "epoch: 9000/40000 loss_train: 0.02496057 + 0.00000000 = 0.02496057 loss_val = 0.02496057\n",
      "epoch: 12000/40000 loss_train: 0.01773845 + 0.00000000 = 0.01773845 loss_val = 0.01773845\n",
      "epoch: 15000/40000 loss_train: 0.01283454 + 0.00000000 = 0.01283454 loss_val = 0.01283454\n",
      "epoch: 18000/40000 loss_train: 0.00950132 + 0.00000000 = 0.00950132 loss_val = 0.00950132\n",
      "epoch: 21000/40000 loss_train: 0.00722808 + 0.00000000 = 0.00722808 loss_val = 0.00722808\n",
      "epoch: 24000/40000 loss_train: 0.00564878 + 0.00000000 = 0.00564878 loss_val = 0.00564878\n",
      "epoch: 27000/40000 loss_train: 0.00453042 + 0.00000000 = 0.00453042 loss_val = 0.00453042\n",
      "epoch: 30000/40000 loss_train: 0.00371736 + 0.00000000 = 0.00371736 loss_val = 0.00371736\n",
      "epoch: 33000/40000 loss_train: 0.00311072 + 0.00000000 = 0.00311072 loss_val = 0.00311072\n",
      "epoch: 36000/40000 loss_train: 0.00264769 + 0.00000000 = 0.00264769 loss_val = 0.00264769\n",
      "epoch: 39000/40000 loss_train: 0.00228650 + 0.00000000 = 0.00228650 loss_val = 0.00228650\n",
      "Para 7 neurônios, 1 camadas,  0.01 learning rate, 8 batch size, 0 dropout rate, temos: \u001b[1m Acurácia: 100.00% \u001b[0m\n",
      "epoch:    0/40000 loss_train: 1.53067819 + 0.00000000 = 1.53067819 loss_val = 1.53067819\n",
      "epoch: 3000/40000 loss_train: 0.08814780 + 0.00000000 = 0.08814780 loss_val = 0.08814780\n",
      "epoch: 6000/40000 loss_train: 0.05544255 + 0.00000000 = 0.05544255 loss_val = 0.05544255\n",
      "Para 7 neurônios, 1 camadas,  0.01 learning rate, 8 batch size, 0.1 dropout rate, temos: \u001b[1m Acurácia: 97.62% \u001b[0m\n",
      "epoch:    0/40000 loss_train: 1.10144604 + 0.00000000 = 1.10144604 loss_val = 1.10144604\n",
      "epoch: 3000/40000 loss_train: 0.61566153 + 0.00000000 = 0.61566153 loss_val = 0.61566153\n",
      "epoch: 6000/40000 loss_train: 0.42336512 + 0.00000000 = 0.42336512 loss_val = 0.42336512\n",
      "epoch: 9000/40000 loss_train: 0.31310696 + 0.00000000 = 0.31310696 loss_val = 0.31310696\n",
      "epoch: 12000/40000 loss_train: 0.25048545 + 0.00000000 = 0.25048545 loss_val = 0.25048545\n",
      "epoch: 15000/40000 loss_train: 0.22015405 + 0.00000000 = 0.22015405 loss_val = 0.22015405\n",
      "epoch: 18000/40000 loss_train: 0.19862745 + 0.00000000 = 0.19862745 loss_val = 0.19862745\n",
      "epoch: 21000/40000 loss_train: 0.17873377 + 0.00000000 = 0.17873377 loss_val = 0.17873377\n",
      "epoch: 24000/40000 loss_train: 0.15919710 + 0.00000000 = 0.15919710 loss_val = 0.15919710\n",
      "epoch: 27000/40000 loss_train: 0.14068039 + 0.00000000 = 0.14068039 loss_val = 0.14068039\n",
      "epoch: 30000/40000 loss_train: 0.12418022 + 0.00000000 = 0.12418022 loss_val = 0.12418022\n",
      "epoch: 33000/40000 loss_train: 0.11111211 + 0.00000000 = 0.11111211 loss_val = 0.11111211\n",
      "epoch: 36000/40000 loss_train: 0.09904089 + 0.00000000 = 0.09904089 loss_val = 0.09904089\n",
      "epoch: 39000/40000 loss_train: 0.08705481 + 0.00000000 = 0.08705481 loss_val = 0.08705481\n",
      "Para 8 neurônios, 1 camadas,  0.001 learning rate, 0 batch size, 0 dropout rate, temos: \u001b[1m Acurácia: 95.24% \u001b[0m\n",
      "epoch:    0/40000 loss_train: 1.13485226 + 0.00000000 = 1.13485226 loss_val = 1.13485226\n",
      "epoch: 3000/40000 loss_train: 0.47749881 + 0.00000000 = 0.47749881 loss_val = 0.47749881\n",
      "epoch: 6000/40000 loss_train: 0.34205898 + 0.00000000 = 0.34205898 loss_val = 0.34205898\n",
      "epoch: 9000/40000 loss_train: 0.25737967 + 0.00000000 = 0.25737967 loss_val = 0.25737967\n",
      "epoch: 12000/40000 loss_train: 0.20409357 + 0.00000000 = 0.20409357 loss_val = 0.20409357\n",
      "epoch: 15000/40000 loss_train: 0.16855572 + 0.00000000 = 0.16855572 loss_val = 0.16855572\n",
      "epoch: 18000/40000 loss_train: 0.14229129 + 0.00000000 = 0.14229129 loss_val = 0.14229129\n",
      "epoch: 21000/40000 loss_train: 0.12317849 + 0.00000000 = 0.12317849 loss_val = 0.12317849\n",
      "epoch: 24000/40000 loss_train: 0.10897895 + 0.00000000 = 0.10897895 loss_val = 0.10897895\n",
      "epoch: 27000/40000 loss_train: 0.09854998 + 0.00000000 = 0.09854998 loss_val = 0.09854998\n",
      "epoch: 30000/40000 loss_train: 0.09061884 + 0.00000000 = 0.09061884 loss_val = 0.09061884\n",
      "epoch: 33000/40000 loss_train: 0.08463244 + 0.00000000 = 0.08463244 loss_val = 0.08463244\n",
      "Para 8 neurônios, 1 camadas,  0.001 learning rate, 0 batch size, 0.1 dropout rate, temos: \u001b[1m Acurácia: 92.86% \u001b[0m\n",
      "epoch:    0/40000 loss_train: 0.77503232 + 0.00000000 = 0.77503232 loss_val = 0.77503232\n",
      "epoch: 3000/40000 loss_train: 0.48124475 + 0.00000000 = 0.48124475 loss_val = 0.48124475\n",
      "epoch: 6000/40000 loss_train: 0.32151307 + 0.00000000 = 0.32151307 loss_val = 0.32151307\n",
      "epoch: 9000/40000 loss_train: 0.22245185 + 0.00000000 = 0.22245185 loss_val = 0.22245185\n",
      "epoch: 12000/40000 loss_train: 0.15924058 + 0.00000000 = 0.15924058 loss_val = 0.15924058\n",
      "epoch: 15000/40000 loss_train: 0.12292423 + 0.00000000 = 0.12292423 loss_val = 0.12292423\n",
      "epoch: 18000/40000 loss_train: 0.10127635 + 0.00000000 = 0.10127635 loss_val = 0.10127635\n",
      "epoch: 21000/40000 loss_train: 0.08674077 + 0.00000000 = 0.08674077 loss_val = 0.08674077\n",
      "epoch: 24000/40000 loss_train: 0.07683934 + 0.00000000 = 0.07683934 loss_val = 0.07683934\n",
      "epoch: 27000/40000 loss_train: 0.06967004 + 0.00000000 = 0.06967004 loss_val = 0.06967004\n",
      "epoch: 30000/40000 loss_train: 0.06412298 + 0.00000000 = 0.06412298 loss_val = 0.06412298\n",
      "epoch: 33000/40000 loss_train: 0.05971713 + 0.00000000 = 0.05971713 loss_val = 0.05971713\n",
      "epoch: 36000/40000 loss_train: 0.05612409 + 0.00000000 = 0.05612409 loss_val = 0.05612409\n",
      "epoch: 39000/40000 loss_train: 0.05308595 + 0.00000000 = 0.05308595 loss_val = 0.05308595\n",
      "Para 8 neurônios, 1 camadas,  0.001 learning rate, 4 batch size, 0 dropout rate, temos: \u001b[1m Acurácia: 97.62% \u001b[0m\n",
      "epoch:    0/40000 loss_train: 1.05121869 + 0.00000000 = 1.05121869 loss_val = 1.05121869\n",
      "epoch: 3000/40000 loss_train: 0.51932081 + 0.00000000 = 0.51932081 loss_val = 0.51932081\n",
      "epoch: 6000/40000 loss_train: 0.35412571 + 0.00000000 = 0.35412571 loss_val = 0.35412571\n",
      "epoch: 9000/40000 loss_train: 0.25306473 + 0.00000000 = 0.25306473 loss_val = 0.25306473\n",
      "epoch: 12000/40000 loss_train: 0.19069906 + 0.00000000 = 0.19069906 loss_val = 0.19069906\n",
      "epoch: 15000/40000 loss_train: 0.14970450 + 0.00000000 = 0.14970450 loss_val = 0.14970450\n",
      "epoch: 18000/40000 loss_train: 0.12274664 + 0.00000000 = 0.12274664 loss_val = 0.12274664\n",
      "epoch: 21000/40000 loss_train: 0.10322759 + 0.00000000 = 0.10322759 loss_val = 0.10322759\n",
      "epoch: 24000/40000 loss_train: 0.08975053 + 0.00000000 = 0.08975053 loss_val = 0.08975053\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 27000/40000 loss_train: 0.07941812 + 0.00000000 = 0.07941812 loss_val = 0.07941812\n",
      "epoch: 30000/40000 loss_train: 0.07189178 + 0.00000000 = 0.07189178 loss_val = 0.07189178\n",
      "epoch: 33000/40000 loss_train: 0.06623296 + 0.00000000 = 0.06623296 loss_val = 0.06623296\n",
      "epoch: 36000/40000 loss_train: 0.06099610 + 0.00000000 = 0.06099610 loss_val = 0.06099610\n",
      "epoch: 39000/40000 loss_train: 0.05650743 + 0.00000000 = 0.05650743 loss_val = 0.05650743\n",
      "Para 8 neurônios, 1 camadas,  0.001 learning rate, 4 batch size, 0.1 dropout rate, temos: \u001b[1m Acurácia: 97.62% \u001b[0m\n",
      "epoch:    0/40000 loss_train: 1.68995977 + 0.00000000 = 1.68995977 loss_val = 1.68995977\n",
      "epoch: 3000/40000 loss_train: 0.67155373 + 0.00000000 = 0.67155373 loss_val = 0.67155373\n",
      "epoch: 6000/40000 loss_train: 0.60119635 + 0.00000000 = 0.60119635 loss_val = 0.60119635\n",
      "epoch: 9000/40000 loss_train: 0.47645523 + 0.00000000 = 0.47645523 loss_val = 0.47645523\n",
      "epoch: 12000/40000 loss_train: 0.33178965 + 0.00000000 = 0.33178965 loss_val = 0.33178965\n",
      "epoch: 15000/40000 loss_train: 0.24028762 + 0.00000000 = 0.24028762 loss_val = 0.24028762\n",
      "epoch: 18000/40000 loss_train: 0.16748146 + 0.00000000 = 0.16748146 loss_val = 0.16748146\n",
      "epoch: 21000/40000 loss_train: 0.13285812 + 0.00000000 = 0.13285812 loss_val = 0.13285812\n",
      "epoch: 24000/40000 loss_train: 0.11513736 + 0.00000000 = 0.11513736 loss_val = 0.11513736\n",
      "epoch: 27000/40000 loss_train: 0.10518055 + 0.00000000 = 0.10518055 loss_val = 0.10518055\n",
      "epoch: 30000/40000 loss_train: 0.09888545 + 0.00000000 = 0.09888545 loss_val = 0.09888545\n",
      "epoch: 33000/40000 loss_train: 0.09430940 + 0.00000000 = 0.09430940 loss_val = 0.09430940\n",
      "epoch: 36000/40000 loss_train: 0.09073954 + 0.00000000 = 0.09073954 loss_val = 0.09073954\n",
      "epoch: 39000/40000 loss_train: 0.08832368 + 0.00000000 = 0.08832368 loss_val = 0.08832368\n",
      "Para 8 neurônios, 1 camadas,  0.001 learning rate, 8 batch size, 0 dropout rate, temos: \u001b[1m Acurácia: 97.62% \u001b[0m\n",
      "epoch:    0/40000 loss_train: 1.16945421 + 0.00000000 = 1.16945421 loss_val = 1.16945421\n",
      "epoch: 3000/40000 loss_train: 0.43154143 + 0.00000000 = 0.43154143 loss_val = 0.43154143\n",
      "epoch: 6000/40000 loss_train: 0.26186066 + 0.00000000 = 0.26186066 loss_val = 0.26186066\n",
      "epoch: 9000/40000 loss_train: 0.17806371 + 0.00000000 = 0.17806371 loss_val = 0.17806371\n",
      "epoch: 12000/40000 loss_train: 0.13713469 + 0.00000000 = 0.13713469 loss_val = 0.13713469\n",
      "epoch: 15000/40000 loss_train: 0.11641711 + 0.00000000 = 0.11641711 loss_val = 0.11641711\n",
      "epoch: 18000/40000 loss_train: 0.10426609 + 0.00000000 = 0.10426609 loss_val = 0.10426609\n",
      "epoch: 21000/40000 loss_train: 0.09641193 + 0.00000000 = 0.09641193 loss_val = 0.09641193\n",
      "epoch: 24000/40000 loss_train: 0.09085048 + 0.00000000 = 0.09085048 loss_val = 0.09085048\n",
      "epoch: 27000/40000 loss_train: 0.08655796 + 0.00000000 = 0.08655796 loss_val = 0.08655796\n",
      "epoch: 30000/40000 loss_train: 0.08318754 + 0.00000000 = 0.08318754 loss_val = 0.08318754\n",
      "epoch: 33000/40000 loss_train: 0.08049129 + 0.00000000 = 0.08049129 loss_val = 0.08049129\n",
      "epoch: 36000/40000 loss_train: 0.07816009 + 0.00000000 = 0.07816009 loss_val = 0.07816009\n",
      "epoch: 39000/40000 loss_train: 0.07620175 + 0.00000000 = 0.07620175 loss_val = 0.07620175\n",
      "Para 8 neurônios, 1 camadas,  0.001 learning rate, 8 batch size, 0.1 dropout rate, temos: \u001b[1m Acurácia: 92.86% \u001b[0m\n",
      "epoch:    0/40000 loss_train: 0.97576991 + 0.00000000 = 0.97576991 loss_val = 0.97576991\n",
      "epoch: 3000/40000 loss_train: 0.13538661 + 0.00000000 = 0.13538661 loss_val = 0.13538661\n",
      "epoch: 6000/40000 loss_train: 0.09265141 + 0.00000000 = 0.09265141 loss_val = 0.09265141\n",
      "epoch: 9000/40000 loss_train: 0.08290460 + 0.00000000 = 0.08290460 loss_val = 0.08290460\n",
      "epoch: 12000/40000 loss_train: 0.07849720 + 0.00000000 = 0.07849720 loss_val = 0.07849720\n",
      "epoch: 15000/40000 loss_train: 0.07606191 + 0.00000000 = 0.07606191 loss_val = 0.07606191\n",
      "epoch: 18000/40000 loss_train: 0.07440912 + 0.00000000 = 0.07440912 loss_val = 0.07440912\n",
      "epoch: 21000/40000 loss_train: 0.07303079 + 0.00000000 = 0.07303079 loss_val = 0.07303079\n",
      "epoch: 24000/40000 loss_train: 0.07160674 + 0.00000000 = 0.07160674 loss_val = 0.07160674\n",
      "epoch: 27000/40000 loss_train: 0.06997666 + 0.00000000 = 0.06997666 loss_val = 0.06997666\n",
      "epoch: 30000/40000 loss_train: 0.06806871 + 0.00000000 = 0.06806871 loss_val = 0.06806871\n",
      "epoch: 33000/40000 loss_train: 0.06584265 + 0.00000000 = 0.06584265 loss_val = 0.06584265\n",
      "epoch: 36000/40000 loss_train: 0.06320104 + 0.00000000 = 0.06320104 loss_val = 0.06320104\n",
      "epoch: 39000/40000 loss_train: 0.06011470 + 0.00000000 = 0.06011470 loss_val = 0.06011470\n",
      "Para 8 neurônios, 1 camadas,  0.005 learning rate, 0 batch size, 0 dropout rate, temos: \u001b[1m Acurácia: 97.62% \u001b[0m\n",
      "epoch:    0/40000 loss_train: 1.08963661 + 0.00000000 = 1.08963661 loss_val = 1.08963661\n",
      "epoch: 3000/40000 loss_train: 0.25547434 + 0.00000000 = 0.25547434 loss_val = 0.25547434\n",
      "epoch: 6000/40000 loss_train: 0.10197033 + 0.00000000 = 0.10197033 loss_val = 0.10197033\n",
      "epoch: 9000/40000 loss_train: 0.06933461 + 0.00000000 = 0.06933461 loss_val = 0.06933461\n",
      "epoch: 12000/40000 loss_train: 0.05693574 + 0.00000000 = 0.05693574 loss_val = 0.05693574\n",
      "Para 8 neurônios, 1 camadas,  0.005 learning rate, 0 batch size, 0.1 dropout rate, temos: \u001b[1m Acurácia: 97.62% \u001b[0m\n",
      "epoch:    0/40000 loss_train: 0.94739366 + 0.00000000 = 0.94739366 loss_val = 0.94739366\n",
      "epoch: 3000/40000 loss_train: 0.15832442 + 0.00000000 = 0.15832442 loss_val = 0.15832442\n",
      "epoch: 6000/40000 loss_train: 0.07402712 + 0.00000000 = 0.07402712 loss_val = 0.07402712\n",
      "epoch: 9000/40000 loss_train: 0.05513901 + 0.00000000 = 0.05513901 loss_val = 0.05513901\n",
      "epoch: 12000/40000 loss_train: 0.04441331 + 0.00000000 = 0.04441331 loss_val = 0.04441331\n",
      "epoch: 15000/40000 loss_train: 0.03677747 + 0.00000000 = 0.03677747 loss_val = 0.03677747\n",
      "epoch: 18000/40000 loss_train: 0.03050720 + 0.00000000 = 0.03050720 loss_val = 0.03050720\n",
      "epoch: 21000/40000 loss_train: 0.02511831 + 0.00000000 = 0.02511831 loss_val = 0.02511831\n",
      "epoch: 24000/40000 loss_train: 0.02052857 + 0.00000000 = 0.02052857 loss_val = 0.02052857\n",
      "epoch: 27000/40000 loss_train: 0.01673335 + 0.00000000 = 0.01673335 loss_val = 0.01673335\n",
      "epoch: 30000/40000 loss_train: 0.01366857 + 0.00000000 = 0.01366857 loss_val = 0.01366857\n",
      "epoch: 33000/40000 loss_train: 0.01124680 + 0.00000000 = 0.01124680 loss_val = 0.01124680\n",
      "epoch: 36000/40000 loss_train: 0.00935046 + 0.00000000 = 0.00935046 loss_val = 0.00935046\n",
      "epoch: 39000/40000 loss_train: 0.00786529 + 0.00000000 = 0.00786529 loss_val = 0.00786529\n",
      "Para 8 neurônios, 1 camadas,  0.005 learning rate, 4 batch size, 0 dropout rate, temos: \u001b[1m Acurácia: 97.62% \u001b[0m\n",
      "epoch:    0/40000 loss_train: 1.54769012 + 0.00000000 = 1.54769012 loss_val = 1.54769012\n",
      "epoch: 3000/40000 loss_train: 0.14415355 + 0.00000000 = 0.14415355 loss_val = 0.14415355\n",
      "epoch: 6000/40000 loss_train: 0.05984939 + 0.00000000 = 0.05984939 loss_val = 0.05984939\n",
      "epoch: 9000/40000 loss_train: 0.04422565 + 0.00000000 = 0.04422565 loss_val = 0.04422565\n",
      "epoch: 12000/40000 loss_train: 0.03655187 + 0.00000000 = 0.03655187 loss_val = 0.03655187\n",
      "epoch: 15000/40000 loss_train: 0.03116214 + 0.00000000 = 0.03116214 loss_val = 0.03116214\n",
      "epoch: 18000/40000 loss_train: 0.02686782 + 0.00000000 = 0.02686782 loss_val = 0.02686782\n",
      "epoch: 21000/40000 loss_train: 0.02358965 + 0.00000000 = 0.02358965 loss_val = 0.02358965\n",
      "epoch: 24000/40000 loss_train: 0.02062038 + 0.00000000 = 0.02062038 loss_val = 0.02062038\n",
      "epoch: 27000/40000 loss_train: 0.01792079 + 0.00000000 = 0.01792079 loss_val = 0.01792079\n",
      "epoch: 30000/40000 loss_train: 0.01550763 + 0.00000000 = 0.01550763 loss_val = 0.01550763\n",
      "epoch: 33000/40000 loss_train: 0.01337311 + 0.00000000 = 0.01337311 loss_val = 0.01337311\n",
      "epoch: 36000/40000 loss_train: 0.01146600 + 0.00000000 = 0.01146600 loss_val = 0.01146600\n",
      "epoch: 39000/40000 loss_train: 0.00986223 + 0.00000000 = 0.00986223 loss_val = 0.00986223\n",
      "Para 8 neurônios, 1 camadas,  0.005 learning rate, 4 batch size, 0.1 dropout rate, temos: \u001b[1m Acurácia: 97.62% \u001b[0m\n",
      "epoch:    0/40000 loss_train: 0.90659150 + 0.00000000 = 0.90659150 loss_val = 0.90659150\n",
      "epoch: 3000/40000 loss_train: 0.21283455 + 0.00000000 = 0.21283455 loss_val = 0.21283455\n",
      "epoch: 6000/40000 loss_train: 0.06937281 + 0.00000000 = 0.06937281 loss_val = 0.06937281\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 9000/40000 loss_train: 0.03696388 + 0.00000000 = 0.03696388 loss_val = 0.03696388\n",
      "epoch: 12000/40000 loss_train: 0.02574510 + 0.00000000 = 0.02574510 loss_val = 0.02574510\n",
      "epoch: 15000/40000 loss_train: 0.01966042 + 0.00000000 = 0.01966042 loss_val = 0.01966042\n",
      "epoch: 18000/40000 loss_train: 0.01575344 + 0.00000000 = 0.01575344 loss_val = 0.01575344\n",
      "epoch: 21000/40000 loss_train: 0.01300234 + 0.00000000 = 0.01300234 loss_val = 0.01300234\n",
      "epoch: 24000/40000 loss_train: 0.01096001 + 0.00000000 = 0.01096001 loss_val = 0.01096001\n",
      "epoch: 27000/40000 loss_train: 0.00939232 + 0.00000000 = 0.00939232 loss_val = 0.00939232\n",
      "epoch: 30000/40000 loss_train: 0.00817141 + 0.00000000 = 0.00817141 loss_val = 0.00817141\n",
      "epoch: 33000/40000 loss_train: 0.00718140 + 0.00000000 = 0.00718140 loss_val = 0.00718140\n",
      "epoch: 36000/40000 loss_train: 0.00636707 + 0.00000000 = 0.00636707 loss_val = 0.00636707\n",
      "epoch: 39000/40000 loss_train: 0.00568831 + 0.00000000 = 0.00568831 loss_val = 0.00568831\n",
      "Para 8 neurônios, 1 camadas,  0.005 learning rate, 8 batch size, 0 dropout rate, temos: \u001b[1m Acurácia: 97.62% \u001b[0m\n",
      "epoch:    0/40000 loss_train: 0.76126651 + 0.00000000 = 0.76126651 loss_val = 0.76126651\n",
      "epoch: 3000/40000 loss_train: 0.12706520 + 0.00000000 = 0.12706520 loss_val = 0.12706520\n",
      "epoch: 6000/40000 loss_train: 0.09437621 + 0.00000000 = 0.09437621 loss_val = 0.09437621\n",
      "epoch: 9000/40000 loss_train: 0.08545997 + 0.00000000 = 0.08545997 loss_val = 0.08545997\n",
      "Para 8 neurônios, 1 camadas,  0.005 learning rate, 8 batch size, 0.1 dropout rate, temos: \u001b[1m Acurácia: 97.62% \u001b[0m\n",
      "epoch:    0/40000 loss_train: 0.73848729 + 0.00000000 = 0.73848729 loss_val = 0.73848729\n",
      "epoch: 3000/40000 loss_train: 0.13664928 + 0.00000000 = 0.13664928 loss_val = 0.13664928\n",
      "epoch: 6000/40000 loss_train: 0.06736554 + 0.00000000 = 0.06736554 loss_val = 0.06736554\n",
      "epoch: 9000/40000 loss_train: 0.04426060 + 0.00000000 = 0.04426060 loss_val = 0.04426060\n",
      "epoch: 12000/40000 loss_train: 0.03183332 + 0.00000000 = 0.03183332 loss_val = 0.03183332\n",
      "epoch: 15000/40000 loss_train: 0.02402983 + 0.00000000 = 0.02402983 loss_val = 0.02402983\n",
      "epoch: 18000/40000 loss_train: 0.01878547 + 0.00000000 = 0.01878547 loss_val = 0.01878547\n",
      "epoch: 21000/40000 loss_train: 0.01508384 + 0.00000000 = 0.01508384 loss_val = 0.01508384\n",
      "epoch: 24000/40000 loss_train: 0.01238821 + 0.00000000 = 0.01238821 loss_val = 0.01238821\n",
      "epoch: 27000/40000 loss_train: 0.01040351 + 0.00000000 = 0.01040351 loss_val = 0.01040351\n",
      "epoch: 30000/40000 loss_train: 0.00885064 + 0.00000000 = 0.00885064 loss_val = 0.00885064\n",
      "epoch: 33000/40000 loss_train: 0.00762720 + 0.00000000 = 0.00762720 loss_val = 0.00762720\n",
      "epoch: 36000/40000 loss_train: 0.00667452 + 0.00000000 = 0.00667452 loss_val = 0.00667452\n",
      "epoch: 39000/40000 loss_train: 0.00592275 + 0.00000000 = 0.00592275 loss_val = 0.00592275\n",
      "Para 8 neurônios, 1 camadas,  0.01 learning rate, 0 batch size, 0 dropout rate, temos: \u001b[1m Acurácia: 100.00% \u001b[0m\n",
      "epoch:    0/40000 loss_train: 1.11940883 + 0.00000000 = 1.11940883 loss_val = 1.11940883\n",
      "epoch: 3000/40000 loss_train: 0.08744365 + 0.00000000 = 0.08744365 loss_val = 0.08744365\n",
      "epoch: 6000/40000 loss_train: 0.03740543 + 0.00000000 = 0.03740543 loss_val = 0.03740543\n",
      "epoch: 9000/40000 loss_train: 0.02211892 + 0.00000000 = 0.02211892 loss_val = 0.02211892\n",
      "epoch: 12000/40000 loss_train: 0.01488976 + 0.00000000 = 0.01488976 loss_val = 0.01488976\n",
      "epoch: 15000/40000 loss_train: 0.01077689 + 0.00000000 = 0.01077689 loss_val = 0.01077689\n",
      "epoch: 18000/40000 loss_train: 0.00806011 + 0.00000000 = 0.00806011 loss_val = 0.00806011\n",
      "Para 8 neurônios, 1 camadas,  0.01 learning rate, 0 batch size, 0.1 dropout rate, temos: \u001b[1m Acurácia: 97.62% \u001b[0m\n",
      "epoch:    0/40000 loss_train: 0.83733313 + 0.00000000 = 0.83733313 loss_val = 0.83733313\n",
      "epoch: 3000/40000 loss_train: 0.06620612 + 0.00000000 = 0.06620612 loss_val = 0.06620612\n",
      "epoch: 6000/40000 loss_train: 0.04521744 + 0.00000000 = 0.04521744 loss_val = 0.04521744\n",
      "epoch: 9000/40000 loss_train: 0.03315700 + 0.00000000 = 0.03315700 loss_val = 0.03315700\n",
      "epoch: 12000/40000 loss_train: 0.02311591 + 0.00000000 = 0.02311591 loss_val = 0.02311591\n",
      "epoch: 15000/40000 loss_train: 0.01484964 + 0.00000000 = 0.01484964 loss_val = 0.01484964\n",
      "epoch: 18000/40000 loss_train: 0.00970533 + 0.00000000 = 0.00970533 loss_val = 0.00970533\n",
      "epoch: 21000/40000 loss_train: 0.00669968 + 0.00000000 = 0.00669968 loss_val = 0.00669968\n",
      "epoch: 24000/40000 loss_train: 0.00490163 + 0.00000000 = 0.00490163 loss_val = 0.00490163\n",
      "epoch: 27000/40000 loss_train: 0.00376467 + 0.00000000 = 0.00376467 loss_val = 0.00376467\n",
      "epoch: 30000/40000 loss_train: 0.00300287 + 0.00000000 = 0.00300287 loss_val = 0.00300287\n",
      "epoch: 33000/40000 loss_train: 0.00246725 + 0.00000000 = 0.00246725 loss_val = 0.00246725\n",
      "epoch: 36000/40000 loss_train: 0.00207504 + 0.00000000 = 0.00207504 loss_val = 0.00207504\n",
      "epoch: 39000/40000 loss_train: 0.00177819 + 0.00000000 = 0.00177819 loss_val = 0.00177819\n",
      "Para 8 neurônios, 1 camadas,  0.01 learning rate, 4 batch size, 0 dropout rate, temos: \u001b[1m Acurácia: 100.00% \u001b[0m\n",
      "epoch:    0/40000 loss_train: 1.65558430 + 0.00000000 = 1.65558430 loss_val = 1.65558430\n",
      "epoch: 3000/40000 loss_train: 0.08484311 + 0.00000000 = 0.08484311 loss_val = 0.08484311\n",
      "epoch: 6000/40000 loss_train: 0.05561591 + 0.00000000 = 0.05561591 loss_val = 0.05561591\n",
      "Para 8 neurônios, 1 camadas,  0.01 learning rate, 4 batch size, 0.1 dropout rate, temos: \u001b[1m Acurácia: 97.62% \u001b[0m\n",
      "epoch:    0/40000 loss_train: 1.17312838 + 0.00000000 = 1.17312838 loss_val = 1.17312838\n",
      "epoch: 3000/40000 loss_train: 0.07651268 + 0.00000000 = 0.07651268 loss_val = 0.07651268\n",
      "epoch: 6000/40000 loss_train: 0.04445923 + 0.00000000 = 0.04445923 loss_val = 0.04445923\n",
      "epoch: 9000/40000 loss_train: 0.03034873 + 0.00000000 = 0.03034873 loss_val = 0.03034873\n",
      "epoch: 12000/40000 loss_train: 0.02117808 + 0.00000000 = 0.02117808 loss_val = 0.02117808\n",
      "epoch: 15000/40000 loss_train: 0.01493336 + 0.00000000 = 0.01493336 loss_val = 0.01493336\n",
      "epoch: 18000/40000 loss_train: 0.01074582 + 0.00000000 = 0.01074582 loss_val = 0.01074582\n",
      "epoch: 21000/40000 loss_train: 0.00796314 + 0.00000000 = 0.00796314 loss_val = 0.00796314\n",
      "epoch: 24000/40000 loss_train: 0.00608521 + 0.00000000 = 0.00608521 loss_val = 0.00608521\n",
      "epoch: 27000/40000 loss_train: 0.00478620 + 0.00000000 = 0.00478620 loss_val = 0.00478620\n",
      "epoch: 30000/40000 loss_train: 0.00386225 + 0.00000000 = 0.00386225 loss_val = 0.00386225\n",
      "epoch: 33000/40000 loss_train: 0.00318684 + 0.00000000 = 0.00318684 loss_val = 0.00318684\n",
      "epoch: 36000/40000 loss_train: 0.00268032 + 0.00000000 = 0.00268032 loss_val = 0.00268032\n",
      "epoch: 39000/40000 loss_train: 0.00229123 + 0.00000000 = 0.00229123 loss_val = 0.00229123\n",
      "Para 8 neurônios, 1 camadas,  0.01 learning rate, 8 batch size, 0 dropout rate, temos: \u001b[1m Acurácia: 100.00% \u001b[0m\n",
      "epoch:    0/40000 loss_train: 1.53911576 + 0.00000000 = 1.53911576 loss_val = 1.53911576\n",
      "epoch: 3000/40000 loss_train: 0.16629652 + 0.00000000 = 0.16629652 loss_val = 0.16629652\n",
      "epoch: 6000/40000 loss_train: 0.09521796 + 0.00000000 = 0.09521796 loss_val = 0.09521796\n",
      "epoch: 9000/40000 loss_train: 0.07791216 + 0.00000000 = 0.07791216 loss_val = 0.07791216\n",
      "epoch: 12000/40000 loss_train: 0.06951403 + 0.00000000 = 0.06951403 loss_val = 0.06951403\n",
      "Para 8 neurônios, 1 camadas,  0.01 learning rate, 8 batch size, 0.1 dropout rate, temos: \u001b[1m Acurácia: 95.24% \u001b[0m\n",
      "epoch:    0/40000 loss_train: 0.98343829 + 0.00000000 = 0.98343829 loss_val = 0.98343829\n",
      "epoch: 3000/40000 loss_train: 0.34321446 + 0.00000000 = 0.34321446 loss_val = 0.34321446\n",
      "epoch: 6000/40000 loss_train: 0.19300111 + 0.00000000 = 0.19300111 loss_val = 0.19300111\n",
      "epoch: 9000/40000 loss_train: 0.13841960 + 0.00000000 = 0.13841960 loss_val = 0.13841960\n",
      "epoch: 12000/40000 loss_train: 0.11577030 + 0.00000000 = 0.11577030 loss_val = 0.11577030\n",
      "epoch: 15000/40000 loss_train: 0.10393907 + 0.00000000 = 0.10393907 loss_val = 0.10393907\n",
      "epoch: 18000/40000 loss_train: 0.09668597 + 0.00000000 = 0.09668597 loss_val = 0.09668597\n",
      "epoch: 21000/40000 loss_train: 0.09161084 + 0.00000000 = 0.09161084 loss_val = 0.09161084\n",
      "epoch: 24000/40000 loss_train: 0.08772234 + 0.00000000 = 0.08772234 loss_val = 0.08772234\n",
      "epoch: 27000/40000 loss_train: 0.08464197 + 0.00000000 = 0.08464197 loss_val = 0.08464197\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 30000/40000 loss_train: 0.08206011 + 0.00000000 = 0.08206011 loss_val = 0.08206011\n",
      "epoch: 33000/40000 loss_train: 0.07984468 + 0.00000000 = 0.07984468 loss_val = 0.07984468\n",
      "epoch: 36000/40000 loss_train: 0.07792791 + 0.00000000 = 0.07792791 loss_val = 0.07792791\n",
      "epoch: 39000/40000 loss_train: 0.07622701 + 0.00000000 = 0.07622701 loss_val = 0.07622701\n",
      "Para 12 neurônios, 1 camadas,  0.001 learning rate, 0 batch size, 0 dropout rate, temos: \u001b[1m Acurácia: 100.00% \u001b[0m\n",
      "epoch:    0/40000 loss_train: 1.41084577 + 0.00000000 = 1.41084577 loss_val = 1.41084577\n",
      "epoch: 3000/40000 loss_train: 0.35936070 + 0.00000000 = 0.35936070 loss_val = 0.35936070\n",
      "epoch: 6000/40000 loss_train: 0.20867458 + 0.00000000 = 0.20867458 loss_val = 0.20867458\n",
      "epoch: 9000/40000 loss_train: 0.14220915 + 0.00000000 = 0.14220915 loss_val = 0.14220915\n",
      "epoch: 12000/40000 loss_train: 0.11183756 + 0.00000000 = 0.11183756 loss_val = 0.11183756\n",
      "epoch: 15000/40000 loss_train: 0.09547652 + 0.00000000 = 0.09547652 loss_val = 0.09547652\n",
      "epoch: 18000/40000 loss_train: 0.08528249 + 0.00000000 = 0.08528249 loss_val = 0.08528249\n",
      "epoch: 21000/40000 loss_train: 0.07801122 + 0.00000000 = 0.07801122 loss_val = 0.07801122\n",
      "epoch: 24000/40000 loss_train: 0.07219179 + 0.00000000 = 0.07219179 loss_val = 0.07219179\n",
      "epoch: 27000/40000 loss_train: 0.06737087 + 0.00000000 = 0.06737087 loss_val = 0.06737087\n",
      "epoch: 30000/40000 loss_train: 0.06331260 + 0.00000000 = 0.06331260 loss_val = 0.06331260\n",
      "epoch: 33000/40000 loss_train: 0.05983729 + 0.00000000 = 0.05983729 loss_val = 0.05983729\n",
      "epoch: 36000/40000 loss_train: 0.05668096 + 0.00000000 = 0.05668096 loss_val = 0.05668096\n",
      "Para 12 neurônios, 1 camadas,  0.001 learning rate, 0 batch size, 0.1 dropout rate, temos: \u001b[1m Acurácia: 100.00% \u001b[0m\n",
      "epoch:    0/40000 loss_train: 1.25355803 + 0.00000000 = 1.25355803 loss_val = 1.25355803\n",
      "epoch: 3000/40000 loss_train: 0.36952890 + 0.00000000 = 0.36952890 loss_val = 0.36952890\n",
      "epoch: 6000/40000 loss_train: 0.21078439 + 0.00000000 = 0.21078439 loss_val = 0.21078439\n",
      "epoch: 9000/40000 loss_train: 0.14462891 + 0.00000000 = 0.14462891 loss_val = 0.14462891\n",
      "epoch: 12000/40000 loss_train: 0.11490661 + 0.00000000 = 0.11490661 loss_val = 0.11490661\n",
      "epoch: 15000/40000 loss_train: 0.09875195 + 0.00000000 = 0.09875195 loss_val = 0.09875195\n",
      "epoch: 18000/40000 loss_train: 0.08840484 + 0.00000000 = 0.08840484 loss_val = 0.08840484\n",
      "epoch: 21000/40000 loss_train: 0.08097290 + 0.00000000 = 0.08097290 loss_val = 0.08097290\n",
      "epoch: 24000/40000 loss_train: 0.07537431 + 0.00000000 = 0.07537431 loss_val = 0.07537431\n",
      "epoch: 27000/40000 loss_train: 0.07081949 + 0.00000000 = 0.07081949 loss_val = 0.07081949\n",
      "epoch: 30000/40000 loss_train: 0.06708058 + 0.00000000 = 0.06708058 loss_val = 0.06708058\n",
      "epoch: 33000/40000 loss_train: 0.06383646 + 0.00000000 = 0.06383646 loss_val = 0.06383646\n",
      "epoch: 36000/40000 loss_train: 0.06097181 + 0.00000000 = 0.06097181 loss_val = 0.06097181\n",
      "epoch: 39000/40000 loss_train: 0.05842000 + 0.00000000 = 0.05842000 loss_val = 0.05842000\n",
      "Para 12 neurônios, 1 camadas,  0.001 learning rate, 4 batch size, 0 dropout rate, temos: \u001b[1m Acurácia: 97.62% \u001b[0m\n",
      "epoch:    0/40000 loss_train: 0.65738247 + 0.00000000 = 0.65738247 loss_val = 0.65738247\n",
      "epoch: 3000/40000 loss_train: 0.44743385 + 0.00000000 = 0.44743385 loss_val = 0.44743385\n",
      "epoch: 6000/40000 loss_train: 0.26764690 + 0.00000000 = 0.26764690 loss_val = 0.26764690\n",
      "epoch: 9000/40000 loss_train: 0.17836124 + 0.00000000 = 0.17836124 loss_val = 0.17836124\n",
      "epoch: 12000/40000 loss_train: 0.13416576 + 0.00000000 = 0.13416576 loss_val = 0.13416576\n",
      "epoch: 15000/40000 loss_train: 0.11240500 + 0.00000000 = 0.11240500 loss_val = 0.11240500\n",
      "epoch: 18000/40000 loss_train: 0.10017989 + 0.00000000 = 0.10017989 loss_val = 0.10017989\n",
      "epoch: 21000/40000 loss_train: 0.09169678 + 0.00000000 = 0.09169678 loss_val = 0.09169678\n",
      "epoch: 24000/40000 loss_train: 0.08558936 + 0.00000000 = 0.08558936 loss_val = 0.08558936\n",
      "epoch: 27000/40000 loss_train: 0.08083500 + 0.00000000 = 0.08083500 loss_val = 0.08083500\n",
      "epoch: 30000/40000 loss_train: 0.07671607 + 0.00000000 = 0.07671607 loss_val = 0.07671607\n",
      "epoch: 33000/40000 loss_train: 0.07308889 + 0.00000000 = 0.07308889 loss_val = 0.07308889\n",
      "epoch: 36000/40000 loss_train: 0.06988699 + 0.00000000 = 0.06988699 loss_val = 0.06988699\n",
      "epoch: 39000/40000 loss_train: 0.06685339 + 0.00000000 = 0.06685339 loss_val = 0.06685339\n",
      "Para 12 neurônios, 1 camadas,  0.001 learning rate, 4 batch size, 0.1 dropout rate, temos: \u001b[1m Acurácia: 97.62% \u001b[0m\n",
      "epoch:    0/40000 loss_train: 1.26668972 + 0.00000000 = 1.26668972 loss_val = 1.26668972\n",
      "epoch: 3000/40000 loss_train: 0.44012263 + 0.00000000 = 0.44012263 loss_val = 0.44012263\n",
      "epoch: 6000/40000 loss_train: 0.29850992 + 0.00000000 = 0.29850992 loss_val = 0.29850992\n",
      "epoch: 9000/40000 loss_train: 0.22231899 + 0.00000000 = 0.22231899 loss_val = 0.22231899\n",
      "epoch: 12000/40000 loss_train: 0.17323925 + 0.00000000 = 0.17323925 loss_val = 0.17323925\n",
      "epoch: 15000/40000 loss_train: 0.13746976 + 0.00000000 = 0.13746976 loss_val = 0.13746976\n",
      "epoch: 18000/40000 loss_train: 0.10966711 + 0.00000000 = 0.10966711 loss_val = 0.10966711\n",
      "epoch: 21000/40000 loss_train: 0.08911467 + 0.00000000 = 0.08911467 loss_val = 0.08911467\n",
      "epoch: 24000/40000 loss_train: 0.07437590 + 0.00000000 = 0.07437590 loss_val = 0.07437590\n",
      "epoch: 27000/40000 loss_train: 0.06388363 + 0.00000000 = 0.06388363 loss_val = 0.06388363\n",
      "epoch: 30000/40000 loss_train: 0.05615376 + 0.00000000 = 0.05615376 loss_val = 0.05615376\n",
      "epoch: 33000/40000 loss_train: 0.05007233 + 0.00000000 = 0.05007233 loss_val = 0.05007233\n",
      "epoch: 36000/40000 loss_train: 0.04537365 + 0.00000000 = 0.04537365 loss_val = 0.04537365\n",
      "epoch: 39000/40000 loss_train: 0.04155773 + 0.00000000 = 0.04155773 loss_val = 0.04155773\n",
      "Para 12 neurônios, 1 camadas,  0.001 learning rate, 8 batch size, 0 dropout rate, temos: \u001b[1m Acurácia: 100.00% \u001b[0m\n",
      "epoch:    0/40000 loss_train: 0.86816108 + 0.00000000 = 0.86816108 loss_val = 0.86816108\n",
      "epoch: 3000/40000 loss_train: 0.57912258 + 0.00000000 = 0.57912258 loss_val = 0.57912258\n",
      "epoch: 6000/40000 loss_train: 0.40547139 + 0.00000000 = 0.40547139 loss_val = 0.40547139\n",
      "epoch: 9000/40000 loss_train: 0.28018227 + 0.00000000 = 0.28018227 loss_val = 0.28018227\n",
      "epoch: 12000/40000 loss_train: 0.20092875 + 0.00000000 = 0.20092875 loss_val = 0.20092875\n",
      "epoch: 15000/40000 loss_train: 0.14776433 + 0.00000000 = 0.14776433 loss_val = 0.14776433\n",
      "epoch: 18000/40000 loss_train: 0.11482824 + 0.00000000 = 0.11482824 loss_val = 0.11482824\n",
      "epoch: 21000/40000 loss_train: 0.09355708 + 0.00000000 = 0.09355708 loss_val = 0.09355708\n",
      "epoch: 24000/40000 loss_train: 0.07881839 + 0.00000000 = 0.07881839 loss_val = 0.07881839\n",
      "epoch: 27000/40000 loss_train: 0.06910923 + 0.00000000 = 0.06910923 loss_val = 0.06910923\n",
      "epoch: 30000/40000 loss_train: 0.06211991 + 0.00000000 = 0.06211991 loss_val = 0.06211991\n",
      "epoch: 33000/40000 loss_train: 0.05664489 + 0.00000000 = 0.05664489 loss_val = 0.05664489\n",
      "epoch: 36000/40000 loss_train: 0.05231438 + 0.00000000 = 0.05231438 loss_val = 0.05231438\n",
      "epoch: 39000/40000 loss_train: 0.04875204 + 0.00000000 = 0.04875204 loss_val = 0.04875204\n",
      "Para 12 neurônios, 1 camadas,  0.001 learning rate, 8 batch size, 0.1 dropout rate, temos: \u001b[1m Acurácia: 100.00% \u001b[0m\n",
      "epoch:    0/40000 loss_train: 0.91540343 + 0.00000000 = 0.91540343 loss_val = 0.91540343\n",
      "epoch: 3000/40000 loss_train: 0.12260710 + 0.00000000 = 0.12260710 loss_val = 0.12260710\n",
      "epoch: 6000/40000 loss_train: 0.07343517 + 0.00000000 = 0.07343517 loss_val = 0.07343517\n",
      "epoch: 9000/40000 loss_train: 0.05765661 + 0.00000000 = 0.05765661 loss_val = 0.05765661\n",
      "epoch: 12000/40000 loss_train: 0.04804147 + 0.00000000 = 0.04804147 loss_val = 0.04804147\n",
      "epoch: 15000/40000 loss_train: 0.04108708 + 0.00000000 = 0.04108708 loss_val = 0.04108708\n",
      "epoch: 18000/40000 loss_train: 0.03568063 + 0.00000000 = 0.03568063 loss_val = 0.03568063\n",
      "epoch: 21000/40000 loss_train: 0.03000227 + 0.00000000 = 0.03000227 loss_val = 0.03000227\n",
      "epoch: 24000/40000 loss_train: 0.02612103 + 0.00000000 = 0.02612103 loss_val = 0.02612103\n",
      "epoch: 27000/40000 loss_train: 0.02284687 + 0.00000000 = 0.02284687 loss_val = 0.02284687\n",
      "epoch: 30000/40000 loss_train: 0.01998387 + 0.00000000 = 0.01998387 loss_val = 0.01998387\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 33000/40000 loss_train: 0.01748907 + 0.00000000 = 0.01748907 loss_val = 0.01748907\n",
      "epoch: 36000/40000 loss_train: 0.01530165 + 0.00000000 = 0.01530165 loss_val = 0.01530165\n",
      "epoch: 39000/40000 loss_train: 0.01339745 + 0.00000000 = 0.01339745 loss_val = 0.01339745\n",
      "Para 12 neurônios, 1 camadas,  0.005 learning rate, 0 batch size, 0 dropout rate, temos: \u001b[1m Acurácia: 97.62% \u001b[0m\n",
      "epoch:    0/40000 loss_train: 0.73666182 + 0.00000000 = 0.73666182 loss_val = 0.73666182\n",
      "epoch: 3000/40000 loss_train: 0.09518169 + 0.00000000 = 0.09518169 loss_val = 0.09518169\n",
      "epoch: 6000/40000 loss_train: 0.04612953 + 0.00000000 = 0.04612953 loss_val = 0.04612953\n",
      "epoch: 9000/40000 loss_train: 0.03273094 + 0.00000000 = 0.03273094 loss_val = 0.03273094\n",
      "epoch: 12000/40000 loss_train: 0.02559557 + 0.00000000 = 0.02559557 loss_val = 0.02559557\n",
      "epoch: 15000/40000 loss_train: 0.02077753 + 0.00000000 = 0.02077753 loss_val = 0.02077753\n",
      "epoch: 18000/40000 loss_train: 0.01737168 + 0.00000000 = 0.01737168 loss_val = 0.01737168\n",
      "epoch: 21000/40000 loss_train: 0.01452485 + 0.00000000 = 0.01452485 loss_val = 0.01452485\n",
      "Para 12 neurônios, 1 camadas,  0.005 learning rate, 0 batch size, 0.1 dropout rate, temos: \u001b[1m Acurácia: 97.62% \u001b[0m\n",
      "epoch:    0/40000 loss_train: 0.77095529 + 0.00000000 = 0.77095529 loss_val = 0.77095529\n",
      "epoch: 3000/40000 loss_train: 0.11957938 + 0.00000000 = 0.11957938 loss_val = 0.11957938\n",
      "epoch: 6000/40000 loss_train: 0.06355190 + 0.00000000 = 0.06355190 loss_val = 0.06355190\n",
      "epoch: 9000/40000 loss_train: 0.04591811 + 0.00000000 = 0.04591811 loss_val = 0.04591811\n",
      "epoch: 12000/40000 loss_train: 0.03615852 + 0.00000000 = 0.03615852 loss_val = 0.03615852\n",
      "epoch: 15000/40000 loss_train: 0.02944909 + 0.00000000 = 0.02944909 loss_val = 0.02944909\n",
      "epoch: 18000/40000 loss_train: 0.02407867 + 0.00000000 = 0.02407867 loss_val = 0.02407867\n",
      "epoch: 21000/40000 loss_train: 0.01987874 + 0.00000000 = 0.01987874 loss_val = 0.01987874\n",
      "epoch: 24000/40000 loss_train: 0.01658583 + 0.00000000 = 0.01658583 loss_val = 0.01658583\n",
      "epoch: 27000/40000 loss_train: 0.01389469 + 0.00000000 = 0.01389469 loss_val = 0.01389469\n",
      "epoch: 30000/40000 loss_train: 0.01170487 + 0.00000000 = 0.01170487 loss_val = 0.01170487\n",
      "epoch: 33000/40000 loss_train: 0.00992685 + 0.00000000 = 0.00992685 loss_val = 0.00992685\n",
      "epoch: 36000/40000 loss_train: 0.00849247 + 0.00000000 = 0.00849247 loss_val = 0.00849247\n",
      "epoch: 39000/40000 loss_train: 0.00733040 + 0.00000000 = 0.00733040 loss_val = 0.00733040\n",
      "Para 12 neurônios, 1 camadas,  0.005 learning rate, 4 batch size, 0 dropout rate, temos: \u001b[1m Acurácia: 100.00% \u001b[0m\n",
      "epoch:    0/40000 loss_train: 0.75937204 + 0.00000000 = 0.75937204 loss_val = 0.75937204\n",
      "epoch: 3000/40000 loss_train: 0.15334363 + 0.00000000 = 0.15334363 loss_val = 0.15334363\n",
      "epoch: 6000/40000 loss_train: 0.09902811 + 0.00000000 = 0.09902811 loss_val = 0.09902811\n",
      "epoch: 9000/40000 loss_train: 0.07854940 + 0.00000000 = 0.07854940 loss_val = 0.07854940\n",
      "epoch: 12000/40000 loss_train: 0.06438387 + 0.00000000 = 0.06438387 loss_val = 0.06438387\n",
      "epoch: 15000/40000 loss_train: 0.05481253 + 0.00000000 = 0.05481253 loss_val = 0.05481253\n",
      "epoch: 18000/40000 loss_train: 0.04838029 + 0.00000000 = 0.04838029 loss_val = 0.04838029\n",
      "epoch: 21000/40000 loss_train: 0.04368193 + 0.00000000 = 0.04368193 loss_val = 0.04368193\n",
      "Para 12 neurônios, 1 camadas,  0.005 learning rate, 4 batch size, 0.1 dropout rate, temos: \u001b[1m Acurácia: 97.62% \u001b[0m\n",
      "epoch:    0/40000 loss_train: 0.71136363 + 0.00000000 = 0.71136363 loss_val = 0.71136363\n",
      "epoch: 3000/40000 loss_train: 0.14590227 + 0.00000000 = 0.14590227 loss_val = 0.14590227\n",
      "epoch: 6000/40000 loss_train: 0.05853977 + 0.00000000 = 0.05853977 loss_val = 0.05853977\n",
      "epoch: 9000/40000 loss_train: 0.03956983 + 0.00000000 = 0.03956983 loss_val = 0.03956983\n",
      "epoch: 12000/40000 loss_train: 0.03081552 + 0.00000000 = 0.03081552 loss_val = 0.03081552\n",
      "epoch: 15000/40000 loss_train: 0.02523674 + 0.00000000 = 0.02523674 loss_val = 0.02523674\n",
      "epoch: 18000/40000 loss_train: 0.02107682 + 0.00000000 = 0.02107682 loss_val = 0.02107682\n",
      "epoch: 21000/40000 loss_train: 0.01785045 + 0.00000000 = 0.01785045 loss_val = 0.01785045\n",
      "epoch: 24000/40000 loss_train: 0.01525274 + 0.00000000 = 0.01525274 loss_val = 0.01525274\n",
      "epoch: 27000/40000 loss_train: 0.01311033 + 0.00000000 = 0.01311033 loss_val = 0.01311033\n",
      "epoch: 30000/40000 loss_train: 0.01132442 + 0.00000000 = 0.01132442 loss_val = 0.01132442\n",
      "epoch: 33000/40000 loss_train: 0.00983551 + 0.00000000 = 0.00983551 loss_val = 0.00983551\n",
      "epoch: 36000/40000 loss_train: 0.00859114 + 0.00000000 = 0.00859114 loss_val = 0.00859114\n",
      "epoch: 39000/40000 loss_train: 0.00755240 + 0.00000000 = 0.00755240 loss_val = 0.00755240\n",
      "Para 12 neurônios, 1 camadas,  0.005 learning rate, 8 batch size, 0 dropout rate, temos: \u001b[1m Acurácia: 100.00% \u001b[0m\n",
      "epoch:    0/40000 loss_train: 0.83028212 + 0.00000000 = 0.83028212 loss_val = 0.83028212\n",
      "epoch: 3000/40000 loss_train: 0.15285218 + 0.00000000 = 0.15285218 loss_val = 0.15285218\n",
      "epoch: 6000/40000 loss_train: 0.09482695 + 0.00000000 = 0.09482695 loss_val = 0.09482695\n",
      "epoch: 9000/40000 loss_train: 0.08018821 + 0.00000000 = 0.08018821 loss_val = 0.08018821\n",
      "Para 12 neurônios, 1 camadas,  0.005 learning rate, 8 batch size, 0.1 dropout rate, temos: \u001b[1m Acurácia: 97.62% \u001b[0m\n",
      "epoch:    0/40000 loss_train: 0.78088586 + 0.00000000 = 0.78088586 loss_val = 0.78088586\n",
      "epoch: 3000/40000 loss_train: 0.05513011 + 0.00000000 = 0.05513011 loss_val = 0.05513011\n",
      "epoch: 6000/40000 loss_train: 0.02894058 + 0.00000000 = 0.02894058 loss_val = 0.02894058\n",
      "epoch: 9000/40000 loss_train: 0.01965717 + 0.00000000 = 0.01965717 loss_val = 0.01965717\n",
      "epoch: 12000/40000 loss_train: 0.01429518 + 0.00000000 = 0.01429518 loss_val = 0.01429518\n",
      "epoch: 15000/40000 loss_train: 0.01071331 + 0.00000000 = 0.01071331 loss_val = 0.01071331\n",
      "epoch: 18000/40000 loss_train: 0.00825136 + 0.00000000 = 0.00825136 loss_val = 0.00825136\n",
      "epoch: 21000/40000 loss_train: 0.00651088 + 0.00000000 = 0.00651088 loss_val = 0.00651088\n",
      "epoch: 24000/40000 loss_train: 0.00526171 + 0.00000000 = 0.00526171 loss_val = 0.00526171\n",
      "epoch: 27000/40000 loss_train: 0.00434228 + 0.00000000 = 0.00434228 loss_val = 0.00434228\n",
      "epoch: 30000/40000 loss_train: 0.00363774 + 0.00000000 = 0.00363774 loss_val = 0.00363774\n",
      "epoch: 33000/40000 loss_train: 0.00309789 + 0.00000000 = 0.00309789 loss_val = 0.00309789\n",
      "epoch: 36000/40000 loss_train: 0.00267675 + 0.00000000 = 0.00267675 loss_val = 0.00267675\n",
      "epoch: 39000/40000 loss_train: 0.00234091 + 0.00000000 = 0.00234091 loss_val = 0.00234091\n",
      "Para 12 neurônios, 1 camadas,  0.01 learning rate, 0 batch size, 0 dropout rate, temos: \u001b[1m Acurácia: 100.00% \u001b[0m\n",
      "epoch:    0/40000 loss_train: 0.74313164 + 0.00000000 = 0.74313164 loss_val = 0.74313164\n",
      "epoch: 3000/40000 loss_train: 0.06371872 + 0.00000000 = 0.06371872 loss_val = 0.06371872\n",
      "epoch: 6000/40000 loss_train: 0.04325943 + 0.00000000 = 0.04325943 loss_val = 0.04325943\n",
      "Para 12 neurônios, 1 camadas,  0.01 learning rate, 0 batch size, 0.1 dropout rate, temos: \u001b[1m Acurácia: 97.62% \u001b[0m\n",
      "epoch:    0/40000 loss_train: 0.70845502 + 0.00000000 = 0.70845502 loss_val = 0.70845502\n",
      "epoch: 3000/40000 loss_train: 0.07312869 + 0.00000000 = 0.07312869 loss_val = 0.07312869\n",
      "epoch: 6000/40000 loss_train: 0.04525362 + 0.00000000 = 0.04525362 loss_val = 0.04525362\n",
      "epoch: 9000/40000 loss_train: 0.03109452 + 0.00000000 = 0.03109452 loss_val = 0.03109452\n",
      "epoch: 12000/40000 loss_train: 0.02134075 + 0.00000000 = 0.02134075 loss_val = 0.02134075\n",
      "epoch: 15000/40000 loss_train: 0.01460266 + 0.00000000 = 0.01460266 loss_val = 0.01460266\n",
      "epoch: 18000/40000 loss_train: 0.01017308 + 0.00000000 = 0.01017308 loss_val = 0.01017308\n",
      "epoch: 21000/40000 loss_train: 0.00733946 + 0.00000000 = 0.00733946 loss_val = 0.00733946\n",
      "epoch: 24000/40000 loss_train: 0.00552666 + 0.00000000 = 0.00552666 loss_val = 0.00552666\n",
      "epoch: 27000/40000 loss_train: 0.00431825 + 0.00000000 = 0.00431825 loss_val = 0.00431825\n",
      "epoch: 30000/40000 loss_train: 0.00347598 + 0.00000000 = 0.00347598 loss_val = 0.00347598\n",
      "epoch: 33000/40000 loss_train: 0.00286780 + 0.00000000 = 0.00286780 loss_val = 0.00286780\n",
      "epoch: 36000/40000 loss_train: 0.00241400 + 0.00000000 = 0.00241400 loss_val = 0.00241400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 39000/40000 loss_train: 0.00206632 + 0.00000000 = 0.00206632 loss_val = 0.00206632\n",
      "Para 12 neurônios, 1 camadas,  0.01 learning rate, 4 batch size, 0 dropout rate, temos: \u001b[1m Acurácia: 100.00% \u001b[0m\n",
      "epoch:    0/40000 loss_train: 0.71990213 + 0.00000000 = 0.71990213 loss_val = 0.71990213\n",
      "epoch: 3000/40000 loss_train: 0.06283559 + 0.00000000 = 0.06283559 loss_val = 0.06283559\n",
      "epoch: 6000/40000 loss_train: 0.03716407 + 0.00000000 = 0.03716407 loss_val = 0.03716407\n",
      "epoch: 9000/40000 loss_train: 0.02609718 + 0.00000000 = 0.02609718 loss_val = 0.02609718\n",
      "epoch: 12000/40000 loss_train: 0.01928906 + 0.00000000 = 0.01928906 loss_val = 0.01928906\n",
      "epoch: 15000/40000 loss_train: 0.01434287 + 0.00000000 = 0.01434287 loss_val = 0.01434287\n",
      "epoch: 18000/40000 loss_train: 0.01083743 + 0.00000000 = 0.01083743 loss_val = 0.01083743\n",
      "epoch: 21000/40000 loss_train: 0.00839689 + 0.00000000 = 0.00839689 loss_val = 0.00839689\n",
      "epoch: 24000/40000 loss_train: 0.00660391 + 0.00000000 = 0.00660391 loss_val = 0.00660391\n",
      "epoch: 27000/40000 loss_train: 0.00532035 + 0.00000000 = 0.00532035 loss_val = 0.00532035\n",
      "Para 12 neurônios, 1 camadas,  0.01 learning rate, 4 batch size, 0.1 dropout rate, temos: \u001b[1m Acurácia: 97.62% \u001b[0m\n",
      "epoch:    0/40000 loss_train: 0.73961029 + 0.00000000 = 0.73961029 loss_val = 0.73961029\n",
      "epoch: 3000/40000 loss_train: 0.04708428 + 0.00000000 = 0.04708428 loss_val = 0.04708428\n",
      "epoch: 6000/40000 loss_train: 0.02860518 + 0.00000000 = 0.02860518 loss_val = 0.02860518\n",
      "epoch: 9000/40000 loss_train: 0.01991595 + 0.00000000 = 0.01991595 loss_val = 0.01991595\n",
      "epoch: 12000/40000 loss_train: 0.01439131 + 0.00000000 = 0.01439131 loss_val = 0.01439131\n",
      "epoch: 15000/40000 loss_train: 0.01068601 + 0.00000000 = 0.01068601 loss_val = 0.01068601\n",
      "epoch: 18000/40000 loss_train: 0.00813607 + 0.00000000 = 0.00813607 loss_val = 0.00813607\n",
      "epoch: 21000/40000 loss_train: 0.00613940 + 0.00000000 = 0.00613940 loss_val = 0.00613940\n",
      "epoch: 24000/40000 loss_train: 0.00481582 + 0.00000000 = 0.00481582 loss_val = 0.00481582\n",
      "epoch: 27000/40000 loss_train: 0.00389019 + 0.00000000 = 0.00389019 loss_val = 0.00389019\n",
      "epoch: 30000/40000 loss_train: 0.00321814 + 0.00000000 = 0.00321814 loss_val = 0.00321814\n",
      "epoch: 33000/40000 loss_train: 0.00271436 + 0.00000000 = 0.00271436 loss_val = 0.00271436\n",
      "epoch: 36000/40000 loss_train: 0.00232876 + 0.00000000 = 0.00232876 loss_val = 0.00232876\n",
      "epoch: 39000/40000 loss_train: 0.00202357 + 0.00000000 = 0.00202357 loss_val = 0.00202357\n",
      "Para 12 neurônios, 1 camadas,  0.01 learning rate, 8 batch size, 0 dropout rate, temos: \u001b[1m Acurácia: 100.00% \u001b[0m\n",
      "epoch:    0/40000 loss_train: 1.17677969 + 0.00000000 = 1.17677969 loss_val = 1.17677969\n",
      "epoch: 3000/40000 loss_train: 0.06672134 + 0.00000000 = 0.06672134 loss_val = 0.06672134\n",
      "epoch: 6000/40000 loss_train: 0.03260652 + 0.00000000 = 0.03260652 loss_val = 0.03260652\n",
      "Para 12 neurônios, 1 camadas,  0.01 learning rate, 8 batch size, 0.1 dropout rate, temos: \u001b[1m Acurácia: 100.00% \u001b[0m\n",
      "epoch:    0/40000 loss_train: 0.78389231 + 0.00000000 = 0.78389231 loss_val = 0.78389231\n",
      "epoch: 3000/40000 loss_train: 0.66194739 + 0.00000000 = 0.66194739 loss_val = 0.66194739\n",
      "epoch: 6000/40000 loss_train: 0.59746282 + 0.00000000 = 0.59746282 loss_val = 0.59746282\n",
      "epoch: 9000/40000 loss_train: 0.50264516 + 0.00000000 = 0.50264516 loss_val = 0.50264516\n",
      "epoch: 12000/40000 loss_train: 0.38023669 + 0.00000000 = 0.38023669 loss_val = 0.38023669\n",
      "epoch: 15000/40000 loss_train: 0.30594905 + 0.00000000 = 0.30594905 loss_val = 0.30594905\n",
      "epoch: 18000/40000 loss_train: 0.27358979 + 0.00000000 = 0.27358979 loss_val = 0.27358979\n",
      "epoch: 21000/40000 loss_train: 0.25186262 + 0.00000000 = 0.25186262 loss_val = 0.25186262\n",
      "epoch: 24000/40000 loss_train: 0.23260648 + 0.00000000 = 0.23260648 loss_val = 0.23260648\n",
      "epoch: 27000/40000 loss_train: 0.21567137 + 0.00000000 = 0.21567137 loss_val = 0.21567137\n",
      "epoch: 30000/40000 loss_train: 0.19915082 + 0.00000000 = 0.19915082 loss_val = 0.19915082\n",
      "epoch: 33000/40000 loss_train: 0.17818507 + 0.00000000 = 0.17818507 loss_val = 0.17818507\n",
      "epoch: 36000/40000 loss_train: 0.15628918 + 0.00000000 = 0.15628918 loss_val = 0.15628918\n",
      "epoch: 39000/40000 loss_train: 0.13715353 + 0.00000000 = 0.13715353 loss_val = 0.13715353\n",
      "Para 7 neurônios, 2 camadas,  0.001 learning rate, 0 batch size, 0 dropout rate, temos: \u001b[1m Acurácia: 92.86% \u001b[0m\n",
      "epoch:    0/40000 loss_train: 1.33592615 + 0.00000000 = 1.33592615 loss_val = 1.33592615\n",
      "epoch: 3000/40000 loss_train: 0.62622640 + 0.00000000 = 0.62622640 loss_val = 0.62622640\n",
      "epoch: 6000/40000 loss_train: 0.49597745 + 0.00000000 = 0.49597745 loss_val = 0.49597745\n",
      "epoch: 9000/40000 loss_train: 0.36905926 + 0.00000000 = 0.36905926 loss_val = 0.36905926\n",
      "epoch: 12000/40000 loss_train: 0.22701870 + 0.00000000 = 0.22701870 loss_val = 0.22701870\n",
      "epoch: 15000/40000 loss_train: 0.14648114 + 0.00000000 = 0.14648114 loss_val = 0.14648114\n",
      "epoch: 18000/40000 loss_train: 0.09837932 + 0.00000000 = 0.09837932 loss_val = 0.09837932\n",
      "epoch: 21000/40000 loss_train: 0.07606132 + 0.00000000 = 0.07606132 loss_val = 0.07606132\n",
      "epoch: 24000/40000 loss_train: 0.06245979 + 0.00000000 = 0.06245979 loss_val = 0.06245979\n",
      "epoch: 27000/40000 loss_train: 0.05295944 + 0.00000000 = 0.05295944 loss_val = 0.05295944\n",
      "epoch: 30000/40000 loss_train: 0.04571599 + 0.00000000 = 0.04571599 loss_val = 0.04571599\n",
      "epoch: 33000/40000 loss_train: 0.04000338 + 0.00000000 = 0.04000338 loss_val = 0.04000338\n",
      "Para 7 neurônios, 2 camadas,  0.001 learning rate, 0 batch size, 0.1 dropout rate, temos: \u001b[1m Acurácia: 100.00% \u001b[0m\n",
      "epoch:    0/40000 loss_train: 0.86642075 + 0.00000000 = 0.86642075 loss_val = 0.86642075\n",
      "epoch: 3000/40000 loss_train: 0.51392908 + 0.00000000 = 0.51392908 loss_val = 0.51392908\n",
      "epoch: 6000/40000 loss_train: 0.32584233 + 0.00000000 = 0.32584233 loss_val = 0.32584233\n",
      "epoch: 9000/40000 loss_train: 0.17636684 + 0.00000000 = 0.17636684 loss_val = 0.17636684\n",
      "epoch: 12000/40000 loss_train: 0.11720717 + 0.00000000 = 0.11720717 loss_val = 0.11720717\n",
      "epoch: 15000/40000 loss_train: 0.09848877 + 0.00000000 = 0.09848877 loss_val = 0.09848877\n",
      "epoch: 18000/40000 loss_train: 0.08963986 + 0.00000000 = 0.08963986 loss_val = 0.08963986\n",
      "epoch: 21000/40000 loss_train: 0.08381467 + 0.00000000 = 0.08381467 loss_val = 0.08381467\n",
      "epoch: 24000/40000 loss_train: 0.07937853 + 0.00000000 = 0.07937853 loss_val = 0.07937853\n",
      "epoch: 27000/40000 loss_train: 0.07623301 + 0.00000000 = 0.07623301 loss_val = 0.07623301\n",
      "epoch: 30000/40000 loss_train: 0.07375900 + 0.00000000 = 0.07375900 loss_val = 0.07375900\n",
      "epoch: 33000/40000 loss_train: 0.07162247 + 0.00000000 = 0.07162247 loss_val = 0.07162247\n",
      "epoch: 36000/40000 loss_train: 0.06984399 + 0.00000000 = 0.06984399 loss_val = 0.06984399\n",
      "epoch: 39000/40000 loss_train: 0.06814444 + 0.00000000 = 0.06814444 loss_val = 0.06814444\n",
      "Para 7 neurônios, 2 camadas,  0.001 learning rate, 4 batch size, 0 dropout rate, temos: \u001b[1m Acurácia: 95.24% \u001b[0m\n",
      "epoch:    0/40000 loss_train: 0.75626712 + 0.00000000 = 0.75626712 loss_val = 0.75626712\n",
      "epoch: 3000/40000 loss_train: 0.49453198 + 0.00000000 = 0.49453198 loss_val = 0.49453198\n",
      "epoch: 6000/40000 loss_train: 0.36630968 + 0.00000000 = 0.36630968 loss_val = 0.36630968\n",
      "epoch: 9000/40000 loss_train: 0.29862853 + 0.00000000 = 0.29862853 loss_val = 0.29862853\n",
      "epoch: 12000/40000 loss_train: 0.26607803 + 0.00000000 = 0.26607803 loss_val = 0.26607803\n",
      "epoch: 15000/40000 loss_train: 0.24820412 + 0.00000000 = 0.24820412 loss_val = 0.24820412\n",
      "epoch: 18000/40000 loss_train: 0.23740546 + 0.00000000 = 0.23740546 loss_val = 0.23740546\n",
      "epoch: 21000/40000 loss_train: 0.22831138 + 0.00000000 = 0.22831138 loss_val = 0.22831138\n",
      "epoch: 24000/40000 loss_train: 0.22092696 + 0.00000000 = 0.22092696 loss_val = 0.22092696\n",
      "epoch: 27000/40000 loss_train: 0.21478795 + 0.00000000 = 0.21478795 loss_val = 0.21478795\n",
      "epoch: 30000/40000 loss_train: 0.20987275 + 0.00000000 = 0.20987275 loss_val = 0.20987275\n",
      "Para 7 neurônios, 2 camadas,  0.001 learning rate, 4 batch size, 0.1 dropout rate, temos: \u001b[1m Acurácia: 88.10% \u001b[0m\n",
      "epoch:    0/40000 loss_train: 1.07182676 + 0.00000000 = 1.07182676 loss_val = 1.07182676\n",
      "epoch: 3000/40000 loss_train: 0.68925045 + 0.00000000 = 0.68925045 loss_val = 0.68925045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6000/40000 loss_train: 0.64807544 + 0.00000000 = 0.64807544 loss_val = 0.64807544\n",
      "epoch: 9000/40000 loss_train: 0.59744734 + 0.00000000 = 0.59744734 loss_val = 0.59744734\n",
      "epoch: 12000/40000 loss_train: 0.48980427 + 0.00000000 = 0.48980427 loss_val = 0.48980427\n",
      "epoch: 15000/40000 loss_train: 0.35194604 + 0.00000000 = 0.35194604 loss_val = 0.35194604\n",
      "epoch: 18000/40000 loss_train: 0.24543329 + 0.00000000 = 0.24543329 loss_val = 0.24543329\n",
      "epoch: 21000/40000 loss_train: 0.18753172 + 0.00000000 = 0.18753172 loss_val = 0.18753172\n",
      "epoch: 24000/40000 loss_train: 0.15469939 + 0.00000000 = 0.15469939 loss_val = 0.15469939\n",
      "epoch: 27000/40000 loss_train: 0.13316036 + 0.00000000 = 0.13316036 loss_val = 0.13316036\n",
      "epoch: 30000/40000 loss_train: 0.11255763 + 0.00000000 = 0.11255763 loss_val = 0.11255763\n",
      "epoch: 33000/40000 loss_train: 0.09370083 + 0.00000000 = 0.09370083 loss_val = 0.09370083\n",
      "epoch: 36000/40000 loss_train: 0.08291724 + 0.00000000 = 0.08291724 loss_val = 0.08291724\n",
      "epoch: 39000/40000 loss_train: 0.07463575 + 0.00000000 = 0.07463575 loss_val = 0.07463575\n",
      "Para 7 neurônios, 2 camadas,  0.001 learning rate, 8 batch size, 0 dropout rate, temos: \u001b[1m Acurácia: 100.00% \u001b[0m\n",
      "epoch:    0/40000 loss_train: 0.78987331 + 0.00000000 = 0.78987331 loss_val = 0.78987331\n",
      "epoch: 3000/40000 loss_train: 0.67976904 + 0.00000000 = 0.67976904 loss_val = 0.67976904\n",
      "epoch: 6000/40000 loss_train: 0.63424066 + 0.00000000 = 0.63424066 loss_val = 0.63424066\n",
      "epoch: 9000/40000 loss_train: 0.55871042 + 0.00000000 = 0.55871042 loss_val = 0.55871042\n",
      "epoch: 12000/40000 loss_train: 0.42814876 + 0.00000000 = 0.42814876 loss_val = 0.42814876\n",
      "epoch: 15000/40000 loss_train: 0.28920723 + 0.00000000 = 0.28920723 loss_val = 0.28920723\n",
      "epoch: 18000/40000 loss_train: 0.18747538 + 0.00000000 = 0.18747538 loss_val = 0.18747538\n",
      "epoch: 21000/40000 loss_train: 0.13207968 + 0.00000000 = 0.13207968 loss_val = 0.13207968\n",
      "epoch: 24000/40000 loss_train: 0.09698613 + 0.00000000 = 0.09698613 loss_val = 0.09698613\n",
      "epoch: 27000/40000 loss_train: 0.07992526 + 0.00000000 = 0.07992526 loss_val = 0.07992526\n",
      "epoch: 30000/40000 loss_train: 0.06805067 + 0.00000000 = 0.06805067 loss_val = 0.06805067\n",
      "Para 7 neurônios, 2 camadas,  0.001 learning rate, 8 batch size, 0.1 dropout rate, temos: \u001b[1m Acurácia: 97.62% \u001b[0m\n",
      "epoch:    0/40000 loss_train: 0.76505997 + 0.00000000 = 0.76505997 loss_val = 0.76505997\n",
      "epoch: 3000/40000 loss_train: 0.15672643 + 0.00000000 = 0.15672643 loss_val = 0.15672643\n",
      "epoch: 6000/40000 loss_train: 0.08356021 + 0.00000000 = 0.08356021 loss_val = 0.08356021\n",
      "epoch: 9000/40000 loss_train: 0.07038554 + 0.00000000 = 0.07038554 loss_val = 0.07038554\n",
      "epoch: 12000/40000 loss_train: 0.05808464 + 0.00000000 = 0.05808464 loss_val = 0.05808464\n",
      "epoch: 15000/40000 loss_train: 0.04413151 + 0.00000000 = 0.04413151 loss_val = 0.04413151\n",
      "epoch: 18000/40000 loss_train: 0.03060207 + 0.00000000 = 0.03060207 loss_val = 0.03060207\n",
      "epoch: 21000/40000 loss_train: 0.01941107 + 0.00000000 = 0.01941107 loss_val = 0.01941107\n",
      "epoch: 24000/40000 loss_train: 0.01312123 + 0.00000000 = 0.01312123 loss_val = 0.01312123\n",
      "epoch: 27000/40000 loss_train: 0.00931582 + 0.00000000 = 0.00931582 loss_val = 0.00931582\n",
      "epoch: 30000/40000 loss_train: 0.00655403 + 0.00000000 = 0.00655403 loss_val = 0.00655403\n",
      "epoch: 33000/40000 loss_train: 0.00462026 + 0.00000000 = 0.00462026 loss_val = 0.00462026\n",
      "epoch: 36000/40000 loss_train: 0.00351279 + 0.00000000 = 0.00351279 loss_val = 0.00351279\n",
      "epoch: 39000/40000 loss_train: 0.00279997 + 0.00000000 = 0.00279997 loss_val = 0.00279997\n",
      "Para 7 neurônios, 2 camadas,  0.005 learning rate, 0 batch size, 0 dropout rate, temos: \u001b[1m Acurácia: 97.62% \u001b[0m\n",
      "epoch:    0/40000 loss_train: 0.66609961 + 0.00000000 = 0.66609961 loss_val = 0.66609961\n",
      "epoch: 3000/40000 loss_train: 0.24400222 + 0.00000000 = 0.24400222 loss_val = 0.24400222\n",
      "epoch: 6000/40000 loss_train: 0.14724312 + 0.00000000 = 0.14724312 loss_val = 0.14724312\n",
      "epoch: 9000/40000 loss_train: 0.07162211 + 0.00000000 = 0.07162211 loss_val = 0.07162211\n",
      "epoch: 12000/40000 loss_train: 0.03563821 + 0.00000000 = 0.03563821 loss_val = 0.03563821\n",
      "epoch: 15000/40000 loss_train: 0.02084821 + 0.00000000 = 0.02084821 loss_val = 0.02084821\n",
      "Para 7 neurônios, 2 camadas,  0.005 learning rate, 0 batch size, 0.1 dropout rate, temos: \u001b[1m Acurácia: 100.00% \u001b[0m\n",
      "epoch:    0/40000 loss_train: 0.68239357 + 0.00000000 = 0.68239357 loss_val = 0.68239357\n",
      "epoch: 3000/40000 loss_train: 0.09870470 + 0.00000000 = 0.09870470 loss_val = 0.09870470\n",
      "epoch: 6000/40000 loss_train: 0.07274248 + 0.00000000 = 0.07274248 loss_val = 0.07274248\n",
      "epoch: 9000/40000 loss_train: 0.06107508 + 0.00000000 = 0.06107508 loss_val = 0.06107508\n",
      "epoch: 12000/40000 loss_train: 0.05097549 + 0.00000000 = 0.05097549 loss_val = 0.05097549\n",
      "epoch: 15000/40000 loss_train: 0.04072486 + 0.00000000 = 0.04072486 loss_val = 0.04072486\n",
      "epoch: 18000/40000 loss_train: 0.03072118 + 0.00000000 = 0.03072118 loss_val = 0.03072118\n",
      "epoch: 21000/40000 loss_train: 0.02210470 + 0.00000000 = 0.02210470 loss_val = 0.02210470\n",
      "epoch: 24000/40000 loss_train: 0.01547191 + 0.00000000 = 0.01547191 loss_val = 0.01547191\n",
      "epoch: 27000/40000 loss_train: 0.01092428 + 0.00000000 = 0.01092428 loss_val = 0.01092428\n",
      "epoch: 30000/40000 loss_train: 0.00794511 + 0.00000000 = 0.00794511 loss_val = 0.00794511\n",
      "epoch: 33000/40000 loss_train: 0.00597285 + 0.00000000 = 0.00597285 loss_val = 0.00597285\n",
      "epoch: 36000/40000 loss_train: 0.00462197 + 0.00000000 = 0.00462197 loss_val = 0.00462197\n",
      "epoch: 39000/40000 loss_train: 0.00366991 + 0.00000000 = 0.00366991 loss_val = 0.00366991\n",
      "Para 7 neurônios, 2 camadas,  0.005 learning rate, 4 batch size, 0 dropout rate, temos: \u001b[1m Acurácia: 97.62% \u001b[0m\n",
      "epoch:    0/40000 loss_train: 1.28010275 + 0.00000000 = 1.28010275 loss_val = 1.28010275\n",
      "epoch: 3000/40000 loss_train: 0.09128995 + 0.00000000 = 0.09128995 loss_val = 0.09128995\n",
      "epoch: 6000/40000 loss_train: 0.04172369 + 0.00000000 = 0.04172369 loss_val = 0.04172369\n",
      "Para 7 neurônios, 2 camadas,  0.005 learning rate, 4 batch size, 0.1 dropout rate, temos: \u001b[1m Acurácia: 97.62% \u001b[0m\n",
      "epoch:    0/40000 loss_train: 0.92392272 + 0.00000000 = 0.92392272 loss_val = 0.92392272\n",
      "epoch: 3000/40000 loss_train: 0.59724749 + 0.00000000 = 0.59724749 loss_val = 0.59724749\n",
      "epoch: 6000/40000 loss_train: 0.15698499 + 0.00000000 = 0.15698499 loss_val = 0.15698499\n",
      "epoch: 9000/40000 loss_train: 0.07626553 + 0.00000000 = 0.07626553 loss_val = 0.07626553\n",
      "epoch: 12000/40000 loss_train: 0.05133352 + 0.00000000 = 0.05133352 loss_val = 0.05133352\n",
      "epoch: 15000/40000 loss_train: 0.03732090 + 0.00000000 = 0.03732090 loss_val = 0.03732090\n",
      "epoch: 18000/40000 loss_train: 0.02806536 + 0.00000000 = 0.02806536 loss_val = 0.02806536\n",
      "epoch: 21000/40000 loss_train: 0.02168996 + 0.00000000 = 0.02168996 loss_val = 0.02168996\n",
      "epoch: 24000/40000 loss_train: 0.01728305 + 0.00000000 = 0.01728305 loss_val = 0.01728305\n",
      "epoch: 27000/40000 loss_train: 0.01419182 + 0.00000000 = 0.01419182 loss_val = 0.01419182\n",
      "epoch: 30000/40000 loss_train: 0.01196860 + 0.00000000 = 0.01196860 loss_val = 0.01196860\n",
      "epoch: 33000/40000 loss_train: 0.01031773 + 0.00000000 = 0.01031773 loss_val = 0.01031773\n",
      "epoch: 36000/40000 loss_train: 0.00905507 + 0.00000000 = 0.00905507 loss_val = 0.00905507\n",
      "epoch: 39000/40000 loss_train: 0.00806335 + 0.00000000 = 0.00806335 loss_val = 0.00806335\n",
      "Para 7 neurônios, 2 camadas,  0.005 learning rate, 8 batch size, 0 dropout rate, temos: \u001b[1m Acurácia: 100.00% \u001b[0m\n",
      "epoch:    0/40000 loss_train: 0.78595135 + 0.00000000 = 0.78595135 loss_val = 0.78595135\n",
      "epoch: 3000/40000 loss_train: 0.51682748 + 0.00000000 = 0.51682748 loss_val = 0.51682748\n",
      "epoch: 6000/40000 loss_train: 0.07977399 + 0.00000000 = 0.07977399 loss_val = 0.07977399\n",
      "epoch: 9000/40000 loss_train: 0.03267236 + 0.00000000 = 0.03267236 loss_val = 0.03267236\n",
      "epoch: 12000/40000 loss_train: 0.01718558 + 0.00000000 = 0.01718558 loss_val = 0.01718558\n",
      "epoch: 15000/40000 loss_train: 0.01005312 + 0.00000000 = 0.01005312 loss_val = 0.01005312\n",
      "Para 7 neurônios, 2 camadas,  0.005 learning rate, 8 batch size, 0.1 dropout rate, temos: \u001b[1m Acurácia: 97.62% \u001b[0m\n",
      "epoch:    0/40000 loss_train: 0.76579715 + 0.00000000 = 0.76579715 loss_val = 0.76579715\n",
      "epoch: 3000/40000 loss_train: 0.03012240 + 0.00000000 = 0.03012240 loss_val = 0.03012240\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6000/40000 loss_train: 0.01118304 + 0.00000000 = 0.01118304 loss_val = 0.01118304\n",
      "epoch: 9000/40000 loss_train: 0.00536655 + 0.00000000 = 0.00536655 loss_val = 0.00536655\n",
      "epoch: 12000/40000 loss_train: 0.00308841 + 0.00000000 = 0.00308841 loss_val = 0.00308841\n",
      "epoch: 15000/40000 loss_train: 0.00202457 + 0.00000000 = 0.00202457 loss_val = 0.00202457\n",
      "epoch: 18000/40000 loss_train: 0.00144812 + 0.00000000 = 0.00144812 loss_val = 0.00144812\n",
      "epoch: 21000/40000 loss_train: 0.00110086 + 0.00000000 = 0.00110086 loss_val = 0.00110086\n",
      "epoch: 24000/40000 loss_train: 0.00087345 + 0.00000000 = 0.00087345 loss_val = 0.00087345\n",
      "epoch: 27000/40000 loss_train: 0.00071602 + 0.00000000 = 0.00071602 loss_val = 0.00071602\n",
      "epoch: 30000/40000 loss_train: 0.00060142 + 0.00000000 = 0.00060142 loss_val = 0.00060142\n",
      "epoch: 33000/40000 loss_train: 0.00051487 + 0.00000000 = 0.00051487 loss_val = 0.00051487\n",
      "epoch: 36000/40000 loss_train: 0.00044783 + 0.00000000 = 0.00044783 loss_val = 0.00044783\n",
      "epoch: 39000/40000 loss_train: 0.00039454 + 0.00000000 = 0.00039454 loss_val = 0.00039454\n",
      "Para 7 neurônios, 2 camadas,  0.01 learning rate, 0 batch size, 0 dropout rate, temos: \u001b[1m Acurácia: 100.00% \u001b[0m\n",
      "epoch:    0/40000 loss_train: 0.97215515 + 0.00000000 = 0.97215515 loss_val = 0.97215515\n",
      "epoch: 3000/40000 loss_train: 0.07673552 + 0.00000000 = 0.07673552 loss_val = 0.07673552\n",
      "epoch: 6000/40000 loss_train: 0.03603726 + 0.00000000 = 0.03603726 loss_val = 0.03603726\n",
      "epoch: 9000/40000 loss_train: 0.01212128 + 0.00000000 = 0.01212128 loss_val = 0.01212128\n",
      "Para 7 neurônios, 2 camadas,  0.01 learning rate, 0 batch size, 0.1 dropout rate, temos: \u001b[1m Acurácia: 97.62% \u001b[0m\n",
      "epoch:    0/40000 loss_train: 0.62449711 + 0.00000000 = 0.62449711 loss_val = 0.62449711\n",
      "epoch: 3000/40000 loss_train: 0.07985040 + 0.00000000 = 0.07985040 loss_val = 0.07985040\n",
      "epoch: 6000/40000 loss_train: 0.07652319 + 0.00000000 = 0.07652319 loss_val = 0.07652319\n",
      "epoch: 9000/40000 loss_train: 0.07621257 + 0.00000000 = 0.07621257 loss_val = 0.07621257\n",
      "epoch: 12000/40000 loss_train: 0.07606965 + 0.00000000 = 0.07606965 loss_val = 0.07606965\n",
      "epoch: 15000/40000 loss_train: 0.07593028 + 0.00000000 = 0.07593028 loss_val = 0.07593028\n",
      "epoch: 18000/40000 loss_train: 0.07577700 + 0.00000000 = 0.07577700 loss_val = 0.07577700\n",
      "epoch: 21000/40000 loss_train: 0.07560573 + 0.00000000 = 0.07560573 loss_val = 0.07560573\n",
      "epoch: 24000/40000 loss_train: 0.07535640 + 0.00000000 = 0.07535640 loss_val = 0.07535640\n",
      "epoch: 27000/40000 loss_train: 0.07513742 + 0.00000000 = 0.07513742 loss_val = 0.07513742\n",
      "epoch: 30000/40000 loss_train: 0.07491813 + 0.00000000 = 0.07491813 loss_val = 0.07491813\n",
      "epoch: 33000/40000 loss_train: 0.07468591 + 0.00000000 = 0.07468591 loss_val = 0.07468591\n",
      "epoch: 36000/40000 loss_train: 0.07444106 + 0.00000000 = 0.07444106 loss_val = 0.07444106\n",
      "epoch: 39000/40000 loss_train: 0.07418409 + 0.00000000 = 0.07418409 loss_val = 0.07418409\n",
      "Para 7 neurônios, 2 camadas,  0.01 learning rate, 4 batch size, 0 dropout rate, temos: \u001b[1m Acurácia: 97.62% \u001b[0m\n",
      "epoch:    0/40000 loss_train: 0.69762473 + 0.00000000 = 0.69762473 loss_val = 0.69762473\n",
      "epoch: 3000/40000 loss_train: 0.22782418 + 0.00000000 = 0.22782418 loss_val = 0.22782418\n",
      "epoch: 6000/40000 loss_train: 0.09106483 + 0.00000000 = 0.09106483 loss_val = 0.09106483\n",
      "Para 7 neurônios, 2 camadas,  0.01 learning rate, 4 batch size, 0.1 dropout rate, temos: \u001b[1m Acurácia: 97.62% \u001b[0m\n",
      "epoch:    0/40000 loss_train: 0.76677141 + 0.00000000 = 0.76677141 loss_val = 0.76677141\n",
      "epoch: 3000/40000 loss_train: 0.03727224 + 0.00000000 = 0.03727224 loss_val = 0.03727224\n",
      "epoch: 6000/40000 loss_train: 0.01742040 + 0.00000000 = 0.01742040 loss_val = 0.01742040\n",
      "epoch: 9000/40000 loss_train: 0.00891396 + 0.00000000 = 0.00891396 loss_val = 0.00891396\n",
      "epoch: 12000/40000 loss_train: 0.00494282 + 0.00000000 = 0.00494282 loss_val = 0.00494282\n",
      "epoch: 15000/40000 loss_train: 0.00307977 + 0.00000000 = 0.00307977 loss_val = 0.00307977\n",
      "epoch: 18000/40000 loss_train: 0.00211174 + 0.00000000 = 0.00211174 loss_val = 0.00211174\n",
      "epoch: 21000/40000 loss_train: 0.00155198 + 0.00000000 = 0.00155198 loss_val = 0.00155198\n",
      "epoch: 24000/40000 loss_train: 0.00119980 + 0.00000000 = 0.00119980 loss_val = 0.00119980\n",
      "epoch: 27000/40000 loss_train: 0.00096202 + 0.00000000 = 0.00096202 loss_val = 0.00096202\n",
      "epoch: 30000/40000 loss_train: 0.00079343 + 0.00000000 = 0.00079343 loss_val = 0.00079343\n",
      "epoch: 33000/40000 loss_train: 0.00066943 + 0.00000000 = 0.00066943 loss_val = 0.00066943\n",
      "epoch: 36000/40000 loss_train: 0.00057479 + 0.00000000 = 0.00057479 loss_val = 0.00057479\n",
      "epoch: 39000/40000 loss_train: 0.00050094 + 0.00000000 = 0.00050094 loss_val = 0.00050094\n",
      "Para 7 neurônios, 2 camadas,  0.01 learning rate, 8 batch size, 0 dropout rate, temos: \u001b[1m Acurácia: 100.00% \u001b[0m\n",
      "epoch:    0/40000 loss_train: 0.72259387 + 0.00000000 = 0.72259387 loss_val = 0.72259387\n",
      "epoch: 3000/40000 loss_train: 0.08326500 + 0.00000000 = 0.08326500 loss_val = 0.08326500\n",
      "Para 7 neurônios, 2 camadas,  0.01 learning rate, 8 batch size, 0.1 dropout rate, temos: \u001b[1m Acurácia: 97.62% \u001b[0m\n",
      "epoch:    0/40000 loss_train: 0.78990028 + 0.00000000 = 0.78990028 loss_val = 0.78990028\n",
      "epoch: 3000/40000 loss_train: 0.53336113 + 0.00000000 = 0.53336113 loss_val = 0.53336113\n",
      "epoch: 6000/40000 loss_train: 0.27383015 + 0.00000000 = 0.27383015 loss_val = 0.27383015\n",
      "epoch: 9000/40000 loss_train: 0.14536854 + 0.00000000 = 0.14536854 loss_val = 0.14536854\n",
      "epoch: 12000/40000 loss_train: 0.11251703 + 0.00000000 = 0.11251703 loss_val = 0.11251703\n",
      "epoch: 15000/40000 loss_train: 0.09976843 + 0.00000000 = 0.09976843 loss_val = 0.09976843\n",
      "epoch: 18000/40000 loss_train: 0.09215622 + 0.00000000 = 0.09215622 loss_val = 0.09215622\n",
      "epoch: 21000/40000 loss_train: 0.08584941 + 0.00000000 = 0.08584941 loss_val = 0.08584941\n",
      "epoch: 24000/40000 loss_train: 0.08185093 + 0.00000000 = 0.08185093 loss_val = 0.08185093\n",
      "epoch: 27000/40000 loss_train: 0.07871278 + 0.00000000 = 0.07871278 loss_val = 0.07871278\n",
      "epoch: 30000/40000 loss_train: 0.07617487 + 0.00000000 = 0.07617487 loss_val = 0.07617487\n",
      "epoch: 33000/40000 loss_train: 0.07420289 + 0.00000000 = 0.07420289 loss_val = 0.07420289\n",
      "epoch: 36000/40000 loss_train: 0.07244130 + 0.00000000 = 0.07244130 loss_val = 0.07244130\n",
      "epoch: 39000/40000 loss_train: 0.07078815 + 0.00000000 = 0.07078815 loss_val = 0.07078815\n",
      "Para 8 neurônios, 2 camadas,  0.001 learning rate, 0 batch size, 0 dropout rate, temos: \u001b[1m Acurácia: 97.62% \u001b[0m\n",
      "epoch:    0/40000 loss_train: 0.97701498 + 0.00000000 = 0.97701498 loss_val = 0.97701498\n",
      "epoch: 3000/40000 loss_train: 0.38271296 + 0.00000000 = 0.38271296 loss_val = 0.38271296\n",
      "epoch: 6000/40000 loss_train: 0.21626646 + 0.00000000 = 0.21626646 loss_val = 0.21626646\n",
      "epoch: 9000/40000 loss_train: 0.13424105 + 0.00000000 = 0.13424105 loss_val = 0.13424105\n",
      "epoch: 12000/40000 loss_train: 0.10400693 + 0.00000000 = 0.10400693 loss_val = 0.10400693\n",
      "epoch: 15000/40000 loss_train: 0.08728465 + 0.00000000 = 0.08728465 loss_val = 0.08728465\n",
      "epoch: 18000/40000 loss_train: 0.07589922 + 0.00000000 = 0.07589922 loss_val = 0.07589922\n",
      "epoch: 21000/40000 loss_train: 0.06781801 + 0.00000000 = 0.06781801 loss_val = 0.06781801\n",
      "epoch: 24000/40000 loss_train: 0.06279681 + 0.00000000 = 0.06279681 loss_val = 0.06279681\n",
      "Para 8 neurônios, 2 camadas,  0.001 learning rate, 0 batch size, 0.1 dropout rate, temos: \u001b[1m Acurácia: 95.24% \u001b[0m\n",
      "epoch:    0/40000 loss_train: 0.80000034 + 0.00000000 = 0.80000034 loss_val = 0.80000034\n",
      "epoch: 3000/40000 loss_train: 0.69896818 + 0.00000000 = 0.69896818 loss_val = 0.69896818\n",
      "epoch: 6000/40000 loss_train: 0.65401823 + 0.00000000 = 0.65401823 loss_val = 0.65401823\n",
      "epoch: 9000/40000 loss_train: 0.59375717 + 0.00000000 = 0.59375717 loss_val = 0.59375717\n",
      "epoch: 12000/40000 loss_train: 0.41953357 + 0.00000000 = 0.41953357 loss_val = 0.41953357\n",
      "epoch: 15000/40000 loss_train: 0.28627340 + 0.00000000 = 0.28627340 loss_val = 0.28627340\n",
      "epoch: 18000/40000 loss_train: 0.20801065 + 0.00000000 = 0.20801065 loss_val = 0.20801065\n",
      "epoch: 21000/40000 loss_train: 0.12014870 + 0.00000000 = 0.12014870 loss_val = 0.12014870\n",
      "epoch: 24000/40000 loss_train: 0.07787940 + 0.00000000 = 0.07787940 loss_val = 0.07787940\n",
      "epoch: 27000/40000 loss_train: 0.06054046 + 0.00000000 = 0.06054046 loss_val = 0.06054046\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 30000/40000 loss_train: 0.05058964 + 0.00000000 = 0.05058964 loss_val = 0.05058964\n",
      "epoch: 33000/40000 loss_train: 0.04391130 + 0.00000000 = 0.04391130 loss_val = 0.04391130\n",
      "epoch: 36000/40000 loss_train: 0.03898935 + 0.00000000 = 0.03898935 loss_val = 0.03898935\n",
      "epoch: 39000/40000 loss_train: 0.03514223 + 0.00000000 = 0.03514223 loss_val = 0.03514223\n",
      "Para 8 neurônios, 2 camadas,  0.001 learning rate, 4 batch size, 0 dropout rate, temos: \u001b[1m Acurácia: 100.00% \u001b[0m\n",
      "epoch:    0/40000 loss_train: 0.71274042 + 0.00000000 = 0.71274042 loss_val = 0.71274042\n",
      "epoch: 3000/40000 loss_train: 0.58743508 + 0.00000000 = 0.58743508 loss_val = 0.58743508\n",
      "epoch: 6000/40000 loss_train: 0.48415372 + 0.00000000 = 0.48415372 loss_val = 0.48415372\n",
      "epoch: 9000/40000 loss_train: 0.32639490 + 0.00000000 = 0.32639490 loss_val = 0.32639490\n",
      "epoch: 12000/40000 loss_train: 0.20618773 + 0.00000000 = 0.20618773 loss_val = 0.20618773\n",
      "epoch: 15000/40000 loss_train: 0.15136394 + 0.00000000 = 0.15136394 loss_val = 0.15136394\n",
      "epoch: 18000/40000 loss_train: 0.11835992 + 0.00000000 = 0.11835992 loss_val = 0.11835992\n",
      "epoch: 21000/40000 loss_train: 0.09657868 + 0.00000000 = 0.09657868 loss_val = 0.09657868\n",
      "epoch: 24000/40000 loss_train: 0.08201380 + 0.00000000 = 0.08201380 loss_val = 0.08201380\n",
      "epoch: 27000/40000 loss_train: 0.07251788 + 0.00000000 = 0.07251788 loss_val = 0.07251788\n",
      "epoch: 30000/40000 loss_train: 0.06608654 + 0.00000000 = 0.06608654 loss_val = 0.06608654\n",
      "Para 8 neurônios, 2 camadas,  0.001 learning rate, 4 batch size, 0.1 dropout rate, temos: \u001b[1m Acurácia: 97.62% \u001b[0m\n",
      "epoch:    0/40000 loss_train: 0.81084681 + 0.00000000 = 0.81084681 loss_val = 0.81084681\n",
      "epoch: 3000/40000 loss_train: 0.48296085 + 0.00000000 = 0.48296085 loss_val = 0.48296085\n",
      "epoch: 6000/40000 loss_train: 0.18887294 + 0.00000000 = 0.18887294 loss_val = 0.18887294\n",
      "epoch: 9000/40000 loss_train: 0.11420341 + 0.00000000 = 0.11420341 loss_val = 0.11420341\n",
      "epoch: 12000/40000 loss_train: 0.09344029 + 0.00000000 = 0.09344029 loss_val = 0.09344029\n",
      "epoch: 15000/40000 loss_train: 0.08376133 + 0.00000000 = 0.08376133 loss_val = 0.08376133\n",
      "epoch: 18000/40000 loss_train: 0.07758520 + 0.00000000 = 0.07758520 loss_val = 0.07758520\n",
      "epoch: 21000/40000 loss_train: 0.07291370 + 0.00000000 = 0.07291370 loss_val = 0.07291370\n",
      "epoch: 24000/40000 loss_train: 0.06934319 + 0.00000000 = 0.06934319 loss_val = 0.06934319\n",
      "epoch: 27000/40000 loss_train: 0.06648844 + 0.00000000 = 0.06648844 loss_val = 0.06648844\n",
      "epoch: 30000/40000 loss_train: 0.06417239 + 0.00000000 = 0.06417239 loss_val = 0.06417239\n",
      "epoch: 33000/40000 loss_train: 0.06233785 + 0.00000000 = 0.06233785 loss_val = 0.06233785\n",
      "epoch: 36000/40000 loss_train: 0.06020568 + 0.00000000 = 0.06020568 loss_val = 0.06020568\n",
      "epoch: 39000/40000 loss_train: 0.05795854 + 0.00000000 = 0.05795854 loss_val = 0.05795854\n",
      "Para 8 neurônios, 2 camadas,  0.001 learning rate, 8 batch size, 0 dropout rate, temos: \u001b[1m Acurácia: 95.24% \u001b[0m\n",
      "epoch:    0/40000 loss_train: 0.80793486 + 0.00000000 = 0.80793486 loss_val = 0.80793486\n",
      "epoch: 3000/40000 loss_train: 0.60047321 + 0.00000000 = 0.60047321 loss_val = 0.60047321\n",
      "epoch: 6000/40000 loss_train: 0.41889698 + 0.00000000 = 0.41889698 loss_val = 0.41889698\n",
      "epoch: 9000/40000 loss_train: 0.24904855 + 0.00000000 = 0.24904855 loss_val = 0.24904855\n",
      "epoch: 12000/40000 loss_train: 0.14459290 + 0.00000000 = 0.14459290 loss_val = 0.14459290\n",
      "epoch: 15000/40000 loss_train: 0.09743931 + 0.00000000 = 0.09743931 loss_val = 0.09743931\n",
      "epoch: 18000/40000 loss_train: 0.07351416 + 0.00000000 = 0.07351416 loss_val = 0.07351416\n",
      "epoch: 21000/40000 loss_train: 0.06023783 + 0.00000000 = 0.06023783 loss_val = 0.06023783\n",
      "epoch: 24000/40000 loss_train: 0.05139727 + 0.00000000 = 0.05139727 loss_val = 0.05139727\n",
      "epoch: 27000/40000 loss_train: 0.04497715 + 0.00000000 = 0.04497715 loss_val = 0.04497715\n",
      "epoch: 30000/40000 loss_train: 0.03995822 + 0.00000000 = 0.03995822 loss_val = 0.03995822\n",
      "epoch: 33000/40000 loss_train: 0.03614382 + 0.00000000 = 0.03614382 loss_val = 0.03614382\n",
      "epoch: 36000/40000 loss_train: 0.03303871 + 0.00000000 = 0.03303871 loss_val = 0.03303871\n",
      "epoch: 39000/40000 loss_train: 0.03040544 + 0.00000000 = 0.03040544 loss_val = 0.03040544\n",
      "Para 8 neurônios, 2 camadas,  0.001 learning rate, 8 batch size, 0.1 dropout rate, temos: \u001b[1m Acurácia: 97.62% \u001b[0m\n",
      "epoch:    0/40000 loss_train: 0.75896157 + 0.00000000 = 0.75896157 loss_val = 0.75896157\n",
      "epoch: 3000/40000 loss_train: 0.21299013 + 0.00000000 = 0.21299013 loss_val = 0.21299013\n",
      "epoch: 6000/40000 loss_train: 0.05756242 + 0.00000000 = 0.05756242 loss_val = 0.05756242\n",
      "epoch: 9000/40000 loss_train: 0.01850573 + 0.00000000 = 0.01850573 loss_val = 0.01850573\n",
      "epoch: 12000/40000 loss_train: 0.01063092 + 0.00000000 = 0.01063092 loss_val = 0.01063092\n",
      "epoch: 15000/40000 loss_train: 0.00713102 + 0.00000000 = 0.00713102 loss_val = 0.00713102\n",
      "epoch: 18000/40000 loss_train: 0.00516128 + 0.00000000 = 0.00516128 loss_val = 0.00516128\n",
      "epoch: 21000/40000 loss_train: 0.00393177 + 0.00000000 = 0.00393177 loss_val = 0.00393177\n",
      "epoch: 24000/40000 loss_train: 0.00310520 + 0.00000000 = 0.00310520 loss_val = 0.00310520\n",
      "epoch: 27000/40000 loss_train: 0.00252138 + 0.00000000 = 0.00252138 loss_val = 0.00252138\n",
      "epoch: 30000/40000 loss_train: 0.00209601 + 0.00000000 = 0.00209601 loss_val = 0.00209601\n",
      "epoch: 33000/40000 loss_train: 0.00177450 + 0.00000000 = 0.00177450 loss_val = 0.00177450\n",
      "epoch: 36000/40000 loss_train: 0.00152602 + 0.00000000 = 0.00152602 loss_val = 0.00152602\n",
      "epoch: 39000/40000 loss_train: 0.00133029 + 0.00000000 = 0.00133029 loss_val = 0.00133029\n",
      "Para 8 neurônios, 2 camadas,  0.005 learning rate, 0 batch size, 0 dropout rate, temos: \u001b[1m Acurácia: 100.00% \u001b[0m\n",
      "epoch:    0/40000 loss_train: 0.79979043 + 0.00000000 = 0.79979043 loss_val = 0.79979043\n",
      "epoch: 3000/40000 loss_train: 0.17180515 + 0.00000000 = 0.17180515 loss_val = 0.17180515\n",
      "epoch: 6000/40000 loss_train: 0.06904493 + 0.00000000 = 0.06904493 loss_val = 0.06904493\n",
      "epoch: 9000/40000 loss_train: 0.04588589 + 0.00000000 = 0.04588589 loss_val = 0.04588589\n",
      "Para 8 neurônios, 2 camadas,  0.005 learning rate, 0 batch size, 0.1 dropout rate, temos: \u001b[1m Acurácia: 97.62% \u001b[0m\n",
      "epoch:    0/40000 loss_train: 0.82367177 + 0.00000000 = 0.82367177 loss_val = 0.82367177\n",
      "epoch: 3000/40000 loss_train: 0.17928772 + 0.00000000 = 0.17928772 loss_val = 0.17928772\n",
      "epoch: 6000/40000 loss_train: 0.03449937 + 0.00000000 = 0.03449937 loss_val = 0.03449937\n",
      "epoch: 9000/40000 loss_train: 0.01943754 + 0.00000000 = 0.01943754 loss_val = 0.01943754\n",
      "epoch: 12000/40000 loss_train: 0.01289929 + 0.00000000 = 0.01289929 loss_val = 0.01289929\n",
      "epoch: 15000/40000 loss_train: 0.00906616 + 0.00000000 = 0.00906616 loss_val = 0.00906616\n",
      "epoch: 18000/40000 loss_train: 0.00665361 + 0.00000000 = 0.00665361 loss_val = 0.00665361\n",
      "epoch: 21000/40000 loss_train: 0.00506304 + 0.00000000 = 0.00506304 loss_val = 0.00506304\n",
      "epoch: 24000/40000 loss_train: 0.00397215 + 0.00000000 = 0.00397215 loss_val = 0.00397215\n",
      "epoch: 27000/40000 loss_train: 0.00319786 + 0.00000000 = 0.00319786 loss_val = 0.00319786\n",
      "epoch: 30000/40000 loss_train: 0.00263191 + 0.00000000 = 0.00263191 loss_val = 0.00263191\n",
      "epoch: 33000/40000 loss_train: 0.00220749 + 0.00000000 = 0.00220749 loss_val = 0.00220749\n",
      "epoch: 36000/40000 loss_train: 0.00188082 + 0.00000000 = 0.00188082 loss_val = 0.00188082\n",
      "epoch: 39000/40000 loss_train: 0.00162479 + 0.00000000 = 0.00162479 loss_val = 0.00162479\n",
      "Para 8 neurônios, 2 camadas,  0.005 learning rate, 4 batch size, 0 dropout rate, temos: \u001b[1m Acurácia: 100.00% \u001b[0m\n",
      "epoch:    0/40000 loss_train: 1.49306901 + 0.00000000 = 1.49306901 loss_val = 1.49306901\n",
      "epoch: 3000/40000 loss_train: 0.15429951 + 0.00000000 = 0.15429951 loss_val = 0.15429951\n",
      "epoch: 6000/40000 loss_train: 0.06436892 + 0.00000000 = 0.06436892 loss_val = 0.06436892\n",
      "Para 8 neurônios, 2 camadas,  0.005 learning rate, 4 batch size, 0.1 dropout rate, temos: \u001b[1m Acurácia: 97.62% \u001b[0m\n",
      "epoch:    0/40000 loss_train: 0.78851090 + 0.00000000 = 0.78851090 loss_val = 0.78851090\n",
      "epoch: 3000/40000 loss_train: 0.09638812 + 0.00000000 = 0.09638812 loss_val = 0.09638812\n",
      "epoch: 6000/40000 loss_train: 0.07666010 + 0.00000000 = 0.07666010 loss_val = 0.07666010\n",
      "epoch: 9000/40000 loss_train: 0.06836558 + 0.00000000 = 0.06836558 loss_val = 0.06836558\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 12000/40000 loss_train: 0.06327289 + 0.00000000 = 0.06327289 loss_val = 0.06327289\n",
      "epoch: 15000/40000 loss_train: 0.06061143 + 0.00000000 = 0.06061143 loss_val = 0.06061143\n",
      "epoch: 18000/40000 loss_train: 0.05885737 + 0.00000000 = 0.05885737 loss_val = 0.05885737\n",
      "epoch: 21000/40000 loss_train: 0.05749466 + 0.00000000 = 0.05749466 loss_val = 0.05749466\n",
      "epoch: 24000/40000 loss_train: 0.05625829 + 0.00000000 = 0.05625829 loss_val = 0.05625829\n",
      "epoch: 27000/40000 loss_train: 0.05506437 + 0.00000000 = 0.05506437 loss_val = 0.05506437\n",
      "epoch: 30000/40000 loss_train: 0.05389770 + 0.00000000 = 0.05389770 loss_val = 0.05389770\n",
      "epoch: 33000/40000 loss_train: 0.05275378 + 0.00000000 = 0.05275378 loss_val = 0.05275378\n",
      "epoch: 36000/40000 loss_train: 0.05164632 + 0.00000000 = 0.05164632 loss_val = 0.05164632\n",
      "epoch: 39000/40000 loss_train: 0.05059107 + 0.00000000 = 0.05059107 loss_val = 0.05059107\n",
      "Para 8 neurônios, 2 camadas,  0.005 learning rate, 8 batch size, 0 dropout rate, temos: \u001b[1m Acurácia: 97.62% \u001b[0m\n",
      "epoch:    0/40000 loss_train: 0.80720164 + 0.00000000 = 0.80720164 loss_val = 0.80720164\n",
      "epoch: 3000/40000 loss_train: 0.14050110 + 0.00000000 = 0.14050110 loss_val = 0.14050110\n",
      "epoch: 6000/40000 loss_train: 0.06379216 + 0.00000000 = 0.06379216 loss_val = 0.06379216\n",
      "epoch: 9000/40000 loss_train: 0.04215514 + 0.00000000 = 0.04215514 loss_val = 0.04215514\n",
      "Para 8 neurônios, 2 camadas,  0.005 learning rate, 8 batch size, 0.1 dropout rate, temos: \u001b[1m Acurácia: 95.24% \u001b[0m\n",
      "epoch:    0/40000 loss_train: 0.86676100 + 0.00000000 = 0.86676100 loss_val = 0.86676100\n",
      "epoch: 3000/40000 loss_train: 0.03943038 + 0.00000000 = 0.03943038 loss_val = 0.03943038\n",
      "epoch: 6000/40000 loss_train: 0.02032249 + 0.00000000 = 0.02032249 loss_val = 0.02032249\n",
      "epoch: 9000/40000 loss_train: 0.01087461 + 0.00000000 = 0.01087461 loss_val = 0.01087461\n",
      "epoch: 12000/40000 loss_train: 0.00608018 + 0.00000000 = 0.00608018 loss_val = 0.00608018\n",
      "epoch: 15000/40000 loss_train: 0.00377098 + 0.00000000 = 0.00377098 loss_val = 0.00377098\n",
      "epoch: 18000/40000 loss_train: 0.00256696 + 0.00000000 = 0.00256696 loss_val = 0.00256696\n",
      "epoch: 21000/40000 loss_train: 0.00187451 + 0.00000000 = 0.00187451 loss_val = 0.00187451\n",
      "epoch: 24000/40000 loss_train: 0.00144183 + 0.00000000 = 0.00144183 loss_val = 0.00144183\n",
      "epoch: 27000/40000 loss_train: 0.00115292 + 0.00000000 = 0.00115292 loss_val = 0.00115292\n",
      "epoch: 30000/40000 loss_train: 0.00094474 + 0.00000000 = 0.00094474 loss_val = 0.00094474\n",
      "epoch: 33000/40000 loss_train: 0.00079386 + 0.00000000 = 0.00079386 loss_val = 0.00079386\n",
      "epoch: 36000/40000 loss_train: 0.00068026 + 0.00000000 = 0.00068026 loss_val = 0.00068026\n",
      "epoch: 39000/40000 loss_train: 0.00059219 + 0.00000000 = 0.00059219 loss_val = 0.00059219\n",
      "Para 8 neurônios, 2 camadas,  0.01 learning rate, 0 batch size, 0 dropout rate, temos: \u001b[1m Acurácia: 97.62% \u001b[0m\n",
      "epoch:    0/40000 loss_train: 0.97085558 + 0.00000000 = 0.97085558 loss_val = 0.97085558\n",
      "epoch: 3000/40000 loss_train: 0.08555733 + 0.00000000 = 0.08555733 loss_val = 0.08555733\n",
      "Para 8 neurônios, 2 camadas,  0.01 learning rate, 0 batch size, 0.1 dropout rate, temos: \u001b[1m Acurácia: 97.62% \u001b[0m\n",
      "epoch:    0/40000 loss_train: 0.81567574 + 0.00000000 = 0.81567574 loss_val = 0.81567574\n",
      "epoch: 3000/40000 loss_train: 0.06023989 + 0.00000000 = 0.06023989 loss_val = 0.06023989\n",
      "epoch: 6000/40000 loss_train: 0.02441956 + 0.00000000 = 0.02441956 loss_val = 0.02441956\n",
      "epoch: 9000/40000 loss_train: 0.00925674 + 0.00000000 = 0.00925674 loss_val = 0.00925674\n",
      "epoch: 12000/40000 loss_train: 0.00437889 + 0.00000000 = 0.00437889 loss_val = 0.00437889\n",
      "epoch: 15000/40000 loss_train: 0.00256427 + 0.00000000 = 0.00256427 loss_val = 0.00256427\n",
      "epoch: 18000/40000 loss_train: 0.00171046 + 0.00000000 = 0.00171046 loss_val = 0.00171046\n",
      "epoch: 21000/40000 loss_train: 0.00124404 + 0.00000000 = 0.00124404 loss_val = 0.00124404\n",
      "epoch: 24000/40000 loss_train: 0.00095938 + 0.00000000 = 0.00095938 loss_val = 0.00095938\n",
      "epoch: 27000/40000 loss_train: 0.00077220 + 0.00000000 = 0.00077220 loss_val = 0.00077220\n",
      "epoch: 30000/40000 loss_train: 0.00064066 + 0.00000000 = 0.00064066 loss_val = 0.00064066\n",
      "epoch: 33000/40000 loss_train: 0.00054411 + 0.00000000 = 0.00054411 loss_val = 0.00054411\n",
      "epoch: 36000/40000 loss_train: 0.00047059 + 0.00000000 = 0.00047059 loss_val = 0.00047059\n",
      "epoch: 39000/40000 loss_train: 0.00041304 + 0.00000000 = 0.00041304 loss_val = 0.00041304\n",
      "Para 8 neurônios, 2 camadas,  0.01 learning rate, 4 batch size, 0 dropout rate, temos: \u001b[1m Acurácia: 100.00% \u001b[0m\n",
      "epoch:    0/40000 loss_train: 0.88447182 + 0.00000000 = 0.88447182 loss_val = 0.88447182\n",
      "epoch: 3000/40000 loss_train: 0.22835568 + 0.00000000 = 0.22835568 loss_val = 0.22835568\n",
      "Para 8 neurônios, 2 camadas,  0.01 learning rate, 4 batch size, 0.1 dropout rate, temos: \u001b[1m Acurácia: 95.24% \u001b[0m\n",
      "epoch:    0/40000 loss_train: 0.82897319 + 0.00000000 = 0.82897319 loss_val = 0.82897319\n",
      "epoch: 3000/40000 loss_train: 0.03074462 + 0.00000000 = 0.03074462 loss_val = 0.03074462\n",
      "epoch: 6000/40000 loss_train: 0.01151875 + 0.00000000 = 0.01151875 loss_val = 0.01151875\n",
      "epoch: 9000/40000 loss_train: 0.00603559 + 0.00000000 = 0.00603559 loss_val = 0.00603559\n",
      "epoch: 12000/40000 loss_train: 0.00364315 + 0.00000000 = 0.00364315 loss_val = 0.00364315\n",
      "epoch: 15000/40000 loss_train: 0.00243654 + 0.00000000 = 0.00243654 loss_val = 0.00243654\n",
      "epoch: 18000/40000 loss_train: 0.00175478 + 0.00000000 = 0.00175478 loss_val = 0.00175478\n",
      "epoch: 21000/40000 loss_train: 0.00133321 + 0.00000000 = 0.00133321 loss_val = 0.00133321\n",
      "epoch: 24000/40000 loss_train: 0.00105467 + 0.00000000 = 0.00105467 loss_val = 0.00105467\n",
      "epoch: 27000/40000 loss_train: 0.00086049 + 0.00000000 = 0.00086049 loss_val = 0.00086049\n",
      "epoch: 30000/40000 loss_train: 0.00071919 + 0.00000000 = 0.00071919 loss_val = 0.00071919\n",
      "epoch: 33000/40000 loss_train: 0.00061293 + 0.00000000 = 0.00061293 loss_val = 0.00061293\n",
      "epoch: 36000/40000 loss_train: 0.00053074 + 0.00000000 = 0.00053074 loss_val = 0.00053074\n",
      "epoch: 39000/40000 loss_train: 0.00046568 + 0.00000000 = 0.00046568 loss_val = 0.00046568\n",
      "Para 8 neurônios, 2 camadas,  0.01 learning rate, 8 batch size, 0 dropout rate, temos: \u001b[1m Acurácia: 97.62% \u001b[0m\n",
      "epoch:    0/40000 loss_train: 0.87858140 + 0.00000000 = 0.87858140 loss_val = 0.87858140\n",
      "epoch: 3000/40000 loss_train: 0.04340831 + 0.00000000 = 0.04340831 loss_val = 0.04340831\n",
      "Para 8 neurônios, 2 camadas,  0.01 learning rate, 8 batch size, 0.1 dropout rate, temos: \u001b[1m Acurácia: 100.00% \u001b[0m\n",
      "epoch:    0/40000 loss_train: 0.96618248 + 0.00000000 = 0.96618248 loss_val = 0.96618248\n",
      "epoch: 3000/40000 loss_train: 0.69767303 + 0.00000000 = 0.69767303 loss_val = 0.69767303\n",
      "epoch: 6000/40000 loss_train: 0.65200190 + 0.00000000 = 0.65200190 loss_val = 0.65200190\n",
      "epoch: 9000/40000 loss_train: 0.52882960 + 0.00000000 = 0.52882960 loss_val = 0.52882960\n",
      "epoch: 12000/40000 loss_train: 0.33129153 + 0.00000000 = 0.33129153 loss_val = 0.33129153\n",
      "epoch: 15000/40000 loss_train: 0.22143177 + 0.00000000 = 0.22143177 loss_val = 0.22143177\n",
      "epoch: 18000/40000 loss_train: 0.14551255 + 0.00000000 = 0.14551255 loss_val = 0.14551255\n",
      "epoch: 21000/40000 loss_train: 0.10803164 + 0.00000000 = 0.10803164 loss_val = 0.10803164\n",
      "epoch: 24000/40000 loss_train: 0.08810756 + 0.00000000 = 0.08810756 loss_val = 0.08810756\n",
      "epoch: 27000/40000 loss_train: 0.07399298 + 0.00000000 = 0.07399298 loss_val = 0.07399298\n",
      "epoch: 30000/40000 loss_train: 0.06299272 + 0.00000000 = 0.06299272 loss_val = 0.06299272\n",
      "epoch: 33000/40000 loss_train: 0.05394815 + 0.00000000 = 0.05394815 loss_val = 0.05394815\n",
      "epoch: 36000/40000 loss_train: 0.04751422 + 0.00000000 = 0.04751422 loss_val = 0.04751422\n",
      "epoch: 39000/40000 loss_train: 0.04228451 + 0.00000000 = 0.04228451 loss_val = 0.04228451\n",
      "Para 12 neurônios, 2 camadas,  0.001 learning rate, 0 batch size, 0 dropout rate, temos: \u001b[1m Acurácia: 97.62% \u001b[0m\n",
      "epoch:    0/40000 loss_train: 0.83825813 + 0.00000000 = 0.83825813 loss_val = 0.83825813\n",
      "epoch: 3000/40000 loss_train: 0.60504673 + 0.00000000 = 0.60504673 loss_val = 0.60504673\n",
      "epoch: 6000/40000 loss_train: 0.40171496 + 0.00000000 = 0.40171496 loss_val = 0.40171496\n",
      "epoch: 9000/40000 loss_train: 0.23827056 + 0.00000000 = 0.23827056 loss_val = 0.23827056\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 12000/40000 loss_train: 0.15472527 + 0.00000000 = 0.15472527 loss_val = 0.15472527\n",
      "epoch: 15000/40000 loss_train: 0.11538995 + 0.00000000 = 0.11538995 loss_val = 0.11538995\n",
      "epoch: 18000/40000 loss_train: 0.09368492 + 0.00000000 = 0.09368492 loss_val = 0.09368492\n",
      "epoch: 21000/40000 loss_train: 0.08165441 + 0.00000000 = 0.08165441 loss_val = 0.08165441\n",
      "Para 12 neurônios, 2 camadas,  0.001 learning rate, 0 batch size, 0.1 dropout rate, temos: \u001b[1m Acurácia: 97.62% \u001b[0m\n",
      "epoch:    0/40000 loss_train: 1.32116694 + 0.00000000 = 1.32116694 loss_val = 1.32116694\n",
      "epoch: 3000/40000 loss_train: 0.60828856 + 0.00000000 = 0.60828856 loss_val = 0.60828856\n",
      "epoch: 6000/40000 loss_train: 0.44357293 + 0.00000000 = 0.44357293 loss_val = 0.44357293\n",
      "epoch: 9000/40000 loss_train: 0.22844776 + 0.00000000 = 0.22844776 loss_val = 0.22844776\n",
      "epoch: 12000/40000 loss_train: 0.11770244 + 0.00000000 = 0.11770244 loss_val = 0.11770244\n",
      "epoch: 15000/40000 loss_train: 0.08182295 + 0.00000000 = 0.08182295 loss_val = 0.08182295\n",
      "epoch: 18000/40000 loss_train: 0.06581404 + 0.00000000 = 0.06581404 loss_val = 0.06581404\n",
      "epoch: 21000/40000 loss_train: 0.05615318 + 0.00000000 = 0.05615318 loss_val = 0.05615318\n",
      "epoch: 24000/40000 loss_train: 0.04922725 + 0.00000000 = 0.04922725 loss_val = 0.04922725\n",
      "epoch: 27000/40000 loss_train: 0.04413047 + 0.00000000 = 0.04413047 loss_val = 0.04413047\n",
      "epoch: 30000/40000 loss_train: 0.04010858 + 0.00000000 = 0.04010858 loss_val = 0.04010858\n",
      "epoch: 33000/40000 loss_train: 0.03679665 + 0.00000000 = 0.03679665 loss_val = 0.03679665\n",
      "epoch: 36000/40000 loss_train: 0.03385182 + 0.00000000 = 0.03385182 loss_val = 0.03385182\n",
      "epoch: 39000/40000 loss_train: 0.03093596 + 0.00000000 = 0.03093596 loss_val = 0.03093596\n",
      "Para 12 neurônios, 2 camadas,  0.001 learning rate, 4 batch size, 0 dropout rate, temos: \u001b[1m Acurácia: 97.62% \u001b[0m\n",
      "epoch:    0/40000 loss_train: 1.35527751 + 0.00000000 = 1.35527751 loss_val = 1.35527751\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [34]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prob_dropout \u001b[38;5;129;01min\u001b[39;00m dropout_rate:\n\u001b[0;32m      8\u001b[0m     nn \u001b[38;5;241m=\u001b[39m setBestNeuralNetwork(num_hidden_layers,num_neurons_layers, num_learning_rate, prob_dropout, input_dim, output_dim)\n\u001b[1;32m---> 10\u001b[0m     \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m40000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mbatch_gen\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrna\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_shuffle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_batch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mpredict(X_val)\n\u001b[0;32m     13\u001b[0m     accu \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m\u001b[38;5;241m*\u001b[39maccuracy_score(y_val, y_pred \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.5\u001b[39m)\n",
      "File \u001b[1;32mC:\\Caio\\DevProjects\\Python\\academico\\doutorado\\tese\\paper_susceptibility_map\\Rede_Neural.py:383\u001b[0m, in \u001b[0;36mNeuralNetwork.fit\u001b[1;34m(self, x_train, y_train, x_val, y_val, epochs, verbose, batch_gen, batch_size)\u001b[0m\n\u001b[0;32m    381\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__backprop(y_batch,y_pred)\n\u001b[0;32m    382\u001b[0m \u001b[38;5;66;03m#early stopping \u001b[39;00m\n\u001b[1;32m--> 383\u001b[0m loss_val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcost_func(y_val, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_val\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;66;03m#calcula perda em cada epoch\u001b[39;00m\n\u001b[0;32m    384\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m loss_val \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_best_loss:   \u001b[38;5;66;03m#melhorou?\u001b[39;00m\n\u001b[0;32m    385\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_best_model, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_best_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers, loss_val\n",
      "File \u001b[1;32mC:\\Caio\\DevProjects\\Python\\academico\\doutorado\\tese\\paper_susceptibility_map\\Rede_Neural.py:401\u001b[0m, in \u001b[0;36mNeuralNetwork.predict\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    400\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m,x):\n\u001b[1;32m--> 401\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__feedforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_training\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m  \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Caio\\DevProjects\\Python\\academico\\doutorado\\tese\\paper_susceptibility_map\\Rede_Neural.py:418\u001b[0m, in \u001b[0;36mNeuralNetwork.__feedforward\u001b[1;34m(self, x, is_training)\u001b[0m\n\u001b[0;32m    416\u001b[0m y \u001b[38;5;241m=\u001b[39m batchnorm_forward(current_layer, y, is_training) \u001b[38;5;28;01mif\u001b[39;00m current_layer\u001b[38;5;241m.\u001b[39mbatch_norm \u001b[38;5;28;01melse\u001b[39;00m y\n\u001b[0;32m    417\u001b[0m \u001b[38;5;66;03m#cria uma máscara ativação (liga/desliga) dos neuronios baseado na probabilidade além de reescalar as ativações para cada iteração.\u001b[39;00m\n\u001b[1;32m--> 418\u001b[0m current_layer\u001b[38;5;241m.\u001b[39m_dropout_mask \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbinomial\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcurrent_layer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout_prob\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m-\u001b[39m current_layer\u001b[38;5;241m.\u001b[39mdropout_prob)\n\u001b[0;32m    419\u001b[0m current_layer\u001b[38;5;241m.\u001b[39m_activ_inp \u001b[38;5;241m=\u001b[39m y\n\u001b[0;32m    421\u001b[0m \u001b[38;5;66;03m#calculo da f. ativacao (saida da atual é a entrada da próxima) & aplica máscara de \u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lst_hyperP = []\n",
    "lst_ANN = []\n",
    "for num_hidden_layers in hidden_layers:\n",
    "    for num_neurons_layers in neurons:\n",
    "        for num_learning_rate in learning_rate:\n",
    "            for num_batch_size in batch_size:\n",
    "                for prob_dropout in dropout_rate:\n",
    "                    nn = setBestNeuralNetwork(num_hidden_layers,num_neurons_layers, num_learning_rate, prob_dropout, input_dim, output_dim)\n",
    "                    \n",
    "                    nn.fit(X_train, y_train, epochs=40000,batch_gen= rna.batch_shuffle, batch_size=num_batch_size, verbose=3000)\n",
    "                \n",
    "                    y_pred = nn.predict(X_val)\n",
    "                    accu = 100*accuracy_score(y_val, y_pred > 0.5)\n",
    "                    #accu = 100*roc_auc_score(y_val, y_pred > 0.5)\n",
    "                    lst_hyperP.append(hyperparametros(accu,num_hidden_layers,num_neurons_layers,num_learning_rate,num_batch_size,prob_dropout) )\n",
    "                    lst_ANN.append(ANN_accuracy(accu,nn))\n",
    "                    print('Para {} neurônios, {} camadas,  {} learning rate, {} batch size, {} dropout rate, temos: \\033[1m Acurácia: {:.2f}% \\033[0m'.format(num_neurons_layers,num_hidden_layers,num_learning_rate,num_batch_size,prob_dropout, accu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hyper = max(lst_hyperP, key=lambda hyper: hyper.acuracia)\n",
    "best_of_best_hyper.append( hyperparametros(best_hyper.acuracia,best_hyper.camadas,best_hyper.neuronios, best_hyper.learning_rate,best_hyper.batch_size,best_hyper.dropout) )\n",
    "print('{:.2f}% foi a melhor\\033[1m acurácia\\033[0m, obtida com {} camadas,{} neurônios, {} de learning rate e {} de batch size!'.format(best_hyper.acuracia,best_hyper.camadas,best_hyper.neuronios, best_hyper.learning_rate,best_hyper.batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_ann = max(lst_ANN, key=lambda hyper: hyper.acuracia)\n",
    "best_of_best_ANN.append(ANN_accuracy(best_ann.acuracia,best_ann.ann))\n",
    "print('{:.2f}% foi a melhor\\033[1m acurácia\\033[0m, obtida com a rede {} '.format(best_ann.acuracia,best_ann.ann))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "lst_hyperP.sort(key = lambda hyper: hyper.acuracia)\n",
    "#[hyper.neuronios for hyper in lst_hyperP]\n",
    "for best_hyper in lst_hyperP: \n",
    "  print('{:.2f}% foi a melhor\\033[1m acurácia\\033[0m, obtida com {} camadas,{} neurônios, {} de learning rate, {} de batch size e {} dropout rate!'.format(best_hyper.acuracia,best_hyper.camadas,best_hyper.neuronios, best_hyper.learning_rate,best_hyper.batch_size,best_hyper.dropout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_hyperP = []\n",
    "lst_ANN = []\n",
    "\n",
    "#for num_learning_rate in learning_rate:\n",
    "#    for num_batch_size in batch_size:\n",
    "#        nn = setBestNeuralNetwork2(num_learning_rate, input_dim, output_dim)\n",
    "\n",
    "#        nn.fit(X_train, y_train, epochs=46000,batch_gen= rna.batch_shuffle, batch_size=num_batch_size, verbose=3000)\n",
    "\n",
    "#        y_pred = nn.predict(X_val)\n",
    "#        accu = 100*accuracy_score(y_val, y_pred > 0.5)\n",
    "        #accu = 100*roc_auc_score(y_val, y_pred > 0.5)\n",
    "#        lst_hyperP.append( hyperparametros(accu,2,10,num_learning_rate,num_batch_size,0) )\n",
    "#        lst_ANN.append(ANN_accuracy(accu,nn))\n",
    "#        print('Para  {} learning rate, {} batch size, temos: \\033[1m Acurácia: {:.2f}% \\033[0m'.format(num_learning_rate,num_batch_size, accu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#best_hyper = max(lst_hyperP, key=lambda hyper: hyper.acuracia)\n",
    "#best_of_best_hyper.append( hyperparametros(best_hyper.acuracia,best_hyper.camadas,best_hyper.neuronios, best_hyper.learning_rate,best_hyper.batch_size,best_hyper.dropout) )\n",
    "#print('{:.2f}% foi a melhor\\033[1m acurácia\\033[0m, obtida com {} camadas,{} neurônios, {} de learning rate e {} de batch size!'.format(best_hyper.acuracia,best_hyper.camadas,best_hyper.neuronios, best_hyper.learning_rate,best_hyper.batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#best_ann2 = max(lst_ANN, key=lambda hyper: hyper.acuracia)\n",
    "#best_of_best_ANN.append(ANN_accuracy(best_ann2.acuracia,best_ann2.ann))\n",
    "#print('{:.2f}% foi a melhor\\033[1m acurácia\\033[0m, obtida com a rede {} '.format(best_ann2.acuracia,best_ann2.ann))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lst_hyperP.sort(key = lambda hyper: hyper.acuracia)\n",
    "#[hyper.neuronios for hyper in lst_hyperP]\n",
    "#for best_hyper in lst_hyperP: \n",
    "#  print('{:.2f}% foi a melhor\\033[1m acurácia\\033[0m, obtida com {} camadas, {} de learning rate, {} de batch size e {} dropout rate!'.format(best_hyper.acuracia,best_hyper.camadas, best_hyper.learning_rate,best_hyper.batch_size,best_hyper.dropout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lst_hyperP = []\n",
    "#lst_ANN = []\n",
    "\n",
    "#for num_learning_rate in learning_rate:\n",
    "#    for num_batch_size in batch_size:\n",
    "#        nn = setBestNeuralNetwork3(num_learning_rate, input_dim, output_dim)\n",
    "\n",
    "#        nn.fit(X_train, y_train, epochs=36000,batch_gen= rna.batch_shuffle, batch_size=num_batch_size, verbose=3000)\n",
    "\n",
    "#        y_pred = nn.predict(X_val)\n",
    "#        accu = 100*accuracy_score(y_val, y_pred > 0.5)\n",
    "#        lst_hyperP.append( hyperparametros(accu,2,20,num_learning_rate,num_batch_size,0) )\n",
    "#        lst_ANN.append(ANN_accuracy(accu,nn))\n",
    "#        print('Para  {} learning rate, {} batch size, temos: \\033[1m Acurácia: {:.2f}% \\033[0m'.format(num_learning_rate,num_batch_size, accu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#best_hyper = max(lst_hyperP, key=lambda hyper: hyper.acuracia)\n",
    "#best_of_best_hyper.append( hyperparametros(best_hyper.acuracia,best_hyper.camadas,best_hyper.neuronios, best_hyper.learning_rate,best_hyper.batch_size,best_hyper.dropout) )\n",
    "#print('{:.2f}% foi a melhor\\033[1m acurácia\\033[0m, obtida com {} camadas,{} neurônios, {} de learning rate e {} de batch size!'.format(best_hyper.acuracia,best_hyper.camadas,best_hyper.neuronios, best_hyper.learning_rate,best_hyper.batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#best_ann3 = max(lst_ANN, key=lambda hyper: hyper.acuracia)\n",
    "#best_of_best_ANN.append(ANN_accuracy(best_ann3.acuracia,best_ann3.ann))\n",
    "#print('{:.2f}% foi a melhor\\033[1m acurácia\\033[0m, obtida com a rede {} '.format(best_ann3.acuracia,best_ann3.ann))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lst_hyperP.sort(key = lambda hyper: hyper.acuracia)\n",
    "#[hyper.neuronios for hyper in lst_hyperP]\n",
    "#for best_hyper in lst_hyperP: \n",
    "#  print('{:.2f}% foi a melhor\\033[1m acurácia\\033[0m, obtida com {} camadas, {} de learning rate, {} de batch size e {} dropout rate!'.format(best_hyper.acuracia,best_hyper.camadas, best_hyper.learning_rate,best_hyper.batch_size,best_hyper.dropout))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Avaliação da rede Proprietária"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://medium.com/mdr-inc/from-sgd-to-adam-c9fce513c4bb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_of_best_ANN_Test = []\n",
    "for best_RNA in best_of_best_ANN: \n",
    "  bestNN = best_RNA.ann\n",
    "  yhat_probs = bestNN.predict(X_test)\n",
    "  yhat_classes = (yhat_probs > 0.5)\n",
    "  accu = 100*accuracy_score(y_test,yhat_classes)\n",
    "  accMedia = (accu + best_RNA.acuracia) / 2\n",
    "  print('{:.2f}% accu_test - {:.2f}% foi a melhor\\033[1m acurácia\\033[0m, {:.2f} acc média {} RNA!'.format(accu,best_RNA.acuracia,accMedia,best_RNA.ann))\n",
    "  accu = accMedia  \n",
    "  best_of_best_ANN_Test.append(ANN_accuracy(accu,bestNN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for best_hyper in best_of_best_hyper: \n",
    "  print('{:.2f}% foi a melhor\\033[1m acurácia\\033[0m, obtida com {} camadas,{} neurônios, {} de learning rate, {} de batch size e {} dropout rate!'.format(best_hyper.acuracia,best_hyper.camadas,best_hyper.neuronios, best_hyper.learning_rate,best_hyper.batch_size,best_hyper.dropout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestNN_Test = max(best_of_best_ANN_Test, key=lambda hyper: hyper.acuracia)\n",
    "bestNN_Test = bestNN_Test.ann"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "yhat_probs = bestNN_Test.predict(X_test)\n",
    "yhat_classes = (yhat_probs > 0.5)\n",
    "accu = 100*accuracy_score(y_test,yhat_classes)\n",
    "print('{:.2f}%, acurácia de teste !'.format(accu))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Segunda abordagem - configura uma nova rede com os parametros obtidos da otimização por força bruta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestNN = max(best_of_best_ANN_Test, key=lambda hyper: hyper.acuracia)\n",
    "bestNN = bestNN.ann"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestHyper = max(best_of_best_hyper, key=lambda hyper: hyper.acuracia)\n",
    "print('{:.2f}% foi a melhor\\033[1m acurácia\\033[0m, obtida com {} camadas, {} de learning rate, {} de batch size e {} dropout rate!'.format(bestHyper.acuracia,bestHyper.camadas, bestHyper.learning_rate,bestHyper.batch_size,bestHyper.dropout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_probs = bestNN.predict(X_test)\n",
    "yhat_classes = (yhat_probs > 0.5)\n",
    "accu = 100*accuracy_score(y_test,yhat_classes)\n",
    "print('{:.2f}%, acurácia de teste !'.format(accu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "bstANN = bestNN\n",
    "#bst_hyper = bstANN.hyper\n",
    "print('{:.2f}% foi a melhor\\033[1m acurácia\\033[0m, obtida com {} camadas, {} de learning rate, {} de batch size e {} dropout rate!'.format(best_hyper.acuracia,best_hyper.camadas, best_hyper.learning_rate,best_hyper.batch_size,best_hyper.dropout))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Quando obtermos uma boa acurácia com os dados de teste, salvamos a rede devidamente treinada__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if accu >= 85:\n",
    "    print('{:.2f}%, ótima acurácia de teste, rede salva!'.format(accu))\n",
    "    bestNN.save('bestANN\\suscetibilidadeNN_guaruja.pkl')\n",
    "else:\n",
    "    print('Acurácia: {:.2f}% acurácia de validação abaixo de .85!'.format(accu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = bestNN.predict(X_train)\n",
    "print('Acurácia: {:.2f}%'.format(100*accuracy_score(y_train, y_pred > 0.5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = bestNN.predict(X_val)\n",
    "print('Acurácia: {:.2f}%'.format(100*accuracy_score(y_val, y_pred > 0.5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = bestNN.predict(X_test)\n",
    "print('Acurácia: {:.2f}%'.format(100*accuracy_score(y_test, y_pred > 0.5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = bestNN.predict(X)\n",
    "print('Acurácia: {:.2f}%'.format(100*accuracy_score(y, y_pred > 0.5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Quando obtermos uma boa acurácia com os dados de teste, salvamos a rede devidamente treinada__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_probs = bestNN.predict(X_train)\n",
    "yhat_classes = (yhat_probs > 0.5)\n",
    "accu = 100*accuracy_score(y_train, yhat_classes)\n",
    "print('Acurácia: {:.2f}%'.format(accu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NetworkPerformance(y_train, yhat_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_probs = bestNN.predict(X_test)\n",
    "yhat_classes = (yhat_probs > 0.5)\n",
    "accu = 100*accuracy_score(y_test,yhat_classes)\n",
    "print('Acurácia: {:.2f}%'.format(accu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NetworkPerformance(y_test, yhat_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_probs = bestNN.predict(X_val)\n",
    "yhat_classes = (yhat_probs > 0.5)\n",
    "accu = 100*accuracy_score(y_val,yhat_classes)\n",
    "print('Acurácia: {:.2f}%'.format(accu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NetworkPerformance(y_val, yhat_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_probs = bestNN.predict(X)\n",
    "yhat_classes = (yhat_probs > 0.5)\n",
    "accu = 100*accuracy_score(y,yhat_classes)\n",
    "print('Acurácia: {:.2f}%'.format(accu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "NetworkPerformance(y, yhat_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printAUROC(y,yhat_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cf_matrix = confusion_matrix(y,(yhat_probs > 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "ax = sns.heatmap(cf_matrix, annot=True, cmap='Blues')\n",
    "\n",
    "ax.set_title('Matriz de Confusão\\n\\n')\n",
    "ax.set_xlabel('Ocorrências Preditas\\nAcurácia={:0.2f}%'.format(100*accuracy_score(y, y_pred > 0.5)))\n",
    "ax.set_ylabel('Ocorrências Reais ')\n",
    "\n",
    "\n",
    "## Ticket labels - List must be in alphabetical order\n",
    "ax.xaxis.set_ticklabels(['Falso','Verdadeiro'])\n",
    "ax.yaxis.set_ticklabels(['Falso','Verdadeiro'])\n",
    "\n",
    "## Display the visualization of the Confusion Matrix.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_proprietario = None\n",
    "output_proprietario = dataset_original\n",
    "output_proprietario['score'] = yhat_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes = output_proprietario['Elevacao']*10\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(14, 9))\n",
    "\n",
    "axes[0].scatter(output_proprietario['Declividade'], output_proprietario['Elevacao'],\n",
    "            c=(output_proprietario['class']), cmap='rainbow', alpha=0.3,\n",
    "            s=sizes, edgecolors='none')\n",
    "legend1 = axes[0].legend(*scatter.legend_elements(),\n",
    "                    loc=\"upper right\", title=\"landslide\")\n",
    "\n",
    "axes[1].scatter(output_proprietario['Declividade'], output_proprietario['Elevacao'],\n",
    "            c=(output_proprietario['score'] > 0.5), cmap='rainbow', alpha=0.3,\n",
    "            s=sizes, edgecolors='none')\n",
    "legend1 = axes[1].legend(*scatter.legend_elements(),\n",
    "                    loc=\"upper right\", title=\"landslide\")\n",
    "\n",
    "fig.tight_layout()\n",
    "text = 'deslizamentos preditos ' + 'acurácia de: {:.2f}%'.format(100*accuracy_score(y, y_pred > 0.5))\n",
    "axes[0].title.set_text('deslizamentos reais')\n",
    "axes[1].title.set_text(text)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora considerar __RepeatedStratifiedKfold__ para avaliação da rede\n",
    "\n",
    "ref. https://www.geeksforgeeks.org/stratified-k-fold-cross-validation/\n",
    "\n",
    "ref. https://medium.com/@venkatasujit272/overview-of-cross-validation-3785d5414ece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Teste com objeto StratifiedKFold.\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "import statistics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X = dataset.drop('class',axis=1)\n",
    "#y= dataset['class']\n",
    "#X = X.to_numpy()    #converts dataframe into array to be used at NN\n",
    "#y = y.to_numpy()    #converts dataframe into array to be used at NN\n",
    "#y = y.reshape(-1,1) #reorganiza o array em um array 1 x 1\n",
    "\n",
    "#normalização do dataset\n",
    "#minmax = MinMaxScaler(feature_range=(-1, 1))\n",
    "#minmax = MinMaxScaler()\n",
    "#X = minmax.fit_transform(X.astype(np.float64))\n",
    "print(X.min(axis=0), X.max(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n",
    "lst_accu_stratified = []\n",
    "\n",
    "rskf = RepeatedStratifiedKFold(n_splits=5, n_repeats=2,random_state=36851234)\n",
    "\n",
    "print ('Numero de Splits_stratified de X: ',rskf.get_n_splits(X, y),'\\n')\n",
    "\n",
    "# Prin_stratifiedting the Train & Test Indices of splits\n",
    "for train_index, test_index in rskf.split(X, y): \n",
    "    #print (\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train_stratified, X_test_stratified = X[train_index], X[test_index]\n",
    "    y_train_stratified, y_test_stratified = y[train_index], y[test_index]\n",
    "    print(X_train_stratified.shape, y_train_stratified.shape)\n",
    "    \n",
    "    yhat_probs = bestNN.predict(X_test_stratified)\n",
    "    lst_accu_stratified.append(100*accuracy_score(y_test_stratified, yhat_probs > 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Lista de acurácias obtidas:', lst_accu_stratified)\n",
    "print('\\nAcurácia máxima obtida:',\n",
    "      max(lst_accu_stratified), '%')\n",
    "print('\\nAcurácia mínima:',\n",
    "      min(lst_accu_stratified), '%')\n",
    "print('\\nAcurácia média:',\n",
    "      statistics.mean(lst_accu_stratified), '%')\n",
    "print('\\nDesvio Padrão:', statistics.stdev(lst_accu_stratified))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recupera modelo otimizado saldo anteriormente e o testa com diferentes conjuntos de dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_pred = bestNN.predict(X)\n",
    "yhat_classes = (y_pred > 0.5)\n",
    "print('Acurácia: {:.2f}%'.format(100*accuracy_score(y, y_pred > 0.5)))\n",
    "output_proprietario = None\n",
    "output_proprietario = dataset_original\n",
    "output_proprietario['score'] = y_pred\n",
    "output_proprietario.to_csv('bestANN/mapaSuscetibilidade_proprietaria.csv', encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NetworkPerformance(y, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_proprietario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot\n",
    "ns_auc = roc_auc_score(y, y_pred)\n",
    "# summarize scores\n",
    "print('ROC AUC=%.3f' % (ns_auc))\n",
    "# calculate roc curves\n",
    "ns_fpr, ns_tpr, _ = roc_curve(y, y_pred)\n",
    "# plot the roc curve for the model\n",
    "pyplot.plot(ns_fpr, ns_tpr, linestyle='--', label='ROC AUC')\n",
    "# axis labels\n",
    "pyplot.xlabel('False Positive Rate')\n",
    "pyplot.ylabel('True Positive Rate')\n",
    "# show the legend\n",
    "pyplot.legend()\n",
    "# show the plot\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ref. https://machinelearningmastery.com/roc-curves-and-precision-recall-curves-for-classification-in-python/\n",
    "- ref. https://stackoverflow.com/questions/25009284/how-to-plot-roc-curve-in-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes = output_proprietario['Elevacao']*10\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(14, 9))\n",
    "\n",
    "axes[0].scatter(output_proprietario['Declividade'], output_proprietario['Elevacao'],\n",
    "            c=(output_proprietario['class']), cmap='rainbow', alpha=0.3,\n",
    "            s=sizes, edgecolors='none')\n",
    "legend1 = axes[0].legend(*scatter.legend_elements(),\n",
    "                    loc=\"upper right\", title=\"landslide\")\n",
    "\n",
    "axes[1].scatter(output_proprietario['Declividade'], output_proprietario['Elevacao'],\n",
    "            c=(output_proprietario['score'] > 0.5), cmap='rainbow', alpha=0.3,\n",
    "            s=sizes, edgecolors='none')\n",
    "legend1 = axes[1].legend(*scatter.legend_elements(),\n",
    "                    loc=\"upper right\", title=\"landslide\")\n",
    "\n",
    "fig.tight_layout()\n",
    "text = 'deslizamentos preditos ' + 'acurácia de: {:.2f}%'.format(100*accuracy_score(y, y_pred > 0.5))\n",
    "axes[0].title.set_text('deslizamentos reais')\n",
    "axes[1].title.set_text(text)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataSet = output_proprietario.sample(n = 50)\n",
    "#dataSet = dataSet.drop('FID',axis=1)\n",
    "#dataSet = dataSet.drop('X',axis=1)\n",
    "#dataSet = dataSet.drop('Y',axis=1)\n",
    "plotSwarmChart(dataSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perguntar para Ale como retornar os dados para ela, se em um csv com cada posição e as respectivas probabilidades?\n",
    "# se for, basta incorporar essa variável y_pred à uma nova coluna do dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end = datetime.datetime.now()\n",
    "print(end-start)\n",
    "quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rede Neural  Keras\n",
    "__Problema de classificação, considerar as melhores práticas:__\n",
    "- Ajuste dos hiperparametros\n",
    "- Baseline para implementação customizada\n",
    "\n",
    "ref. https://machinelearningmastery.com/data-preparation-without-data-leakage/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora, configura a rede com os melhores parâmetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(X_train), 'train examples')\n",
    "print(len(X_val), 'validation examples')\n",
    "print(len(X_test), 'test examples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#{'num_hidden_layers': 1, 'num_units': 4, 'dropout_rate': 0.5, 'learning_rate': 0.004682800657889146}       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HyperParameters: {'num_hidden_layers': 2, 'num_units': 12, 'dropout_rate': 0.2821478566400208, 'learning_rate': 0.006549533067877217}\n",
    "#conjunto de hyperparametros para o Guaruja 87% acuracia dados de teste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypermodel = keras.Sequential()\n",
    "hypermodel.add(Dense(24, activation='relu', input_dim=input_dim))\n",
    "\n",
    "hypermodel.add(Dense(24, activation='relu'))\n",
    "hypermodel.add(Dropout(0.43719886068153724))\n",
    "\n",
    "#hypermodel.add(Dense(24, activation='relu'))\n",
    "#hypermodel.add(Dropout(0.5))\n",
    "#hypermodel.add(Dense(24, activation='relu'))\n",
    "#hypermodel.add(Dropout(0.5))\n",
    "#hypermodel.add(Dense(24, activation='relu'))\n",
    "#hypermodel.add(Dropout(0.5))\n",
    "#hypermodel.add(Dense(24, activation='relu'))\n",
    "#hypermodel.add(Dropout(0.5))\n",
    "#hypermodel.add(Dense(24, activation='relu'))\n",
    "#hypermodel.add(Dropout(0.5))\n",
    "#hypermodel.add(Dense(24, activation='relu'))\n",
    "#hypermodel.add(Dropout(0.5))\n",
    "#hypermodel.add(Dense(24, activation='relu'))\n",
    "#hypermodel.add(Dropout(0.5))\n",
    "\n",
    "hypermodel.add(Dense(1, activation='sigmoid'))\n",
    "learning_rate= 0.01\n",
    "myOptimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "hypermodel.compile(optimizer=myOptimizer, loss=\"binary_crossentropy\", metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.fit(X_train, y_train, epochs=100, batch_size=5)\n",
    "history           = hypermodel.fit(X_train, y_train, epochs=500, batch_size=5)\n",
    "val_acc_per_epoch = history.history['accuracy']\n",
    "best_epoch        = val_acc_per_epoch.index(max(val_acc_per_epoch)) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Best epoch: %d' % (best_epoch,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# avaliação via keras\n",
    "_, accuracy = hypermodel.evaluate(X_test, y_test)\n",
    "print('Accuracy: %.2f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrain the model\n",
    "history = hypermodel.fit(X_train, y_train, epochs=best_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# avaliação via keras\n",
    "_, accuracy = hypermodel.evaluate(X_test, y_test)\n",
    "print('Accuracy: %.2f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# rankdir='LR' is used to make the graph horizontal.\n",
    "tf.keras.utils.plot_model(hypermodel, show_shapes=True, rankdir=\"LR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if accuracy >= .80:\n",
    "    print('ótima acurácia, rede salva!')\n",
    "    hypermodel.save('bestANN\\suscetibilidadeKeras_guaruja.pkl')\n",
    "else:\n",
    "    print('acurácia abaixo de .85')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Avaliação da rede keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X = dataset.drop('class',axis=1)\n",
    "#y = dataset['class']\n",
    "#X = X.to_numpy()    #converts dataframe into array to be used at NN\n",
    "#y = y.to_numpy()    #converts dataframe into array to be used at NN\n",
    "#y = y.reshape(-1,1) #reorganiza o array em um array 1 x 1\n",
    "\n",
    "#normalização do dataset\n",
    "#minmax = MinMaxScaler(feature_range=(-1, 1))\n",
    "#X = minmax.fit_transform(X.astype(np.float64))\n",
    "#print(X.min(axis=0), X.max(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n",
    "lst_accu_stratified = []\n",
    "\n",
    "rskf = RepeatedStratifiedKFold(n_splits=5, n_repeats=2,random_state=36851234)\n",
    "\n",
    "print ('Numero de Splits de X: ',rskf.get_n_splits(X, y),'\\n')\n",
    "\n",
    "# Printing the Train & Test Indices of splits\n",
    "for train_index, test_index in rskf.split(X, y): \n",
    "    #print (\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train_stratified_keras, X_test_stratified_keras = X[train_index], X[test_index]\n",
    "    y_train_stratified_keras, y_test_stratified_keras = y[train_index], y[test_index]\n",
    "    #print('novo dataset')\n",
    "    #print(X_train_stratified_keras.shape, y_train_stratified_keras.shape)\n",
    "    \n",
    "    yhat_probs = hypermodel.predict(X_test_stratified_keras)\n",
    "    lst_accu_stratified.append(100*accuracy_score(y_test_stratified_keras, yhat_probs > 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Lista de acurácias obtidas:', lst_accu_stratified)\n",
    "print('\\nAcurácia máxima obtida:',\n",
    "      max(lst_accu_stratified), '%')\n",
    "print('\\nAcurácia mínima:',\n",
    "      min(lst_accu_stratified), '%')\n",
    "print('\\nAcurácia média:',\n",
    "      statistics.mean(lst_accu_stratified), '%')\n",
    "print('\\nDesvio Padrão:', statistics.stdev(lst_accu_stratified))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = hypermodel.predict(X_test)\n",
    "#print('Predições:', y_pred, sep='\\n')\n",
    "print('Acurácia: {:.2f}%'.format(100*accuracy_score(y_test, y_pred > 0.5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_pred = hypermodel.predict(X_train)\n",
    "print('Acurácia: {:.2f}%'.format(100*accuracy_score(y_train, y_pred > 0.5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = hypermodel.predict(X_val)\n",
    "print('Acurácia: {:.2f}%'.format(100*accuracy_score(y_val, y_pred > 0.5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = hypermodel.predict(X)\n",
    "print('Acurácia: {:.2f}%'.format(100*accuracy_score(y, y_pred > 0.5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "output_keras = None\n",
    "output_keras = dataset_original\n",
    "output_keras['score'] = y_pred\n",
    "output_keras.to_csv('bestANN/mapaSuscetibilidade_keras.csv', encoding=\"utf-8\")\n",
    "output_keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ns_auc = roc_auc_score(y, y_pred)\n",
    "# summarize scores\n",
    "print('ROC AUC=%.3f' % (ns_auc))\n",
    "# calculate roc curves\n",
    "ns_fpr, ns_tpr, _ = roc_curve(y, y_pred)\n",
    "# plot the roc curve for the model\n",
    "pyplot.plot(ns_fpr, ns_tpr, linestyle='--', label='ROC AUC')\n",
    "# axis labels\n",
    "pyplot.xlabel('False Positive Rate')\n",
    "pyplot.ylabel('True Positive Rate')\n",
    "# show the legend\n",
    "pyplot.legend()\n",
    "# show the plot\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes = output_proprietario['Elevacao']*10\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(14, 9))\n",
    "\n",
    "axes[0].scatter(output_proprietario['Declividade'], output_proprietario['Elevacao'],\n",
    "            c=(output_proprietario['class']), cmap='rainbow', alpha=0.3,\n",
    "            s=sizes, edgecolors='none')\n",
    "legend1 = axes[0].legend(*scatter.legend_elements(),\n",
    "                    loc=\"upper right\", title=\"landslide\")\n",
    "\n",
    "axes[1].scatter(output_proprietario['Declividade'], output_proprietario['Elevacao'],\n",
    "            c=(output_proprietario['score'] > 0.5), cmap='rainbow', alpha=0.3,\n",
    "            s=sizes, edgecolors='none')\n",
    "legend1 = axes[1].legend(*scatter.legend_elements(),\n",
    "                    loc=\"upper right\", title=\"landslide\")\n",
    "\n",
    "fig.tight_layout()\n",
    "text = 'deslizamentos preditos ' + 'acurácia de: {:.2f}%'.format(100*accuracy_score(y, y_pred > 0.5))\n",
    "axes[0].title.set_text('deslizamentos reais')\n",
    "axes[1].title.set_text(text)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataSet = output_keras.sample(n = 50)\n",
    "#dataSet = dataSet.drop('FID',axis=1)\n",
    "#dataSet = dataSet.drop('X',axis=1)\n",
    "#dataSet = dataSet.drop('Y',axis=1)\n",
    "plotSwarmChart(dataSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end = datetime.datetime.now()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b7e61c40c58399eabfdc57275b655b0084201ad0151cf37bc45bab8193ea43c1"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
