{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras \n",
    "import Rede_Neural as rna\n",
    "from Rede_Neural import NeuralNetwork\n",
    "from Rede_Neural import Layer\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import pyplot\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from utils import plot\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential # type: ignore\n",
    "from tensorflow.keras.layers import Dense # type: ignore\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from keras.layers import Input\n",
    "\n",
    "#from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from scikeras.wrappers import KerasClassifier  \n",
    "#from sklearn.wrappers import KerasClassifier\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "import pickle\n",
    "import joblib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_highly_correlated_columns(data, threshold=0.9):\n",
    "    \"\"\"\n",
    "    Remove columns from a NumPy array where the correlation coefficient\n",
    "    is greater than a specified threshold.\n",
    "\n",
    "    Parameters:\n",
    "    - data: np.ndarray, the input array.\n",
    "    - threshold: float, the correlation coefficient threshold.\n",
    "\n",
    "    Returns:\n",
    "    - np.ndarray, the array with highly correlated columns removed.\n",
    "    \"\"\"\n",
    "    if not isinstance(data, np.ndarray):\n",
    "        raise ValueError(\"Input data must be a NumPy array.\")\n",
    "    \n",
    "    corr_matrix = np.corrcoef(data, rowvar=False)\n",
    "    to_remove = set()\n",
    "    \n",
    "    for i in range(corr_matrix.shape[0]):\n",
    "        for j in range(i+1, corr_matrix.shape[0]):\n",
    "            if abs(corr_matrix[i, j]) > threshold:\n",
    "                to_remove.add(j)\n",
    "\n",
    "    columns_to_keep = [i for i in range(data.shape[1]) if i not in to_remove]\n",
    "    filtered_data = data[:, columns_to_keep]\n",
    "\n",
    "    return filtered_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataprep_full(dataset, target, split = True):\n",
    "    X_train, X_test, y_train, y_test = None, None, None, None\n",
    "    \n",
    "    pearson_threshold = 0.9\n",
    "    # Create a pipeline for standardization and PCA\n",
    "    pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('pca', PCA(n_components=0.95))\n",
    "    ])\n",
    "\n",
    "    dataset = remove_highly_correlated_columns(dataset, pearson_threshold)\n",
    "    print('features n-correlacionadas: ', dataset.shape)\n",
    "\n",
    "    if split:\n",
    "        seed = 7\n",
    "        test_size = 0.30\n",
    "        X_train, X_test, y_train, y_test = train_test_split(dataset, target, test_size=test_size, random_state=seed)\n",
    "        \n",
    "        # Transform the features\n",
    "        X_train = pipeline.fit_transform(X_train)\n",
    "        X_test = pipeline.fit_transform(X_test)\n",
    "        #print(X_train.min(axis=0), X_train.max(axis=0))\n",
    "    else:\n",
    "        dataset  = pipeline.fit_transform(dataset)\n",
    "\n",
    "    return dataset, X_train, X_test, y_train, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataprep_standardScaler(dataset, target, split = True):\n",
    "    X_train, X_test, y_train, y_test = None, None, None, None\n",
    "    \n",
    "    pearson_threshold = 0.9\n",
    "    # Create a pipeline for standardization and PCA\n",
    "    pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        #('pca', PCA(n_components=0.95))\n",
    "    ])\n",
    "\n",
    "    dataset = remove_highly_correlated_columns(dataset, pearson_threshold)\n",
    "    print('features n-correlacionadas: ', dataset.shape)\n",
    "\n",
    "    if split:\n",
    "        seed = 7\n",
    "        test_size = 0.30\n",
    "        X_train, X_test, y_train, y_test = train_test_split(dataset, target, test_size=test_size, random_state=seed)\n",
    "        \n",
    "        # Transform the features\n",
    "        X_train = pipeline.fit_transform(X_train)\n",
    "        X_test = pipeline.fit_transform(X_test)\n",
    "        #print(X_train.min(axis=0), X_train.max(axis=0))\n",
    "    else:\n",
    "        dataset  = pipeline.fit_transform(dataset)\n",
    "\n",
    "    return dataset, X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataprep_pearson(dataset, target, split = True):\n",
    "    X_train, X_test, y_train, y_test = None, None, None, None\n",
    "    \n",
    "    pearson_threshold = 0.9\n",
    "    dataset = remove_highly_correlated_columns(dataset, pearson_threshold)\n",
    "    print('features n-correlacionadas: ', dataset.shape)\n",
    "\n",
    "    if split:\n",
    "        seed = 7\n",
    "        test_size = 0.30\n",
    "        X_train, X_test, y_train, y_test = train_test_split(dataset, target, test_size=test_size, random_state=seed)\n",
    "    \n",
    "    return dataset, X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataprep_split(dataset, target, split = True):\n",
    "    X_train, X_test, y_train, y_test = None, None, None, None\n",
    "\n",
    "    if split:\n",
    "        seed = 7\n",
    "        test_size = 0.30\n",
    "        X_train, X_test, y_train, y_test = train_test_split(dataset, target, test_size=test_size, random_state=seed)\n",
    "    \n",
    "    return dataset, X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['X_train', 'y_train', 'X_test', 'y_test']\n",
      "features:  (100855, 16)\n"
     ]
    }
   ],
   "source": [
    "# Carrega a base de dados a partir de seu caminho\n",
    "data = np.load('data/dataset.npz')\n",
    "print(data.files)\n",
    "\n",
    "XX_train = data['X_train']\n",
    "yy_train = data['y_train']\n",
    "yy_train = yy_train.reshape(-1,1) #reorganiza o array em um array 1 x 1\n",
    "    \n",
    "X = XX_train\n",
    "y = yy_train\n",
    "\n",
    "unseen_data = data['X_test']\n",
    "print('features: ', unseen_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features n-correlacionadas:  (47777, 15)\n"
     ]
    }
   ],
   "source": [
    "X, X_train, X_test, y_train, y_test = dataprep_standardScaler(X, y, split = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features n-correlacionadas:  (100855, 15)\n",
      "features:  (100855, 15)\n",
      "15 1\n"
     ]
    }
   ],
   "source": [
    "unseen_data, _ , _, _, _ = dataprep_standardScaler(dataset=unseen_data, target=None, split=False)\n",
    "print('features: ', unseen_data.shape)\n",
    "input_dim, output_dim = unseen_data.shape[1], 1\n",
    "print(input_dim, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_result(predictions, submission_file):\n",
    "    y_test_probs = predictions.ravel()\n",
    "    # Set the best threshold on validation data\n",
    "    best_threshold = 0.5\n",
    "    # Apply the best threshold to make binary predictions\n",
    "    predicted_classes = (y_test_probs > best_threshold).astype(int)\n",
    "\n",
    "    \n",
    "    unique_values, indices, counts = np.unique(predicted_classes, return_index=True, return_counts=True)\n",
    "    print(\"Unique values:\", unique_values)\n",
    "    print(\"Indices:\", indices)\n",
    "    print(\"Counts:\", counts)\n",
    "    print(\"Proportion:\", counts[0]/counts[1])\n",
    "    \n",
    "    \n",
    "    # Create a DataFrame with zipped data and column names\n",
    "    #num_samples = unseen_data.shape[0]\n",
    "    \n",
    "    df = pd.DataFrame(list(zip(range(1, len(predicted_classes) + 1), predicted_classes)), columns=['ID', 'Prediction'])\n",
    "    #df = pd.DataFrame({'ID': np.arange(1, num_samples + 1),'Prediction': predicted_classes})\n",
    "    df.to_csv(submission_file, index=False)\n",
    "    # Print the predictions\n",
    "    #print(\"Predicted probabilities:\\n\", y_test_probs)\n",
    "    #print(\"Predicted classes:\\n\", predicted_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling Sequential.call().\n\n\u001b[1mInput 0 of layer \"dense\" is incompatible with the layer: expected axis -1 of input shape to have value 16, but received input with shape (None, 15)\u001b[0m\n\nArguments received by Sequential.call():\n  • inputs=tf.Tensor(shape=(None, 15), dtype=float32)\n  • training=False\n  • mask=None",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Load the model using the appropriate function\u001b[39;00m\n\u001b[0;32m      2\u001b[0m loaded_model \u001b[38;5;241m=\u001b[39m  keras\u001b[38;5;241m.\u001b[39mmodels\u001b[38;5;241m.\u001b[39mload_model(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbestANN/model_1.keras\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m test_loss, test_accuracy \u001b[38;5;241m=\u001b[39m loaded_model\u001b[38;5;241m.\u001b[39mevaluate(X_test, y_test)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFor testing dataset. Loss: \u001b[39m\u001b[38;5;132;01m{:.4f}\u001b[39;00m\u001b[38;5;124m and Accuracy: \u001b[39m\u001b[38;5;132;01m{:.2f}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(test_loss, \u001b[38;5;241m100\u001b[39m\u001b[38;5;241m*\u001b[39mtest_accuracy))\n\u001b[0;32m      7\u001b[0m predictions \u001b[38;5;241m=\u001b[39m loaded_model\u001b[38;5;241m.\u001b[39mpredict(unseen_data)\n",
      "File \u001b[1;32mc:\\Users\\cazev\\miniconda3\\envs\\webapp\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\cazev\\miniconda3\\envs\\webapp\\Lib\\site-packages\\keras\\src\\layers\\input_spec.py:227\u001b[0m, in \u001b[0;36massert_input_compatibility\u001b[1;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[0;32m    222\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m axis, value \u001b[38;5;129;01min\u001b[39;00m spec\u001b[38;5;241m.\u001b[39maxes\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m    223\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m shape[axis] \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m {\n\u001b[0;32m    224\u001b[0m             value,\n\u001b[0;32m    225\u001b[0m             \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    226\u001b[0m         }:\n\u001b[1;32m--> 227\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    228\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInput \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of layer \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    229\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mincompatible with the layer: expected axis \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    230\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mof input shape to have value \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    231\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut received input with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    232\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    233\u001b[0m             )\n\u001b[0;32m    234\u001b[0m \u001b[38;5;66;03m# Check shape.\u001b[39;00m\n\u001b[0;32m    235\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m spec\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mValueError\u001b[0m: Exception encountered when calling Sequential.call().\n\n\u001b[1mInput 0 of layer \"dense\" is incompatible with the layer: expected axis -1 of input shape to have value 16, but received input with shape (None, 15)\u001b[0m\n\nArguments received by Sequential.call():\n  • inputs=tf.Tensor(shape=(None, 15), dtype=float32)\n  • training=False\n  • mask=None"
     ]
    }
   ],
   "source": [
    "# Load the model using the appropriate function\n",
    "loaded_model =  keras.models.load_model('bestANN/model_1.keras')\n",
    "\n",
    "test_loss, test_accuracy = loaded_model.evaluate(X_test, y_test)\n",
    "print('For testing dataset. Loss: {:.4f} and Accuracy: {:.2f}%'.format(test_loss, 100*test_accuracy))\n",
    "\n",
    "predictions = loaded_model.predict(unseen_data)\n",
    "save_result(predictions=predictions, submission_file='submission_1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second approch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.8181 - loss: 0.3935\n",
      "For testing dataset. Loss: 0.3857 and Accuracy: 82.04%\n",
      "\u001b[1m3152/3152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 3ms/step\n",
      "Unique values: [0 1]\n",
      "Indices: [0 1]\n",
      "Counts: [54937 45918]\n",
      "Proportion: 1.1964153491005707\n"
     ]
    }
   ],
   "source": [
    "loaded_model =  keras.models.load_model('bestANN/model_2.keras')\n",
    "\n",
    "test_loss, test_accuracy = loaded_model.evaluate(X_test, y_test)\n",
    "print('For testing dataset. Loss: {:.4f} and Accuracy: {:.2f}%'.format(test_loss, 100*test_accuracy))\n",
    "\n",
    "predictions = loaded_model.predict(unseen_data)\n",
    "save_result(predictions=predictions, submission_file='submission_2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Third approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For testing dataset. Accuracy: 81.25%\n",
      "Unique values: [0 1]\n",
      "Indices: [0 1]\n",
      "Counts: [54290 46565]\n",
      "Proportion: 1.1658971330398369\n"
     ]
    }
   ],
   "source": [
    "loaded_model = NeuralNetwork.load('bestANN/model_3.pkl')\n",
    "\n",
    "y_test_probs = loaded_model.predict(X_test)\n",
    "print('For testing dataset. Accuracy: {:.2f}%'.format(100*accuracy_score(y_test, y_test_probs > 0.5)))\n",
    "\n",
    "predictions = loaded_model.predict(unseen_data)\n",
    "save_result(predictions=predictions, submission_file='submission_3.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fourth approch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For testing dataset. Accuracy: 82.07%\n",
      "Unique values: [0 1]\n",
      "Indices: [0 5]\n",
      "Counts: [56862 43993]\n",
      "Proportion: 1.292523810606233\n"
     ]
    }
   ],
   "source": [
    "loaded_model = NeuralNetwork.load('bestANN/model_4.pkl')\n",
    "\n",
    "y_test_probs = loaded_model.predict(X_test)\n",
    "print('For testing dataset. Accuracy: {:.2f}%'.format(100*accuracy_score(y_test, y_test_probs > 0.5)))\n",
    "\n",
    "predictions = loaded_model.predict(unseen_data)\n",
    "save_result(predictions=predictions, submission_file='submission_4.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "webapp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
