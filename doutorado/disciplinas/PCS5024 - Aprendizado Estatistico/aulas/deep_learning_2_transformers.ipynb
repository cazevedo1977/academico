{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2318972",
   "metadata": {},
   "source": [
    "# PCS5024 - Aprendizado Estat√≠stico - Statistical Learning - 2023/1\n",
    "### Professors: \n",
    "### Anna Helena Reali Costa (anna.reali@usp.br)\n",
    "### Fabio G. Cozman (fgcozman@usp.br)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a7d787f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --quiet torch transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d378e917-4b88-4ad6-8262-26bc03ca70c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217d362d-d6b6-427d-bdab-ccd648bd94a7",
   "metadata": {},
   "source": [
    "# Transformers\n",
    "\n",
    "The Transformers architecture, introduced by Vaswani et al. in their 2017 paper \"Attention is All You Need,\" has revolutionized the field of natural language processing (NLP) and deep learning. This architecture is based on the concept of self-attention, which allows the model to weigh the importance of different words in a sequence. It has led to state-of-the-art results in various NLP tasks, such as machine translation, question-answering, and text summarization. This text aims to provide a detailed overview of the Transformers architecture and its key components.\n",
    "\n",
    "1. Word Embeddings:\n",
    "    The first step in the Transformers architecture is to convert the input text into a sequence of word embeddings. The word embeddings are learned during training and are used to represent the meaning of the words.\n",
    "\n",
    "2. Positional Encoding:\n",
    "    The Transformer architecture incorporates positional encoding to provide information about the relative positions of words. The positional encoding is added to the input word embeddings before they are fed into the self-attention layers. This enables the model to learn and exploit the order of the words in the sequence during training.\n",
    "\n",
    "3. Attention Mechanism:\n",
    "    The core innovation in the Transformers architecture is the attention mechanism, which allows the model to weigh the importance of different words in a sequence with respect to a given word. This is achieved by computing an attention score for each word, which is based on the similarity between the given word's representation and the other words in the sequence. The attention scores are then used to compute a weighted sum of the input representations, which forms the output of the attention layer.\n",
    "\n",
    "3. Pretraining and Fine-tuning:\n",
    "    The large-scale pretrained models have demonstrated the power of the Transformers architecture. These models are pretrained on massive amounts of text data in a self-supervised manner, learning to generate or predict masked words in a sentence. After pretraining, the models can be fine-tuned on specific tasks such as text classification, question-answering, and machine translation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e811d73c",
   "metadata": {},
   "source": [
    "In general, Transformers can be represented as following:\n",
    "\n",
    "\n",
    "<img src='https://upload.wikimedia.org/wikipedia/commons/8/8f/The-Transformer-model-architecture.png'  width=\"50%\" height=\"50%\">\n",
    "\n",
    "\n",
    "\n",
    "The transformer architecture is the \"engine\" behind all state-of-the-art large language models (LLMs), such as, GPT, Bard, T5 and others."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed82cdd",
   "metadata": {},
   "source": [
    "## Word embeddings\n",
    "\n",
    "Embeddings in deep learning are maps between indices and high dimensional representations. In LLMs each token is associated to an integer. The embedding module maps each one of this integers to a learnable vector of size K.\n",
    "\n",
    "BERT (Devlin et al.) is a very influential Transformer model that uses $K = 512$, which means each token is internally represented by a vector of size 512.\n",
    "\n",
    "As the training progresses the parameters associated to each word are learned. This is important so tokens that are related map to similar embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09031ab4",
   "metadata": {},
   "source": [
    "## The Attention Mechanism\n",
    "\n",
    "The attention mechanism is a powerful technique used in deep learning models, particularly in sequence-to-sequence tasks like neural machine translation, text summarization, and question answering. The primary purpose of attention is to enable the model to selectively focus on different parts of the input sequence while generating the output. It helps the model to capture long-range dependencies and better understand the context.\n",
    "\n",
    "The attention mechanism can be understood in three main steps:\n",
    "\n",
    "1. Query, Key, and Value Vectors:\n",
    "    For each word in the input sequence, the model generates three vectors, called the query (Q), key (K), and value (V) vectors. These vectors are derived by multiplying the input word embeddings with their respective weight matrices ($W_q$, $W_k$, and $W_v$), which are learned during training. The query vector is used to represent the current word, while the key and value vectors represent the other words in the sequence.\n",
    "\n",
    "2. Attention Scores:\n",
    "    The attention mechanism computes a score for each word in the input sequence with respect to the current word. This score indicates the importance or relevance of the other words to the current word. The attention score is calculated as the dot product between the query vector (Q) of the current word and the key vector (K) of the other words, followed by scaling it with the square root of the key vector's dimension (usually denoted as $d_k$). The attention scores are then passed through a softmax function to convert them into probabilities that sum to one.\n",
    "\n",
    " <img src='https://production-media.paperswithcode.com/methods/multi-head-attention_l1A3G7a.png'  width=\"20%\" height=\"20%\"> \n",
    "\n",
    " <img src='https://drive.google.com/uc?id=177C6VCnAXHnE1WLK_tSUi6dHV8FFLvHp'  width=\"15%\" height=\"15%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d15a2f",
   "metadata": {},
   "source": [
    "The attention mechanism analyses the fully connected **graph** in which each token is a node. Based on the value of the dot product between the vector representations of each token pair it determines which tokens are related.\n",
    "\n",
    "<img src='https://1.bp.blogspot.com/-AVGK0ApREtk/WaiAuzddKVI/AAAAAAAAB_A/WPV5ropBU-cxrcMpqJBFHg73K9NX4vywwCLcBGAs/s1600/image2.png'  width=\"60%\" height=\"60%\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde0cd11",
   "metadata": {},
   "source": [
    "## Positional Encoding\n",
    "\n",
    "Positional encoding is a critical component of the Transformer architecture. The purpose of positional encoding is to provide the model with information about the positions of the tokens in a sequence, as the self-attention mechanism in transformers does not inherently capture this information.\n",
    "\n",
    "The original Transformer paper uses sine and cosine functions of varying frequencies as the basis for creating the positional encodings. The encoding is added to the token embeddings before they are fed into the layers of the Transformer model. The encoding function is defined as:\n",
    "\n",
    "$PE(pos, 2i) = sin(\\frac{pos}{10000^{(2i/d_{m})}})$\n",
    "\n",
    "$PE(pos, 2i+1) = cos(\\frac{pos}{10000^{(2i/d_{m})}})$\n",
    "\n",
    "where:\n",
    "\n",
    "- $pos$ is the position of the token in the sequence\n",
    "- $i$ is the dimension of the encoding\n",
    "- $d_m$ is the dimension of the token embeddings\n",
    "\n",
    "These functions generate a unique encoding for each position in the input sequence, which is then added to the token embeddings. The intuition behind these functions is that they can represent the position of a token in the sequence using a mix of different frequency sinusoids. This allows the model to learn and generalize patterns based on relative positions, rather than absolute positions.\n",
    "\n",
    "Adding positional encodings to the token embeddings enables the attention mechanism to consider both the content of the tokens and their positions in the sequence. This is particularly important for natural language processing tasks, where word order is crucial for understanding the meaning of a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6d0a1f0-48a4-4d7a-b7aa-00a48330f4d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def classify_sentiment(text):\n",
    "    # Load the tokenizer and the model\n",
    "    tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "    model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased-finetuned-sst-2-english')\n",
    "\n",
    "    # Tokenize the input text and convert tokens to tensor\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "    input_ids = inputs['input_ids']\n",
    "    attention_mask = inputs['attention_mask']\n",
    "\n",
    "    # Run the model and get the classification logits\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "    logits = outputs.logits\n",
    "\n",
    "    # Convert the logits to probabilities and find the predicted class\n",
    "    probs = torch.softmax(logits, dim=-1)\n",
    "    predicted_class = torch.argmax(probs, dim=-1).item()\n",
    "\n",
    "    # Map the predicted class to a sentiment label\n",
    "    sentiment_label = \"positive\" if predicted_class == 1 else \"negative\"\n",
    "    return sentiment_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e882fb7e-942f-4f5b-a4e9-c79c625ed358",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f0b8de26d044877ab22bddb7f5172ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cazev\\miniconda3\\envs\\cashme\\lib\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\cazev\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment: positive\n"
     ]
    }
   ],
   "source": [
    "text = \"I absolutely love this product! It's amazing.\"\n",
    "sentiment = classify_sentiment(text)\n",
    "print(f\"Sentiment: {sentiment}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "421690d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment: negative\n"
     ]
    }
   ],
   "source": [
    "text = \"I'm not sure how to feel about this.\"\n",
    "sentiment = classify_sentiment(text)\n",
    "print(f\"Sentiment: {sentiment}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e27e3f16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment: positive\n"
     ]
    }
   ],
   "source": [
    "text = \"I'm not sure how to feel about this, but I'm looking forward to trying it\"\n",
    "sentiment = classify_sentiment(text)\n",
    "print(f\"Sentiment: {sentiment}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57aed16d",
   "metadata": {},
   "source": [
    "Transformers have been extremely successful, but there are also several limitations that represent very active areas of research.\n",
    "\n",
    "- Resource intensity: Transformers demand substantial computational power and memory during both training and inference stages, which can limit their use in settings with restricted resources or on less powerful devices.\n",
    "\n",
    "- Handling long sequences: Due to their quadratic self-attention mechanism, transformers have difficulty processing very long sequences, restricting their applicability in certain domains.\n",
    "\n",
    "- Lack of interpretability: The complex inner workings of transformers are challenging to comprehend, making it problematic to explain their decisions or detect biases in their outputs.\n",
    "\n",
    "- Overfitting and generalization issues: Transformers, particularly large-scale models, may sometimes overfit the training data, resulting in poor generalization to new, unseen data.\n",
    "\n",
    "- Inefficient data utilization: Transformers often necessitate vast amounts of labeled data for training, which can be costly or time-consuming to collect."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9ea5b3",
   "metadata": {},
   "source": [
    "A more comprehensive overview of the Transformer architecture can be found at: https://jalammar.github.io/illustrated-transformer/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cashme] *",
   "language": "python",
   "name": "conda-env-cashme-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
