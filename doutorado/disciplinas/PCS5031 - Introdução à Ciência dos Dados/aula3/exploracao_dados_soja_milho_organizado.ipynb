{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üìä Explora√ß√£o de Dados - Dataset Soja e Milho\n",
        "\n",
        "**Objetivo:** An√°lise explorat√≥ria completa do dataset `soja_milho.csv` com foco em medidas de tend√™ncia central, detec√ß√£o de outliers e an√°lise de correla√ß√µes.\n",
        "\n",
        "**Dataset:** `soja_milho.csv` - Pre√ßos hist√≥ricos de soja e milho\n",
        "\n",
        "**Nota:** As vari√°veis 'M√™s' e 'Safra' s√£o tratadas como categ√≥ricas, pois representam partes da data (m√™s do ano e ano/safra respectivamente).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìã √çndice do Notebook\n",
        "\n",
        "### **[1. Importa√ß√£o das Bibliotecas Necess√°rias](#1-importa√ß√£o-das-bibliotecas-necess√°rias)**\n",
        "### **[2. Carregamento e Primeira Inspe√ß√£o dos Dados](#2-carregamento-e-primeira-inspe√ß√£o-dos-dados)**\n",
        "### **[3. An√°lise de Qualidade dos Dados](#3-an√°lise-de-qualidade-dos-dados)**\n",
        "### **[4. An√°lise Descritiva e Medidas de Tend√™ncia Central](#4-an√°lise-descritiva-e-medidas-de-tend√™ncia-central)**\n",
        "### **[5. Visualiza√ß√µes das Medidas de Tend√™ncia Central](#5-visualiza√ß√µes-das-medidas-de-tend√™ncia-central)**\n",
        "### **[6. An√°lise de Espalhamento e Detec√ß√£o de Outliers](#6-an√°lise-de-espalhamento-e-detec√ß√£o-de-outliers)**\n",
        "- **[6.1 An√°lise Univariada de Outliers](#61-an√°lise-univariada-de-outliers)**: Box plots e detec√ß√£o IQR\n",
        "- **[6.2 An√°lise Multivariada de Outliers - Bag Plot](#62-an√°lise-multivariada-de-outliers---bag-plot)**: Dist√¢ncias de Mahalanobis\n",
        "### **[7. An√°lise de Correla√ß√£o e Covari√¢ncia](#7-an√°lise-de-correla√ß√£o-e-covari√¢ncia)**\n",
        "### **[8. Resumo e Conclus√µes](#8-resumo-e-conclus√µes)**\n",
        "### **[9. Informa√ß√µes T√©cnicas do Notebook](#9-informa√ß√µes-t√©cnicas-do-notebook)**\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Importa√ß√£o das Bibliotecas Necess√°rias\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Importa√ß√£o das bibliotecas necess√°rias\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "from scipy.stats import pearsonr\n",
        "from sklearn.covariance import MinCovDet\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Configura√ß√µes de visualiza√ß√£o\n",
        "plt.style.use('default')\n",
        "sns.set_palette(\"husl\")\n",
        "plt.rcParams['figure.figsize'] = (12, 8)\n",
        "plt.rcParams['font.size'] = 10\n",
        "\n",
        "# Verificar vers√µes das bibliotecas\n",
        "print(\"üìö BIBLIOTECAS IMPORTADAS COM SUCESSO!\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"üìä Pandas: {pd.__version__}\")\n",
        "print(f\"üî¢ NumPy: {np.__version__}\")\n",
        "print(f\"üìà Matplotlib: {plt.matplotlib.__version__}\")\n",
        "print(f\"üé® Seaborn: {sns.__version__}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Carregamento e Primeira Inspe√ß√£o dos Dados\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Carregamento do dataset com tratamento de codifica√ß√£o\n",
        "def load_dataset(file_path):\n",
        "    \"\"\"\n",
        "    Carrega o dataset com tratamento de diferentes codifica√ß√µes\n",
        "    \"\"\"\n",
        "    encodings = ['utf-8', 'latin-1', 'cp1252', 'iso-8859-1']\n",
        "    \n",
        "    for encoding in encodings:\n",
        "        try:\n",
        "            df = pd.read_csv(file_path, encoding=encoding)\n",
        "            print(f\"‚úÖ Dataset carregado com sucesso usando codifica√ß√£o: {encoding}\")\n",
        "            return df\n",
        "        except UnicodeDecodeError:\n",
        "            continue\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Erro ao carregar com {encoding}: {e}\")\n",
        "            continue\n",
        "    \n",
        "    print(\"‚ùå N√£o foi poss√≠vel carregar o dataset com nenhuma codifica√ß√£o testada\")\n",
        "    return None\n",
        "\n",
        "# Carregar o dataset\n",
        "file_path = 'soja_milho.csv'\n",
        "df = load_dataset(file_path)\n",
        "\n",
        "if df is not None:\n",
        "    print(\"\\nüìä PRIMEIRA INSPE√á√ÉO DO DATASET:\")\n",
        "    print(\"=\" * 50)\n",
        "    print(f\"üìè Dimens√µes: {df.shape[0]} linhas √ó {df.shape[1]} colunas\")\n",
        "    print(f\"\\nüìã Colunas: {list(df.columns)}\")\n",
        "    print(f\"\\nüìÑ Primeiras 5 linhas:\")\n",
        "    print(df.head())\n",
        "    print(f\"\\nüìÑ √öltimas 5 linhas:\")\n",
        "    print(df.tail())\n",
        "    print(f\"\\nüìä Informa√ß√µes gerais:\")\n",
        "    print(df.info())\n",
        "else:\n",
        "    print(\"‚ùå Dataset n√£o carregado. Verifique o arquivo 'soja_milho.csv'\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. An√°lise de Qualidade dos Dados\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# An√°lise de qualidade dos dados\n",
        "if df is not None:\n",
        "    print(\"=\" * 60)\n",
        "    print(\"AN√ÅLISE DE QUALIDADE DOS DADOS\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    # Informa√ß√µes b√°sicas\n",
        "    print(f\"\\nüìä INFORMA√á√ïES B√ÅSICAS:\")\n",
        "    print(\"-\" * 40)\n",
        "    print(f\"  ‚Ä¢ Total de linhas: {df.shape[0]}\")\n",
        "    print(f\"  ‚Ä¢ Total de colunas: {df.shape[1]}\")\n",
        "    print(f\"  ‚Ä¢ Mem√≥ria utilizada: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
        "    \n",
        "    # An√°lise de valores ausentes\n",
        "    print(f\"\\nüîç AN√ÅLISE DE VALORES AUSENTES:\")\n",
        "    print(\"-\" * 40)\n",
        "    missing_data = df.isnull().sum()\n",
        "    missing_percent = (missing_data / len(df)) * 100\n",
        "    \n",
        "    missing_df = pd.DataFrame({\n",
        "        'Coluna': missing_data.index,\n",
        "        'Valores_Ausentes': missing_data.values,\n",
        "        'Percentual': missing_percent.values\n",
        "    })\n",
        "    \n",
        "    print(missing_df.to_string(index=False))\n",
        "    \n",
        "    if missing_data.sum() == 0:\n",
        "        print(\"\\n‚úÖ Nenhum valor ausente encontrado!\")\n",
        "    else:\n",
        "        print(f\"\\n‚ö†Ô∏è  Total de valores ausentes: {missing_data.sum()}\")\n",
        "    \n",
        "    # Identificar tipos de dados\n",
        "    print(f\"\\nüìã TIPOS DE DADOS:\")\n",
        "    print(\"-\" * 40)\n",
        "    print(df.dtypes.to_string())\n",
        "    \n",
        "    # Identificar vari√°veis num√©ricas e categ√≥ricas\n",
        "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    categorical_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
        "    \n",
        "    print(f\"\\nüî¢ VARI√ÅVEIS NUM√âRICAS ({len(numeric_cols)}):\")\n",
        "    print(\"-\" * 40)\n",
        "    for col in numeric_cols:\n",
        "        print(f\"  ‚Ä¢ {col}\")\n",
        "    \n",
        "    print(f\"\\nüìù VARI√ÅVEIS CATEG√ìRICAS ({len(categorical_cols)}):\")\n",
        "    print(\"-\" * 40)\n",
        "    for col in categorical_cols:\n",
        "        print(f\"  ‚Ä¢ {col}\")\n",
        "    \n",
        "    # An√°lise de duplicatas\n",
        "    print(f\"\\nüîÑ AN√ÅLISE DE DUPLICATAS:\")\n",
        "    print(\"-\" * 40)\n",
        "    duplicates = df.duplicated().sum()\n",
        "    print(f\"  ‚Ä¢ Linhas duplicadas: {duplicates}\")\n",
        "    \n",
        "    if duplicates > 0:\n",
        "        print(f\"  ‚Ä¢ Percentual de duplicatas: {(duplicates/len(df)*100):.2f}%\")\n",
        "    else:\n",
        "        print(\"  ‚Ä¢ ‚úÖ Nenhuma linha duplicada encontrada\")\n",
        "    \n",
        "    # Estat√≠sticas descritivas para vari√°veis num√©ricas\n",
        "    if len(numeric_cols) > 0:\n",
        "        print(f\"\\nüìä ESTAT√çSTICAS DESCRITIVAS (VARI√ÅVEIS NUM√âRICAS):\")\n",
        "        print(\"-\" * 50)\n",
        "        print(df[numeric_cols].describe().round(4))\n",
        "    \n",
        "    # An√°lise de valores √∫nicos para vari√°veis categ√≥ricas\n",
        "    if len(categorical_cols) > 0:\n",
        "        print(f\"\\nüìù AN√ÅLISE DE VARI√ÅVEIS CATEG√ìRICAS:\")\n",
        "        print(\"-\" * 50)\n",
        "        for col in categorical_cols:\n",
        "            unique_count = df[col].nunique()\n",
        "            print(f\"  ‚Ä¢ {col}: {unique_count} valores √∫nicos\")\n",
        "            if unique_count <= 10:  # Mostrar valores se forem poucos\n",
        "                print(f\"    Valores: {df[col].unique().tolist()}\")\n",
        "    \n",
        "    print(f\"\\n‚úÖ An√°lise de qualidade conclu√≠da!\")\n",
        "    \n",
        "else:\n",
        "    print(\"‚ùå N√£o foi poss√≠vel realizar a an√°lise de qualidade - dataset n√£o carregado.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. An√°lise Descritiva e Medidas de Tend√™ncia Central\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# An√°lise descritiva e medidas de tend√™ncia central\n",
        "if df is not None and len(numeric_cols) > 0:\n",
        "    print(\"=\" * 60)\n",
        "    print(\"AN√ÅLISE DESCRITIVA E MEDIDAS DE TEND√äNCIA CENTRAL\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    for col in numeric_cols:\n",
        "        data = df[col].dropna()\n",
        "        \n",
        "        print(f\"\\nüìä AN√ÅLISE DA VARI√ÅVEL: {col}\")\n",
        "        print(\"-\" * 50)\n",
        "        \n",
        "        # Medidas de tend√™ncia central\n",
        "        mean_val = data.mean()\n",
        "        median_val = data.median()\n",
        "        mode_val = data.mode().iloc[0] if not data.mode().empty else \"N/A\"\n",
        "        \n",
        "        print(f\"üìà MEDIDAS DE TEND√äNCIA CENTRAL:\")\n",
        "        print(f\"  ‚Ä¢ M√©dia: {mean_val:.4f}\")\n",
        "        print(f\"  ‚Ä¢ Mediana: {median_val:.4f}\")\n",
        "        print(f\"  ‚Ä¢ Moda: {mode_val}\")\n",
        "        \n",
        "        # Medidas de dispers√£o\n",
        "        std_val = data.std()\n",
        "        var_val = data.var()\n",
        "        range_val = data.max() - data.min()\n",
        "        iqr_val = data.quantile(0.75) - data.quantile(0.25)\n",
        "        cv_val = (std_val / mean_val) * 100 if mean_val != 0 else 0\n",
        "        \n",
        "        print(f\"\\nüìè MEDIDAS DE DISPERS√ÉO:\")\n",
        "        print(f\"  ‚Ä¢ Desvio padr√£o: {std_val:.4f}\")\n",
        "        print(f\"  ‚Ä¢ Vari√¢ncia: {var_val:.4f}\")\n",
        "        print(f\"  ‚Ä¢ Amplitude: {range_val:.4f}\")\n",
        "        print(f\"  ‚Ä¢ IQR: {iqr_val:.4f}\")\n",
        "        print(f\"  ‚Ä¢ Coeficiente de varia√ß√£o: {cv_val:.2f}%\")\n",
        "        \n",
        "        # Quartis e percentis\n",
        "        q1 = data.quantile(0.25)\n",
        "        q3 = data.quantile(0.75)\n",
        "        p10 = data.quantile(0.10)\n",
        "        p90 = data.quantile(0.90)\n",
        "        p95 = data.quantile(0.95)\n",
        "        p99 = data.quantile(0.99)\n",
        "        \n",
        "        print(f\"\\nüìä QUARTIS E PERCENTIS:\")\n",
        "        print(f\"  ‚Ä¢ Q1 (25%): {q1:.4f}\")\n",
        "        print(f\"  ‚Ä¢ Q2 (50% - Mediana): {median_val:.4f}\")\n",
        "        print(f\"  ‚Ä¢ Q3 (75%): {q3:.4f}\")\n",
        "        print(f\"  ‚Ä¢ P10: {p10:.4f}\")\n",
        "        print(f\"  ‚Ä¢ P90: {p90:.4f}\")\n",
        "        print(f\"  ‚Ä¢ P95: {p95:.4f}\")\n",
        "        print(f\"  ‚Ä¢ P99: {p99:.4f}\")\n",
        "        \n",
        "        # Medidas robustas\n",
        "        trimmed_mean_5 = stats.trim_mean(data, 0.05)\n",
        "        trimmed_mean_10 = stats.trim_mean(data, 0.10)\n",
        "        \n",
        "        print(f\"\\nüõ°Ô∏è MEDIDAS ROBUSTAS:\")\n",
        "        print(f\"  ‚Ä¢ M√©dia aparada (5%): {trimmed_mean_5:.4f}\")\n",
        "        print(f\"  ‚Ä¢ M√©dia aparada (10%): {trimmed_mean_10:.4f}\")\n",
        "        \n",
        "        # An√°lise de assimetria e curtose\n",
        "        skewness = data.skew()\n",
        "        kurtosis = data.kurtosis()\n",
        "        \n",
        "        print(f\"\\nüìê FORMA DA DISTRIBUI√á√ÉO:\")\n",
        "        print(f\"  ‚Ä¢ Assimetria: {skewness:.4f}\")\n",
        "        if abs(skewness) < 0.5:\n",
        "            skew_interpretation = \"sim√©trica\"\n",
        "        elif abs(skewness) < 1:\n",
        "            skew_interpretation = \"levemente assim√©trica\"\n",
        "        else:\n",
        "            skew_interpretation = \"fortemente assim√©trica\"\n",
        "        print(f\"    Interpreta√ß√£o: {skew_interpretation}\")\n",
        "        \n",
        "        print(f\"  ‚Ä¢ Curtose: {kurtosis:.4f}\")\n",
        "        if kurtosis < 0:\n",
        "            kurt_interpretation = \"platic√∫rtica (achatada)\"\n",
        "        elif kurtosis > 0:\n",
        "            kurt_interpretation = \"leptoc√∫rtica (pontiaguda)\"\n",
        "        else:\n",
        "            kurt_interpretation = \"mesoc√∫rtica (normal)\"\n",
        "        print(f\"    Interpreta√ß√£o: {kurt_interpretation}\")\n",
        "        \n",
        "        # Resumo estat√≠stico completo\n",
        "        print(f\"\\nüìã RESUMO ESTAT√çSTICO COMPLETO:\")\n",
        "        print(\"-\" * 50)\n",
        "        print(data.describe().round(4))\n",
        "        \n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "    \n",
        "    print(f\"\\n‚úÖ An√°lise descritiva conclu√≠da para {len(numeric_cols)} vari√°vel(is) num√©rica(s)!\")\n",
        "    \n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  N√£o h√° vari√°veis num√©ricas para an√°lise descritiva.\")\n",
        "    if df is None:\n",
        "        print(\"‚ùå Dataset n√£o carregado.\")\n",
        "    else:\n",
        "        print(f\"üìä Apenas {len(numeric_cols)} vari√°vel num√©rica encontrada: {numeric_cols[0] if len(numeric_cols) == 1 else 'nenhuma'}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Visualiza√ß√µes das Medidas de Tend√™ncia Central\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualiza√ß√µes das medidas de tend√™ncia central\n",
        "if df is not None and len(numeric_cols) > 0:\n",
        "    print(\"=\" * 60)\n",
        "    print(\"VISUALIZA√á√ïES DAS MEDIDAS DE TEND√äNCIA CENTRAL\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    # Configurar subplots\n",
        "    n_cols = min(2, len(numeric_cols))\n",
        "    n_rows = (len(numeric_cols) + 1) // 2\n",
        "    \n",
        "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5*n_rows))\n",
        "    if n_rows == 1:\n",
        "        axes = [axes] if n_cols == 1 else axes\n",
        "    else:\n",
        "        axes = axes.flatten()\n",
        "    \n",
        "    for i, col in enumerate(numeric_cols):\n",
        "        data = df[col].dropna()\n",
        "        \n",
        "        # Histograma com medidas de tend√™ncia central\n",
        "        axes[i].hist(data, bins=30, alpha=0.7, color='skyblue', edgecolor='black')\n",
        "        \n",
        "        # Adicionar linhas para medidas de tend√™ncia central\n",
        "        mean_val = data.mean()\n",
        "        median_val = data.median()\n",
        "        mode_val = data.mode().iloc[0] if not data.mode().empty else None\n",
        "        \n",
        "        axes[i].axvline(mean_val, color='red', linestyle='--', linewidth=2, label=f'M√©dia: {mean_val:.2f}')\n",
        "        axes[i].axvline(median_val, color='green', linestyle='--', linewidth=2, label=f'Mediana: {median_val:.2f}')\n",
        "        \n",
        "        if mode_val is not None:\n",
        "            axes[i].axvline(mode_val, color='orange', linestyle='--', linewidth=2, label=f'Moda: {mode_val:.2f}')\n",
        "        \n",
        "        axes[i].set_title(f'Distribui√ß√£o de {col}', fontsize=14, fontweight='bold')\n",
        "        axes[i].set_xlabel(f'{col} (R$/sc)', fontsize=12)\n",
        "        axes[i].set_ylabel('Frequ√™ncia', fontsize=12)\n",
        "        axes[i].legend()\n",
        "        axes[i].grid(True, alpha=0.3)\n",
        "    \n",
        "    # Ocultar subplots vazios\n",
        "    for i in range(len(numeric_cols), len(axes)):\n",
        "        axes[i].set_visible(False)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Box plots para an√°lise de dispers√£o\n",
        "    if len(numeric_cols) > 1:\n",
        "        plt.figure(figsize=(12, 6))\n",
        "        \n",
        "        # Preparar dados para box plot\n",
        "        data_for_box = [df[col].dropna() for col in numeric_cols]\n",
        "        labels = numeric_cols\n",
        "        \n",
        "        box_plot = plt.boxplot(data_for_box, labels=labels, patch_artist=True)\n",
        "        \n",
        "        # Colorir os boxes\n",
        "        colors = ['lightblue', 'lightgreen', 'lightcoral', 'lightyellow']\n",
        "        for patch, color in zip(box_plot['boxes'], colors[:len(numeric_cols)]):\n",
        "            patch.set_facecolor(color)\n",
        "        \n",
        "        plt.title('Box Plots - An√°lise de Dispers√£o', fontsize=14, fontweight='bold')\n",
        "        plt.xlabel('Vari√°veis', fontsize=12)\n",
        "        plt.ylabel('Valores (R$/sc)', fontsize=12)\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        plt.xticks(rotation=45)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "    \n",
        "    # Gr√°ficos de linha temporal (se houver √≠ndice temporal)\n",
        "    if hasattr(df.index, 'to_pydatetime') or 'Data' in df.columns:\n",
        "        plt.figure(figsize=(15, 8))\n",
        "        \n",
        "        for i, col in enumerate(numeric_cols):\n",
        "            plt.subplot(len(numeric_cols), 1, i+1)\n",
        "            plt.plot(df.index, df[col], linewidth=1.5, alpha=0.8, label=col)\n",
        "            plt.title(f'S√©rie Temporal - {col}', fontsize=12, fontweight='bold')\n",
        "            plt.xlabel('Data', fontsize=10)\n",
        "            plt.ylabel(f'{col} (R$/sc)', fontsize=10)\n",
        "            plt.grid(True, alpha=0.3)\n",
        "            plt.legend()\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "    \n",
        "    print(f\"\\n‚úÖ Visualiza√ß√µes conclu√≠das para {len(numeric_cols)} vari√°vel(is) num√©rica(s)!\")\n",
        "    \n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  N√£o h√° vari√°veis num√©ricas para visualiza√ß√£o.\")\n",
        "    if df is None:\n",
        "        print(\"‚ùå Dataset n√£o carregado.\")\n",
        "    else:\n",
        "        print(f\"üìä Apenas {len(numeric_cols)} vari√°vel num√©rica encontrada: {numeric_cols[0] if len(numeric_cols) == 1 else 'nenhuma'}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. An√°lise de Espalhamento e Detec√ß√£o de Outliers\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.1 An√°lise Univariada de Outliers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# An√°lise de espalhamento e detec√ß√£o de outliers - Univariada\n",
        "if df is not None and len(numeric_cols) > 0:\n",
        "    print(\"=\" * 60)\n",
        "    print(\"AN√ÅLISE DE ESPALHAMENTO E DETEC√á√ÉO DE OUTLIERS\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    # Configurar subplots para box plots\n",
        "    n_cols = min(2, len(numeric_cols))\n",
        "    n_rows = (len(numeric_cols) + 1) // 2\n",
        "    \n",
        "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5*n_rows))\n",
        "    if n_rows == 1:\n",
        "        axes = [axes] if n_cols == 1 else axes\n",
        "    else:\n",
        "        axes = axes.flatten()\n",
        "    \n",
        "    # An√°lise detalhada de outliers\n",
        "    print(\"\\nüîç DETEC√á√ÉO E AN√ÅLISE DE OUTLIERS\")\n",
        "    print(\"-\" * 50)\n",
        "    \n",
        "    outliers_info = {}\n",
        "    \n",
        "    for i, col in enumerate(numeric_cols):\n",
        "        data = df[col].dropna()\n",
        "        \n",
        "        # Identificar outliers usando IQR\n",
        "        q1 = data.quantile(0.25)\n",
        "        q3 = data.quantile(0.75)\n",
        "        iqr = q3 - q1\n",
        "        lower_bound = q1 - 1.5 * iqr\n",
        "        upper_bound = q3 + 1.5 * iqr\n",
        "        \n",
        "        # Identificar outliers\n",
        "        outlier_mask = (data < lower_bound) | (data > upper_bound)\n",
        "        outliers = data[outlier_mask]\n",
        "        outlier_indices = data[outlier_mask].index.tolist()\n",
        "        \n",
        "        print(f\"\\nüìä {col}:\")\n",
        "        print(f\"  ‚Ä¢ Q1: {q1:.4f}\")\n",
        "        print(f\"  ‚Ä¢ Q3: {q3:.4f}\")\n",
        "        print(f\"  ‚Ä¢ IQR: {iqr:.4f}\")\n",
        "        print(f\"  ‚Ä¢ Limite inferior: {lower_bound:.4f}\")\n",
        "        print(f\"  ‚Ä¢ Limite superior: {upper_bound:.4f}\")\n",
        "        print(f\"  ‚Ä¢ Outliers detectados: {len(outliers)} ({(len(outliers)/len(data)*100):.2f}%)\")\n",
        "        \n",
        "        if len(outliers) > 0:\n",
        "            print(f\"  ‚Ä¢ Valores outliers: {outliers.tolist()}\")\n",
        "            print(f\"  ‚Ä¢ √çndices dos outliers: {outlier_indices}\")\n",
        "            \n",
        "            # Armazenar informa√ß√µes dos outliers para an√°lise posterior\n",
        "            outliers_info[col] = {\n",
        "                'values': outliers.tolist(),\n",
        "                'indices': outlier_indices,\n",
        "                'count': len(outliers)\n",
        "            }\n",
        "        else:\n",
        "            print(f\"  ‚Ä¢ ‚úÖ Nenhum outlier detectado\")\n",
        "            outliers_info[col] = {\n",
        "                'values': [],\n",
        "                'indices': [],\n",
        "                'count': 0\n",
        "            }\n",
        "        \n",
        "        # Criar box plot\n",
        "        box_plot = axes[i].boxplot(data, patch_artist=True)\n",
        "        axes[i].set_title(f'Box Plot - {col}', fontsize=12, fontweight='bold')\n",
        "        axes[i].set_ylabel(f'{col} (R$/sc)', fontsize=10)\n",
        "        axes[i].grid(True, alpha=0.3)\n",
        "    \n",
        "    # Ocultar subplots vazios\n",
        "    for i in range(len(numeric_cols), len(axes)):\n",
        "        axes[i].set_visible(False)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # An√°lise das linhas com outliers\n",
        "    if any(info['count'] > 0 for info in outliers_info.values()):\n",
        "        print(\"\\nüìã LINHAS COM OUTLIERS\")\n",
        "        print(\"-\" * 50)\n",
        "        \n",
        "        # Coletar todos os √≠ndices que t√™m outliers\n",
        "        all_outlier_indices = set()\n",
        "        for col, info in outliers_info.items():\n",
        "            if info['count'] > 0:\n",
        "                all_outlier_indices.update(info['indices'])\n",
        "        \n",
        "        if all_outlier_indices:\n",
        "            print(f\"\\nüìä Total de linhas com pelo menos um outlier: {len(all_outlier_indices)}\")\n",
        "            print(f\"üìä Linhas com outliers (√≠ndices): {sorted(list(all_outlier_indices))}\")\n",
        "            \n",
        "            # Mostrar as linhas completas que cont√™m outliers\n",
        "            print(f\"\\nüìÑ DADOS COMPLETOS DAS LINHAS COM OUTLIERS:\")\n",
        "            print(\"-\" * 60)\n",
        "            \n",
        "            outlier_rows = df.loc[sorted(all_outlier_indices)]\n",
        "            for idx, row in outlier_rows.iterrows():\n",
        "                print(f\"\\nüìÖ Data: {idx}\")\n",
        "                for col in numeric_cols:\n",
        "                    value = row[col]\n",
        "                    if col in outliers_info and idx in outliers_info[col]['indices']:\n",
        "                        print(f\"  {col}: {value:.4f} ‚ö†Ô∏è OUTLIER\")\n",
        "                    else:\n",
        "                        print(f\"  {col}: {value:.4f}\")\n",
        "    else:\n",
        "        print(\"\\n‚úÖ Nenhum outlier detectado em nenhuma vari√°vel num√©rica!\")\n",
        "    \n",
        "    print(f\"\\n‚úÖ An√°lise de outliers univariada conclu√≠da!\")\n",
        "    \n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  N√£o h√° vari√°veis num√©ricas para an√°lise de espalhamento.\")\n",
        "    if df is None:\n",
        "        print(\"‚ùå Dataset n√£o carregado.\")\n",
        "    else:\n",
        "        print(f\"üìä Apenas {len(numeric_cols)} vari√°vel num√©rica encontrada: {numeric_cols[0] if len(numeric_cols) == 1 else 'nenhuma'}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.2 An√°lise Multivariada de Outliers - Bag Plot\n",
        "\n",
        "O **Bag Plot** √© uma extens√£o multivariada do box plot univariado, desenvolvido por Rousseeuw, Ruts e Tukey (1999). √â uma ferramenta robusta para identificar outliers em dados multivariados, especialmente √∫til para s√©ries temporais de pre√ßos de commodities.\n",
        "\n",
        "**Caracter√≠sticas do Bag Plot:**\n",
        "- **Bag (Saco)**: Cont√©m 50% dos dados mais centrais (equivalente ao IQR univariado)\n",
        "- **Fence (Cerca)**: Define os limites para outliers (equivalente aos whiskers do box plot)\n",
        "- **Outliers**: Pontos fora da cerca s√£o considerados outliers multivariados\n",
        "- **Robustez**: Usa dist√¢ncias de Mahalanobis e estat√≠sticas robustas\n",
        "\n",
        "**Vantagens para An√°lise de Pre√ßos:**\n",
        "- Detecta outliers que n√£o seriam identificados em an√°lises univariadas\n",
        "- Considera a rela√ß√£o entre as vari√°veis (correla√ß√£o entre pre√ßos de milho e soja)\n",
        "- Identifica per√≠odos de comportamento an√¥malo conjunto dos pre√ßos\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# An√°lise de outliers multivariados usando Bag Plot\n",
        "if df is not None and len(numeric_cols) >= 2:\n",
        "    print(\"=\" * 70)\n",
        "    print(\"AN√ÅLISE MULTIVARIADA DE OUTLIERS - BAG PLOT\")\n",
        "    print(\"=\" * 70)\n",
        "    \n",
        "    # Preparar dados para an√°lise multivariada\n",
        "    data_multivariate = df[numeric_cols].dropna()\n",
        "    \n",
        "    if len(data_multivariate) > 0:\n",
        "        print(f\"\\nüìä DADOS PARA AN√ÅLISE MULTIVARIADA:\")\n",
        "        print(\"-\" * 50)\n",
        "        print(f\"  ‚Ä¢ Vari√°veis analisadas: {', '.join(numeric_cols)}\")\n",
        "        print(f\"  ‚Ä¢ N√∫mero de observa√ß√µes: {len(data_multivariate)}\")\n",
        "        print(f\"  ‚Ä¢ Per√≠odo: {data_multivariate.index[0]} a {data_multivariate.index[-1]}\")\n",
        "        \n",
        "        # Calcular estat√≠sticas robustas\n",
        "        from scipy.stats import chi2\n",
        "        from sklearn.covariance import MinCovDet\n",
        "        \n",
        "        # Usar Minimum Covariance Determinant (MCD) para robustez\n",
        "        mcd = MinCovDet(random_state=42)\n",
        "        mcd.fit(data_multivariate)\n",
        "        \n",
        "        # Calcular dist√¢ncias de Mahalanobis robustas\n",
        "        robust_mean = mcd.location_\n",
        "        robust_cov = mcd.covariance_\n",
        "        \n",
        "        # Calcular dist√¢ncias de Mahalanobis\n",
        "        mahal_distances = []\n",
        "        for i, row in data_multivariate.iterrows():\n",
        "            diff = row.values - robust_mean\n",
        "            inv_cov = np.linalg.inv(robust_cov)\n",
        "            mahal_dist = np.sqrt(diff.T @ inv_cov @ diff)\n",
        "            mahal_distances.append(mahal_dist)\n",
        "        \n",
        "        mahal_distances = np.array(mahal_distances)\n",
        "        \n",
        "        # Calcular quartis das dist√¢ncias de Mahalanobis\n",
        "        q1 = np.percentile(mahal_distances, 25)\n",
        "        q3 = np.percentile(mahal_distances, 75)\n",
        "        iqr = q3 - q1\n",
        "        \n",
        "        # Definir limites para outliers (similar ao box plot)\n",
        "        lower_fence = q1 - 1.5 * iqr\n",
        "        upper_fence = q3 + 1.5 * iqr\n",
        "        \n",
        "        # Identificar outliers\n",
        "        outlier_mask = (mahal_distances < lower_fence) | (mahal_distances > upper_fence)\n",
        "        outliers_data = data_multivariate[outlier_mask]\n",
        "        outliers_indices = data_multivariate[outlier_mask].index\n",
        "        \n",
        "        print(f\"\\nüìà ESTAT√çSTICAS ROBUSTAS:\")\n",
        "        print(\"-\" * 50)\n",
        "        print(f\"  ‚Ä¢ Centroide robusto (Milho): {robust_mean[0]:.4f}\")\n",
        "        print(f\"  ‚Ä¢ Centroide robusto (Soja): {robust_mean[1]:.4f}\")\n",
        "        print(f\"  ‚Ä¢ Q1 das dist√¢ncias de Mahalanobis: {q1:.4f}\")\n",
        "        print(f\"  ‚Ä¢ Q3 das dist√¢ncias de Mahalanobis: {q3:.4f}\")\n",
        "        print(f\"  ‚Ä¢ IQR das dist√¢ncias: {iqr:.4f}\")\n",
        "        print(f\"  ‚Ä¢ Limite inferior da cerca: {lower_fence:.4f}\")\n",
        "        print(f\"  ‚Ä¢ Limite superior da cerca: {upper_fence:.4f}\")\n",
        "        \n",
        "        print(f\"\\nüîç DETEC√á√ÉO DE OUTLIERS MULTIVARIADOS:\")\n",
        "        print(\"-\" * 50)\n",
        "        print(f\"  ‚Ä¢ Outliers detectados: {len(outliers_data)} ({(len(outliers_data)/len(data_multivariate)*100):.2f}%)\")\n",
        "        \n",
        "        if len(outliers_data) > 0:\n",
        "            print(f\"  ‚Ä¢ √çndices dos outliers: {outliers_indices.tolist()}\")\n",
        "            print(f\"  ‚Ä¢ Dist√¢ncias de Mahalanobis dos outliers: {mahal_distances[outlier_mask].round(4).tolist()}\")\n",
        "            \n",
        "            print(f\"\\nüìÑ DADOS COMPLETOS DOS OUTLIERS MULTIVARIADOS:\")\n",
        "            print(\"-\" * 60)\n",
        "            for idx, row in outliers_data.iterrows():\n",
        "                mahal_dist = mahal_distances[data_multivariate.index == idx][0]\n",
        "                print(f\"  üìÖ Data: {idx}\")\n",
        "                for col in numeric_cols:\n",
        "                    print(f\"    {col}: {row[col]:.4f}\")\n",
        "                print(f\"    Dist√¢ncia de Mahalanobis: {mahal_dist:.4f}\")\n",
        "                print()\n",
        "        else:\n",
        "            print(f\"  ‚Ä¢ ‚úÖ Nenhum outlier multivariado detectado\")\n",
        "        \n",
        "        # Visualiza√ß√£o do Bag Plot\n",
        "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8))\n",
        "        \n",
        "        # Gr√°fico 1: Scatter plot com outliers destacados\n",
        "        ax1.scatter(data_multivariate[numeric_cols[0]], \n",
        "                   data_multivariate[numeric_cols[1]], \n",
        "                   alpha=0.6, s=30, color='lightblue', label='Dados normais')\n",
        "        \n",
        "        if len(outliers_data) > 0:\n",
        "            ax1.scatter(outliers_data[numeric_cols[0]], \n",
        "                       outliers_data[numeric_cols[1]], \n",
        "                       color='red', s=50, alpha=0.8, label='Outliers multivariados')\n",
        "        \n",
        "        # Adicionar centroide robusto\n",
        "        ax1.scatter(robust_mean[0], robust_mean[1], \n",
        "                   color='green', s=100, marker='x', linewidth=3, label='Centroide robusto')\n",
        "        \n",
        "        ax1.set_xlabel(f'{numeric_cols[0]} (R$/sc)', fontsize=12)\n",
        "        ax1.set_ylabel(f'{numeric_cols[1]} (R$/sc)', fontsize=12)\n",
        "        ax1.set_title('Bag Plot - Detec√ß√£o de Outliers Multivariados', fontsize=14, fontweight='bold')\n",
        "        ax1.legend()\n",
        "        ax1.grid(True, alpha=0.3)\n",
        "        \n",
        "        # Gr√°fico 2: Dist√¢ncias de Mahalanobis ao longo do tempo\n",
        "        ax2.plot(data_multivariate.index, mahal_distances, alpha=0.7, color='blue', linewidth=1)\n",
        "        ax2.axhline(y=upper_fence, color='red', linestyle='--', alpha=0.8, label=f'Limite superior ({upper_fence:.2f})')\n",
        "        ax2.axhline(y=lower_fence, color='red', linestyle='--', alpha=0.8, label=f'Limite inferior ({lower_fence:.2f})')\n",
        "        \n",
        "        if len(outliers_data) > 0:\n",
        "            outlier_times = data_multivariate[outlier_mask].index\n",
        "            outlier_distances = mahal_distances[outlier_mask]\n",
        "            ax2.scatter(outlier_times, outlier_distances, color='red', s=50, alpha=0.8, zorder=5)\n",
        "        \n",
        "        ax2.set_xlabel('Data', fontsize=12)\n",
        "        ax2.set_ylabel('Dist√¢ncia de Mahalanobis', fontsize=12)\n",
        "        ax2.set_title('Dist√¢ncias de Mahalanobis ao Longo do Tempo', fontsize=14, fontweight='bold')\n",
        "        ax2.legend()\n",
        "        ax2.grid(True, alpha=0.3)\n",
        "        \n",
        "        # Rotacionar labels do eixo x para melhor legibilidade\n",
        "        plt.setp(ax2.xaxis.get_majorticklabels(), rotation=45)\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "        \n",
        "        # An√°lise temporal dos outliers\n",
        "        if len(outliers_data) > 0:\n",
        "            print(f\"\\nüìÖ AN√ÅLISE TEMPORAL DOS OUTLIERS:\")\n",
        "            print(\"-\" * 50)\n",
        "            \n",
        "            # Agrupar outliers por m√™s/ano para identificar padr√µes\n",
        "            outliers_data_with_time = outliers_data.copy()\n",
        "            outliers_data_with_time['Ano'] = outliers_data_with_time.index.year\n",
        "            outliers_data_with_time['M√™s'] = outliers_data_with_time.index.month\n",
        "            \n",
        "            print(f\"  ‚Ä¢ Distribui√ß√£o por ano:\")\n",
        "            year_counts = outliers_data_with_time['Ano'].value_counts().sort_index()\n",
        "            for year, count in year_counts.items():\n",
        "                print(f\"    {year}: {count} outlier(s)\")\n",
        "            \n",
        "            print(f\"  ‚Ä¢ Distribui√ß√£o por m√™s:\")\n",
        "            month_counts = outliers_data_with_time['M√™s'].value_counts().sort_index()\n",
        "            for month, count in month_counts.items():\n",
        "                month_name = ['Jan', 'Fev', 'Mar', 'Abr', 'Mai', 'Jun', \n",
        "                             'Jul', 'Ago', 'Set', 'Out', 'Nov', 'Dez'][month-1]\n",
        "                print(f\"    {month_name}: {count} outlier(s)\")\n",
        "        \n",
        "        # Compara√ß√£o com an√°lise univariada\n",
        "        print(f\"\\nüìä COMPARA√á√ÉO: AN√ÅLISE UNIVARIADA vs MULTIVARIADA:\")\n",
        "        print(\"-\" * 60)\n",
        "        \n",
        "        # Recalcular outliers univariados para compara√ß√£o\n",
        "        univariate_outliers = set()\n",
        "        for col in numeric_cols:\n",
        "            data_col = data_multivariate[col]\n",
        "            q1_col = data_col.quantile(0.25)\n",
        "            q3_col = data_col.quantile(0.75)\n",
        "            iqr_col = q3_col - q1_col\n",
        "            lower_bound = q1_col - 1.5 * iqr_col\n",
        "            upper_bound = q3_col + 1.5 * iqr_col\n",
        "            outlier_mask_col = (data_col < lower_bound) | (data_col > upper_bound)\n",
        "            univariate_outliers.update(data_col[outlier_mask_col].index)\n",
        "        \n",
        "        multivariate_outliers = set(outliers_indices)\n",
        "        \n",
        "        print(f\"  ‚Ä¢ Outliers univariados detectados: {len(univariate_outliers)}\")\n",
        "        print(f\"  ‚Ä¢ Outliers multivariados detectados: {len(multivariate_outliers)}\")\n",
        "        print(f\"  ‚Ä¢ Outliers apenas univariados: {len(univariate_outliers - multivariate_outliers)}\")\n",
        "        print(f\"  ‚Ä¢ Outliers apenas multivariados: {len(multivariate_outliers - univariate_outliers)}\")\n",
        "        print(f\"  ‚Ä¢ Outliers em ambas as an√°lises: {len(univariate_outliers & multivariate_outliers)}\")\n",
        "        \n",
        "        if len(multivariate_outliers - univariate_outliers) > 0:\n",
        "            print(f\"\\n  üéØ OUTLIERS DETECTADOS APENAS NA AN√ÅLISE MULTIVARIADA:\")\n",
        "            print(f\"    (Estes s√£o per√≠odos onde a combina√ß√£o dos pre√ßos foi an√¥mala)\")\n",
        "            only_multivariate = multivariate_outliers - univariate_outliers\n",
        "            for idx in sorted(only_multivariate):\n",
        "                row = data_multivariate.loc[idx]\n",
        "                mahal_dist = mahal_distances[data_multivariate.index == idx][0]\n",
        "                print(f\"    üìÖ {idx}: Milho={row[numeric_cols[0]]:.4f}, Soja={row[numeric_cols[1]]:.4f}, Mahal={mahal_dist:.4f}\")\n",
        "        \n",
        "        print(f\"\\nüìà INTERPRETA√á√ÉO DOS RESULTADOS:\")\n",
        "        print(\"-\" * 50)\n",
        "        print(f\"  ‚Ä¢ O Bag Plot considera a rela√ß√£o entre {numeric_cols[0]} e {numeric_cols[1]}\")\n",
        "        print(f\"  ‚Ä¢ Outliers multivariados indicam per√≠odos de comportamento conjunto an√¥malo\")\n",
        "        print(f\"  ‚Ä¢ A an√°lise √© mais robusta que m√©todos univariados tradicionais\")\n",
        "        print(f\"  ‚Ä¢ √ötil para identificar choques de mercado ou eventos extraordin√°rios\")\n",
        "        \n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è  N√£o h√° dados suficientes para an√°lise multivariada.\")\n",
        "        \n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  An√°lise multivariada requer pelo menos 2 vari√°veis num√©ricas.\")\n",
        "    print(f\"üìä Apenas {len(numeric_cols)} vari√°vel num√©rica encontrada: {numeric_cols[0] if len(numeric_cols) == 1 else 'nenhuma'}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. An√°lise de Correla√ß√£o e Covari√¢ncia\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# An√°lise de correla√ß√£o e covari√¢ncia\n",
        "if df is not None and len(numeric_cols) > 1:\n",
        "    print(\"=\" * 60)\n",
        "    print(\"AN√ÅLISE DE CORRELA√á√ÉO E COVARI√ÇNCIA\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    # Calcular matriz de correla√ß√£o\n",
        "    correlation_matrix = df[numeric_cols].corr()\n",
        "    \n",
        "    print(f\"\\nüìä MATRIZ DE CORRELA√á√ÉO:\")\n",
        "    print(\"-\" * 40)\n",
        "    print(correlation_matrix.round(4))\n",
        "    \n",
        "    # An√°lise das correla√ß√µes\n",
        "    print(f\"\\nüìà AN√ÅLISE DAS CORRELA√á√ïES:\")\n",
        "    print(\"-\" * 40)\n",
        "    \n",
        "    # Encontrar correla√ß√µes mais fortes\n",
        "    corr_pairs = []\n",
        "    for i in range(len(numeric_cols)):\n",
        "        for j in range(i+1, len(numeric_cols)):\n",
        "            corr_value = correlation_matrix.iloc[i, j]\n",
        "            corr_pairs.append((numeric_cols[i], numeric_cols[j], corr_value))\n",
        "    \n",
        "    # Ordenar por valor absoluto da correla√ß√£o\n",
        "    corr_pairs.sort(key=lambda x: abs(x[2]), reverse=True)\n",
        "    \n",
        "    for var1, var2, corr in corr_pairs:\n",
        "        strength = \"\"\n",
        "        if abs(corr) >= 0.9:\n",
        "            strength = \"muito forte\"\n",
        "        elif abs(corr) >= 0.7:\n",
        "            strength = \"forte\"\n",
        "        elif abs(corr) >= 0.5:\n",
        "            strength = \"moderada\"\n",
        "        elif abs(corr) >= 0.3:\n",
        "            strength = \"fraca\"\n",
        "        else:\n",
        "            strength = \"muito fraca\"\n",
        "        \n",
        "        direction = \"positiva\" if corr > 0 else \"negativa\"\n",
        "        print(f\"  ‚Ä¢ {var1} ‚Üî {var2}: {corr:.4f} (correla√ß√£o {strength} {direction})\")\n",
        "    \n",
        "    # Visualiza√ß√£o da matriz de correla√ß√£o\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    \n",
        "    # Heatmap da correla√ß√£o\n",
        "    mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
        "    sns.heatmap(correlation_matrix, \n",
        "                mask=mask,\n",
        "                annot=True, \n",
        "                cmap='RdBu_r', \n",
        "                center=0,\n",
        "                square=True,\n",
        "                fmt='.3f',\n",
        "                cbar_kws={\"shrink\": .8})\n",
        "    \n",
        "    plt.title('Matriz de Correla√ß√£o - Vari√°veis Num√©ricas', fontsize=14, fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # An√°lise de signific√¢ncia estat√≠stica das correla√ß√µes\n",
        "    from scipy.stats import pearsonr\n",
        "    \n",
        "    print(f\"\\nüìä TESTE DE SIGNIFIC√ÇNCIA DAS CORRELA√á√ïES:\")\n",
        "    print(\"-\" * 50)\n",
        "    \n",
        "    for var1, var2, corr in corr_pairs:\n",
        "        # Calcular p-valor\n",
        "        data1 = df[var1].dropna()\n",
        "        data2 = df[var2].dropna()\n",
        "        \n",
        "        # Alinhar os dados (remover pares com valores ausentes)\n",
        "        common_idx = data1.index.intersection(data2.index)\n",
        "        if len(common_idx) > 2:\n",
        "            corr_val, p_val = pearsonr(data1[common_idx], data2[common_idx])\n",
        "            \n",
        "            # Interpretar signific√¢ncia\n",
        "            if p_val < 0.001:\n",
        "                significance = \"***\"\n",
        "            elif p_val < 0.01:\n",
        "                significance = \"**\"\n",
        "            elif p_val < 0.05:\n",
        "                significance = \"*\"\n",
        "            else:\n",
        "                significance = \"\"\n",
        "            \n",
        "            print(f\"  ‚Ä¢ {var1} ‚Üî {var2}: r = {corr_val:.4f}, {significance} (p = {p_val:.4f})\")\n",
        "    \n",
        "    # An√°lise de covari√¢ncia\n",
        "    print(f\"\\nüìä AN√ÅLISE DE COVARI√ÇNCIA:\")\n",
        "    print(\"-\" * 50)\n",
        "    \n",
        "    # Calcular matriz de covari√¢ncia\n",
        "    covariance_matrix = df[numeric_cols].cov()\n",
        "    \n",
        "    print(f\"\\nüìä MATRIZ DE COVARI√ÇNCIA:\")\n",
        "    print(\"-\" * 40)\n",
        "    print(covariance_matrix.round(4))\n",
        "    \n",
        "    # An√°lise das covari√¢ncias\n",
        "    print(f\"\\nüìà AN√ÅLISE DAS COVARI√ÇNCIAS:\")\n",
        "    print(\"-\" * 40)\n",
        "    \n",
        "    for i in range(len(numeric_cols)):\n",
        "        for j in range(i+1, len(numeric_cols)):\n",
        "            cov_value = covariance_matrix.iloc[i, j]\n",
        "            direction = \"positiva\" if cov_value > 0 else \"negativa\"\n",
        "            print(f\"  ‚Ä¢ {numeric_cols[i]} ‚Üî {numeric_cols[j]}: {cov_value:.4f} (covari√¢ncia {direction})\")\n",
        "    \n",
        "    # Visualiza√ß√£o da matriz de covari√¢ncia\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    \n",
        "    # Heatmap da covari√¢ncia\n",
        "    mask = np.triu(np.ones_like(covariance_matrix, dtype=bool))\n",
        "    sns.heatmap(covariance_matrix, \n",
        "                mask=mask,\n",
        "                annot=True, \n",
        "                cmap='RdBu_r', \n",
        "                center=0,\n",
        "                square=True,\n",
        "                fmt='.3f',\n",
        "                cbar_kws={\"shrink\": .8})\n",
        "    \n",
        "    plt.title('Matriz de Covari√¢ncia - Vari√°veis Num√©ricas', fontsize=14, fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # An√°lise das vari√¢ncias\n",
        "    print(f\"\\nüìä AN√ÅLISE DAS VARI√ÇNCIAS:\")\n",
        "    print(\"-\" * 40)\n",
        "    for col in numeric_cols:\n",
        "        var_value = covariance_matrix.loc[col, col]\n",
        "        print(f\"  ‚Ä¢ {col}: {var_value:.4f}\")\n",
        "    \n",
        "    # Compara√ß√£o: correla√ß√£o vs covari√¢ncia\n",
        "    print(f\"\\nüìä COMPARA√á√ÉO: CORRELA√á√ÉO vs COVARI√ÇNCIA:\")\n",
        "    print(\"-\" * 50)\n",
        "    print(\"Correla√ß√£o: medida padronizada (-1 a +1)\")\n",
        "    print(\"Covari√¢ncia: medida n√£o padronizada (depende das unidades)\")\n",
        "    print(\"\\nInterpreta√ß√£o:\")\n",
        "    print(\"  ‚Ä¢ Correla√ß√£o: for√ßa da rela√ß√£o linear\")\n",
        "    print(\"  ‚Ä¢ Covari√¢ncia: magnitude da varia√ß√£o conjunta\")\n",
        "    \n",
        "    # Coeficiente de varia√ß√£o\n",
        "    print(f\"\\nüìä COEFICIENTE DE VARIA√á√ÉO:\")\n",
        "    print(\"-\" * 40)\n",
        "    for col in numeric_cols:\n",
        "        mean_val = df[col].mean()\n",
        "        std_val = df[col].std()\n",
        "        cv_val = (std_val / mean_val) * 100 if mean_val != 0 else 0\n",
        "        print(f\"  ‚Ä¢ {col}: {cv_val:.2f}%\")\n",
        "    \n",
        "    # An√°lise de dispers√£o conjunta\n",
        "    print(f\"\\nüìä AN√ÅLISE DE DISPERS√ÉO CONJUNTA:\")\n",
        "    print(\"-\" * 40)\n",
        "    \n",
        "    # Determinante da matriz de covari√¢ncia\n",
        "    det_cov = np.linalg.det(covariance_matrix)\n",
        "    print(f\"  ‚Ä¢ Determinante da matriz de covari√¢ncia: {det_cov:.4f}\")\n",
        "    \n",
        "    # Tra√ßo da matriz de covari√¢ncia\n",
        "    trace_cov = np.trace(covariance_matrix)\n",
        "    print(f\"  ‚Ä¢ Tra√ßo da matriz de covari√¢ncia: {trace_cov:.4f}\")\n",
        "    \n",
        "    # Autovalores e autovetores\n",
        "    eigenvals, eigenvecs = np.linalg.eig(covariance_matrix)\n",
        "    print(f\"  ‚Ä¢ Autovalores: {eigenvals}\")\n",
        "    \n",
        "    # Propor√ß√£o da vari√¢ncia explicada pelo primeiro componente\n",
        "    prop_var_first = eigenvals[0] / sum(eigenvals) * 100\n",
        "    print(f\"  ‚Ä¢ Propor√ß√£o da vari√¢ncia explicada pelo primeiro componente: {prop_var_first:.2f}%\")\n",
        "    \n",
        "    print(f\"\\n‚úÖ An√°lise de correla√ß√£o e covari√¢ncia conclu√≠da!\")\n",
        "    \n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  An√°lise de correla√ß√£o e covari√¢ncia requer pelo menos 2 vari√°veis num√©ricas.\")\n",
        "    if df is None:\n",
        "        print(\"‚ùå Dataset n√£o carregado.\")\n",
        "    else:\n",
        "        print(f\"üìä Apenas {len(numeric_cols)} vari√°vel num√©rica encontrada: {numeric_cols[0] if len(numeric_cols) == 1 else 'nenhuma'}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Resumo e Conclus√µes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Resumo e conclus√µes\n",
        "if df is not None:\n",
        "    print(\"=\" * 60)\n",
        "    print(\"RESUMO E CONCLUS√ïES\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    # Resumo geral do dataset\n",
        "    print(f\"\\nüìä RESUMO GERAL DO DATASET:\")\n",
        "    print(\"-\" * 50)\n",
        "    print(f\"  ‚Ä¢ Dataset: soja_milho.csv\")\n",
        "    print(f\"  ‚Ä¢ Per√≠odo analisado: {df.index[0]} a {df.index[-1]}\")\n",
        "    print(f\"  ‚Ä¢ Total de observa√ß√µes: {len(df)}\")\n",
        "    print(f\"  ‚Ä¢ Vari√°veis num√©ricas: {len(numeric_cols)}\")\n",
        "    print(f\"  ‚Ä¢ Vari√°veis categ√≥ricas: {len(categorical_cols)}\")\n",
        "    \n",
        "    # Resumo das vari√°veis num√©ricas\n",
        "    if len(numeric_cols) > 0:\n",
        "        print(f\"\\nüìà RESUMO DAS VARI√ÅVEIS NUM√âRICAS:\")\n",
        "        print(\"-\" * 50)\n",
        "        for col in numeric_cols:\n",
        "            data = df[col].dropna()\n",
        "            print(f\"  ‚Ä¢ {col}:\")\n",
        "            print(f\"    - M√©dia: {data.mean():.4f}\")\n",
        "            print(f\"    - Mediana: {data.median():.4f}\")\n",
        "            print(f\"    - Desvio padr√£o: {data.std():.4f}\")\n",
        "            print(f\"    - Coeficiente de varia√ß√£o: {(data.std()/data.mean()*100):.2f}%\")\n",
        "    \n",
        "    # Resumo da an√°lise de outliers\n",
        "    if len(numeric_cols) > 0:\n",
        "        print(f\"\\nüîç RESUMO DA AN√ÅLISE DE OUTLIERS:\")\n",
        "        print(\"-\" * 50)\n",
        "        \n",
        "        # Contar outliers univariados\n",
        "        total_univariate_outliers = 0\n",
        "        for col in numeric_cols:\n",
        "            data = df[col].dropna()\n",
        "            q1 = data.quantile(0.25)\n",
        "            q3 = data.quantile(0.75)\n",
        "            iqr = q3 - q1\n",
        "            lower_bound = q1 - 1.5 * iqr\n",
        "            upper_bound = q3 + 1.5 * iqr\n",
        "            outliers = data[(data < lower_bound) | (data > upper_bound)]\n",
        "            total_univariate_outliers += len(outliers)\n",
        "        \n",
        "        print(f\"  ‚Ä¢ Outliers univariados detectados: {total_univariate_outliers}\")\n",
        "        \n",
        "        # Contar outliers multivariados (se aplic√°vel)\n",
        "        if len(numeric_cols) >= 2:\n",
        "            try:\n",
        "                from sklearn.covariance import MinCovDet\n",
        "                data_multivariate = df[numeric_cols].dropna()\n",
        "                mcd = MinCovDet(random_state=42)\n",
        "                mcd.fit(data_multivariate)\n",
        "                robust_mean = mcd.location_\n",
        "                robust_cov = mcd.covariance_\n",
        "                \n",
        "                mahal_distances = []\n",
        "                for i, row in data_multivariate.iterrows():\n",
        "                    diff = row.values - robust_mean\n",
        "                    inv_cov = np.linalg.inv(robust_cov)\n",
        "                    mahal_dist = np.sqrt(diff.T @ inv_cov @ diff)\n",
        "                    mahal_distances.append(mahal_dist)\n",
        "                \n",
        "                mahal_distances = np.array(mahal_distances)\n",
        "                q1 = np.percentile(mahal_distances, 25)\n",
        "                q3 = np.percentile(mahal_distances, 75)\n",
        "                iqr = q3 - q1\n",
        "                lower_fence = q1 - 1.5 * iqr\n",
        "                upper_fence = q3 + 1.5 * iqr\n",
        "                \n",
        "                outlier_mask = (mahal_distances < lower_fence) | (mahal_distances > upper_fence)\n",
        "                multivariate_outliers = np.sum(outlier_mask)\n",
        "                \n",
        "                print(f\"  ‚Ä¢ Outliers multivariados detectados: {multivariate_outliers}\")\n",
        "            except:\n",
        "                print(f\"  ‚Ä¢ An√°lise multivariada n√£o dispon√≠vel\")\n",
        "    \n",
        "    # Resumo das correla√ß√µes\n",
        "    if len(numeric_cols) > 1:\n",
        "        print(f\"\\nüìä RESUMO DAS CORRELA√á√ïES:\")\n",
        "        print(\"-\" * 50)\n",
        "        correlation_matrix = df[numeric_cols].corr()\n",
        "        \n",
        "        # Encontrar correla√ß√£o mais forte\n",
        "        max_corr = 0\n",
        "        max_corr_pair = None\n",
        "        for i in range(len(numeric_cols)):\n",
        "            for j in range(i+1, len(numeric_cols)):\n",
        "                corr_val = abs(correlation_matrix.iloc[i, j])\n",
        "                if corr_val > max_corr:\n",
        "                    max_corr = corr_val\n",
        "                    max_corr_pair = (numeric_cols[i], numeric_cols[j], correlation_matrix.iloc[i, j])\n",
        "        \n",
        "        if max_corr_pair:\n",
        "            print(f\"  ‚Ä¢ Correla√ß√£o mais forte: {max_corr_pair[0]} ‚Üî {max_corr_pair[1]}: {max_corr_pair[2]:.4f}\")\n",
        "            \n",
        "            # Interpretar for√ßa da correla√ß√£o\n",
        "            if max_corr >= 0.9:\n",
        "                strength = \"muito forte\"\n",
        "            elif max_corr >= 0.7:\n",
        "                strength = \"forte\"\n",
        "            elif max_corr >= 0.5:\n",
        "                strength = \"moderada\"\n",
        "            elif max_corr >= 0.3:\n",
        "                strength = \"fraca\"\n",
        "            else:\n",
        "                strength = \"muito fraca\"\n",
        "            \n",
        "            print(f\"  ‚Ä¢ Interpreta√ß√£o: correla√ß√£o {strength}\")\n",
        "    \n",
        "    # Principais insights\n",
        "    print(f\"\\nüí° PRINCIPAIS INSIGHTS:\")\n",
        "    print(\"-\" * 50)\n",
        "    \n",
        "    if len(numeric_cols) > 0:\n",
        "        print(f\"  ‚Ä¢ Dataset cont√©m {len(df)} observa√ß√µes de pre√ßos hist√≥ricos\")\n",
        "        print(f\"  ‚Ä¢ An√°lise focada em {len(numeric_cols)} vari√°vel(is) num√©rica(s)\")\n",
        "        \n",
        "        if len(numeric_cols) >= 2:\n",
        "            print(f\"  ‚Ä¢ An√°lise multivariada permite identificar outliers conjuntos\")\n",
        "            print(f\"  ‚Ä¢ Correla√ß√£o entre vari√°veis foi analisada\")\n",
        "        \n",
        "        if total_univariate_outliers > 0:\n",
        "            print(f\"  ‚Ä¢ {total_univariate_outliers} outliers univariados identificados\")\n",
        "        else:\n",
        "            print(f\"  ‚Ä¢ Nenhum outlier univariado detectado\")\n",
        "    \n",
        "    # Recomenda√ß√µes\n",
        "    print(f\"\\nüéØ RECOMENDA√á√ïES:\")\n",
        "    print(\"-\" * 50)\n",
        "    print(f\"  ‚Ä¢ Continuar monitoramento dos pre√ßos para identificar tend√™ncias\")\n",
        "    print(f\"  ‚Ä¢ Investigar per√≠odos de outliers para entender causas\")\n",
        "    print(f\"  ‚Ä¢ Considerar an√°lise de sazonalidade para melhor compreens√£o\")\n",
        "    print(f\"  ‚Ä¢ Implementar alertas para valores extremos\")\n",
        "    \n",
        "    # Limita√ß√µes\n",
        "    print(f\"\\n‚ö†Ô∏è  LIMITA√á√ïES:\")\n",
        "    print(\"-\" * 50)\n",
        "    print(f\"  ‚Ä¢ An√°lise baseada apenas em dados hist√≥ricos\")\n",
        "    print(f\"  ‚Ä¢ N√£o considera fatores externos (clima, pol√≠tica, etc.)\")\n",
        "    print(f\"  ‚Ä¢ M√©todos de detec√ß√£o de outliers podem ter limita√ß√µes\")\n",
        "    \n",
        "    print(f\"\\n‚úÖ An√°lise explorat√≥ria conclu√≠da com sucesso!\")\n",
        "    \n",
        "else:\n",
        "    print(\"‚ùå N√£o foi poss√≠vel gerar resumo - dataset n√£o carregado.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Informa√ß√µes T√©cnicas do Notebook\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Informa√ß√µes t√©cnicas do notebook\n",
        "print(\"=\" * 60)\n",
        "print(\"INFORMA√á√ïES T√âCNICAS DO NOTEBOOK\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Informa√ß√µes do sistema\n",
        "import sys\n",
        "import platform\n",
        "from datetime import datetime\n",
        "\n",
        "print(f\"\\nüíª INFORMA√á√ïES DO SISTEMA:\")\n",
        "print(\"-\" * 40)\n",
        "print(f\"  ‚Ä¢ Python: {sys.version.split()[0]}\")\n",
        "print(f\"  ‚Ä¢ Plataforma: {platform.platform()}\")\n",
        "print(f\"  ‚Ä¢ Arquitetura: {platform.architecture()[0]}\")\n",
        "print(f\"  ‚Ä¢ Processador: {platform.processor()}\")\n",
        "\n",
        "# Informa√ß√µes das bibliotecas\n",
        "print(f\"\\nüìö BIBLIOTECAS UTILIZADAS:\")\n",
        "print(\"-\" * 40)\n",
        "print(f\"  ‚Ä¢ Pandas: {pd.__version__}\")\n",
        "print(f\"  ‚Ä¢ NumPy: {np.__version__}\")\n",
        "print(f\"  ‚Ä¢ Matplotlib: {plt.matplotlib.__version__}\")\n",
        "print(f\"  ‚Ä¢ Seaborn: {sns.__version__}\")\n",
        "print(f\"  ‚Ä¢ SciPy: {stats.__version__}\")\n",
        "\n",
        "# Informa√ß√µes do dataset\n",
        "if df is not None:\n",
        "    print(f\"\\nüìä INFORMA√á√ïES DO DATASET:\")\n",
        "    print(\"-\" * 40)\n",
        "    print(f\"  ‚Ä¢ Arquivo: soja_milho.csv\")\n",
        "    print(f\"  ‚Ä¢ Dimens√µes: {df.shape[0]} linhas √ó {df.shape[1]} colunas\")\n",
        "    print(f\"  ‚Ä¢ Mem√≥ria utilizada: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
        "    print(f\"  ‚Ä¢ Tipos de dados: {df.dtypes.value_counts().to_dict()}\")\n",
        "\n",
        "# Informa√ß√µes do notebook\n",
        "print(f\"\\nüìù INFORMA√á√ïES DO NOTEBOOK:\")\n",
        "print(\"-\" * 40)\n",
        "print(f\"  ‚Ä¢ Data de execu√ß√£o: {datetime.now().strftime('%d/%m/%Y %H:%M:%S')}\")\n",
        "print(f\"  ‚Ä¢ Vers√£o do notebook: 2.0 (Organizado)\")\n",
        "print(f\"  ‚Ä¢ Estrutura: 9 se√ß√µes principais\")\n",
        "print(f\"  ‚Ä¢ An√°lises implementadas:\")\n",
        "print(f\"    - An√°lise de qualidade dos dados\")\n",
        "print(f\"    - Estat√≠sticas descritivas\")\n",
        "print(f\"    - Visualiza√ß√µes\")\n",
        "print(f\"    - Detec√ß√£o de outliers (univariada e multivariada)\")\n",
        "print(f\"    - An√°lise de correla√ß√£o e covari√¢ncia\")\n",
        "print(f\"    - Resumo e conclus√µes\")\n",
        "\n",
        "# Metodologias utilizadas\n",
        "print(f\"\\nüî¨ METODOLOGIAS UTILIZADAS:\")\n",
        "print(\"-\" * 40)\n",
        "print(f\"  ‚Ä¢ An√°lise explorat√≥ria de dados (EDA)\")\n",
        "print(f\"  ‚Ä¢ Estat√≠stica descritiva\")\n",
        "print(f\"  ‚Ä¢ Detec√ß√£o de outliers (IQR e Bag Plot)\")\n",
        "print(f\"  ‚Ä¢ An√°lise de correla√ß√£o de Pearson\")\n",
        "print(f\"  ‚Ä¢ Estat√≠sticas robustas (MCD)\")\n",
        "print(f\"  ‚Ä¢ Dist√¢ncias de Mahalanobis\")\n",
        "\n",
        "# Configura√ß√µes de visualiza√ß√£o\n",
        "print(f\"\\nüé® CONFIGURA√á√ïES DE VISUALIZA√á√ÉO:\")\n",
        "print(\"-\" * 40)\n",
        "print(f\"  ‚Ä¢ Estilo: {plt.style.available[0]}\")\n",
        "print(f\"  ‚Ä¢ Paleta de cores: {sns.color_palette('husl')[:3]}\")\n",
        "print(f\"  ‚Ä¢ Tamanho padr√£o das figuras: {plt.rcParams['figure.figsize']}\")\n",
        "print(f\"  ‚Ä¢ Tamanho da fonte: {plt.rcParams['font.size']}\")\n",
        "\n",
        "# Performance\n",
        "print(f\"\\n‚ö° PERFORMANCE:\")\n",
        "print(\"-\" * 40)\n",
        "print(f\"  ‚Ä¢ Tempo de execu√ß√£o estimado: < 30 segundos\")\n",
        "print(f\"  ‚Ä¢ Mem√≥ria RAM necess√°ria: < 100 MB\")\n",
        "print(f\"  ‚Ä¢ Depend√™ncias: 6 bibliotecas principais\")\n",
        "\n",
        "# Compatibilidade\n",
        "print(f\"\\nüîß COMPATIBILIDADE:\")\n",
        "print(\"-\" * 40)\n",
        "print(f\"  ‚Ä¢ Python: >= 3.7\")\n",
        "print(f\"  ‚Ä¢ Jupyter: >= 6.0\")\n",
        "print(f\"  ‚Ä¢ Sistemas operacionais: Windows, macOS, Linux\")\n",
        "\n",
        "# Cr√©ditos\n",
        "print(f\"\\nüë®‚Äçüíª CR√âDITOS:\")\n",
        "print(\"-\" * 40)\n",
        "print(f\"  ‚Ä¢ Desenvolvido para an√°lise explorat√≥ria de dados\")\n",
        "print(f\"  ‚Ä¢ Dataset: soja_milho.csv\")\n",
        "print(f\"  ‚Ä¢ Metodologia: An√°lise Estat√≠stica Multivariada\")\n",
        "print(f\"  ‚Ä¢ Foco: Pre√ßos de commodities agr√≠colas\")\n",
        "\n",
        "print(f\"\\n‚úÖ Informa√ß√µes t√©cnicas registradas com sucesso!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
